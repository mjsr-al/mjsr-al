{"cells":[{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2022-04-19T12:23:18.29978Z","iopub.status.busy":"2022-04-19T12:23:18.29921Z","iopub.status.idle":"2022-04-19T12:23:18.303774Z","shell.execute_reply":"2022-04-19T12:23:18.302846Z","shell.execute_reply.started":"2022-04-19T12:23:18.299738Z"},"trusted":true},"outputs":[],"source":["# !git clone https://github.com/cubeyoung/TA-VAAL.git"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2022-04-19T12:23:18.341251Z","iopub.status.busy":"2022-04-19T12:23:18.34065Z","iopub.status.idle":"2022-04-19T12:23:18.34516Z","shell.execute_reply":"2022-04-19T12:23:18.344276Z","shell.execute_reply.started":"2022-04-19T12:23:18.341208Z"},"trusted":true},"outputs":[],"source":["# !python3 TA-VAAL/main.py -m TA-VAAL -d cifar10 -c 3"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## TA-VAAL CelebA"]},{"cell_type":"code","execution_count":55,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T16:50:20.849599Z","iopub.status.busy":"2023-06-06T16:50:20.849090Z","iopub.status.idle":"2023-06-06T16:50:35.762035Z","shell.execute_reply":"2023-06-06T16:50:35.760681Z","shell.execute_reply.started":"2023-06-06T16:50:20.849460Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/usr/bin/zsh: /home/saurabh/.conda/envs/AL/lib/python3.9/site-packages/cv2/../../../../lib/libncursesw.so.6: no version information available (required by /usr/bin/zsh)\n","Requirement already satisfied: torchsummary in /home/saurabh/.conda/envs/AL/lib/python3.9/site-packages (1.5.1)\n"]}],"source":["!pip3 install torchsummary"]},{"cell_type":"code","execution_count":56,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T16:50:35.764567Z","iopub.status.busy":"2023-06-06T16:50:35.764280Z","iopub.status.idle":"2023-06-06T16:50:45.601924Z","shell.execute_reply":"2023-06-06T16:50:45.600986Z","shell.execute_reply.started":"2023-06-06T16:50:35.764531Z"},"trusted":true},"outputs":[],"source":["import os\n","import math\n","import random\n","import torch\n","import numpy as np\n","from tqdm import tqdm\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.init as init\n","import torch.nn.functional as F\n","from torch.distributions import Normal\n","from torch.nn.parameter import Parameter\n","from torch.nn.modules.module import Module\n","from torch.utils.data import DataLoader, Dataset\n","import torch.optim.lr_scheduler as lr_scheduler\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torchvision.datasets import CIFAR100, CIFAR10, FashionMNIST, SVHN, CelebA\n","import torchvision.transforms as T\n","import torchvision.models as models\n","import argparse\n","from sklearn.metrics import confusion_matrix\n","import pandas as pd\n","# from tensorflow.keras.utils import plot_model, to_categorical\n","import cv2\n","from torchvision import models\n","from torchsummary import summary\n","from torch.utils.tensorboard import SummaryWriter\n","import csv"]},{"cell_type":"code","execution_count":57,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T16:50:45.603894Z","iopub.status.busy":"2023-06-06T16:50:45.603557Z","iopub.status.idle":"2023-06-06T16:50:45.631720Z","shell.execute_reply":"2023-06-06T16:50:45.630994Z","shell.execute_reply.started":"2023-06-06T16:50:45.603848Z"},"trusted":true},"outputs":[],"source":["class CelebADataset(Dataset):\n","\n","    def __init__(self, csv_file, root_dir, eval_file, targets, split='train', transform=None):\n","\n","        self.attr = pd.read_csv(csv_file)\n","        self.eval_partition = pd.read_csv(eval_file)\n","        self.root_dir = root_dir\n","        self.split = split\n","        self.transform = transform\n","        self.targets = targets\n","        \n","        self.attr = self.attr[self.targets]\n","        self.attr = self.attr.replace(-1, 0)\n","        self.attr = self.attr.set_index('image_id')\n","        self.eval_partition = self.eval_partition.set_index('image_id')\n","        self.attr = self.attr.join(self.eval_partition)\n","        self.attr['image_id'] = self.attr.index\n","        \n","        if self.split == 'train':\n","            self.attr = self.attr.loc[self.attr['partition']==0]\n","            self.attr = self.attr.drop('partition', axis=1)\n","        else:\n","            self.attr = self.attr.loc[self.attr['partition']==2]\n","            self.attr = self.attr.drop('partition', axis=1)\n","            \n","        self.attr = self.attr[self.targets] \n","\n","    def __len__(self):\n","        return len(self.attr)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","    \n","        img_name = self.root_dir + self.attr.iloc[idx, 0]\n","        \n","        image = cv2.imread(img_name) / 255.0\n","        attr = self.attr.iloc[idx, 1:]\n","        attr = np.array([attr])\n","        attr = attr.astype('float')\n","        \n","        image = cv2.resize(image, dsize=(96, 96), interpolation=cv2.INTER_AREA)\n","        image = image.reshape(image.shape[2], image.shape[0], -1)\n","        sample = (torch.tensor(image).float(), attr)\n","\n","        if self.transform:\n","            sample = self.transform(sample)\n","\n","        return sample\n","    \n","class SubsetSequentialSampler(torch.utils.data.Sampler):\n","    r\"\"\"Samples elements sequentially from a given list of indices, without replacement.\n","\n","    Arguments:\n","        indices (sequence): a sequence of indices\n","    \"\"\"\n","\n","    def __init__(self, indices):\n","        self.indices = indices\n","\n","    def __iter__(self):\n","        return (self.indices[i] for i in range(len(self.indices)))\n","    \n","    def __len__(self):\n","        return len(self.indices)\n","    \n","class MyDataset(Dataset):\n","    def __init__(self, dataset_name, train_flag, targets = None, transf=None):\n","        self.dataset_name = dataset_name\n","        \n","        if self.dataset_name == \"cifar10\":\n","            self.cifar10 = CIFAR10('../cifar10', train=train_flag, \n","                                    download=True, transform=transf)\n","            \n","        if self.dataset_name == \"celeba\":\n","            attr_file = '../input/celeba-dataset/list_attr_celeba.csv'\n","            eval_file = '../input/celeba-dataset/list_eval_partition.csv'\n","            path = '../input/celeba-dataset/img_align_celeba/img_align_celeba/'            \n","            self.celeba = CelebADataset(attr_file, path, eval_file, targets, split='train')  \n","\n","    def __getitem__(self, index):\n","        if self.dataset_name == \"cifar10\":\n","            data, target = self.cifar10[index]\n","            \n","        if self.dataset_name == \"celeba\":\n","            data, target = self.celeba[index]\n","        \n","        return data, target, index\n","\n","    def __len__(self):\n","        if self.dataset_name == \"cifar10\":\n","            return len(self.cifar10)\n","        \n","        if self.dataset_name == \"celeba\":\n","            return len(self.celeba)"]},{"cell_type":"code","execution_count":58,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T16:50:45.633844Z","iopub.status.busy":"2023-06-06T16:50:45.633434Z","iopub.status.idle":"2023-06-06T16:50:45.666052Z","shell.execute_reply":"2023-06-06T16:50:45.664751Z","shell.execute_reply.started":"2023-06-06T16:50:45.633809Z"},"trusted":true},"outputs":[],"source":["def load_dataset(args, targets = None):\n","    \n","    train_transform = T.Compose([\n","        T.RandomHorizontalFlip(),\n","        T.RandomCrop(size=32, padding=4),\n","        T.ToTensor(),\n","        T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]) \n","    ])\n","\n","    test_transform = T.Compose([\n","        T.ToTensor(),\n","        T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]) \n","    ])\n","        \n","    if args['dataset'] == 'celeba':\n","        \n","        attr_file = '../input/celeba-dataset/list_attr_celeba.csv'\n","        eval_file = '../input/celeba-dataset/list_eval_partition.csv'\n","        path = '../input/celeba-dataset/img_align_celeba/img_align_celeba/'\n","        \n","        data_train = CelebADataset(attr_file, path, eval_file, targets, split='train')    \n","        data_unlabeled = MyDataset(args['dataset'], True, targets)\n","        data_test  = CelebADataset(attr_file, path, eval_file, targets, split='test')\n","        NO_CLASSES = len(targets) - 1\n","        adden = args['ADDENDUM']\n","        NUM_TRAIN = len(data_train)\n","        no_train = NUM_TRAIN\n","        \n","    return data_train, data_unlabeled, data_test, adden, NO_CLASSES, no_train"]},{"cell_type":"code","execution_count":59,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T16:50:45.668125Z","iopub.status.busy":"2023-06-06T16:50:45.667799Z","iopub.status.idle":"2023-06-06T16:50:45.701024Z","shell.execute_reply":"2023-06-06T16:50:45.699965Z","shell.execute_reply.started":"2023-06-06T16:50:45.668080Z"},"trusted":true},"outputs":[],"source":["class BasicBlock(nn.Module):\n","    \n","    def __init__(self, in_planes, planes, expansion, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.expansion = expansion\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10, expansion=1):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","        self.num_classes = num_classes\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], 1, stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], 1, stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], 1, stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], 1, stride=2)\n","        \n","        self.linear1 = nn.Linear(512*expansion, 2)\n","        self.linear2 = nn.Linear(512*expansion, 2)\n","        self.linear3 = nn.Linear(512*expansion, 2)\n","        self.linear4 = nn.Linear(512*expansion, 2)\n","        self.linear5 = nn.Linear(512*expansion, 2)\n","        self.linear6 = nn.Linear(512*expansion, 2)\n","        self.linear7 = nn.Linear(512*expansion, 2)\n","        self.linear8 = nn.Linear(512*expansion, 2)\n","        self.linear9 = nn.Linear(512*expansion, 2)\n","        self.linear10 = nn.Linear(512*expansion, 2)\n","        self.linear11 = nn.Linear(512*expansion, 2)\n","\n","    def _make_layer(self, block, planes, num_blocks, expansion, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, expansion, stride))\n","            self.in_planes = planes * expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out1 = self.layer1(out)\n","        out2 = self.layer2(out1)\n","        out3 = self.layer3(out2)\n","        out4 = self.layer4(out3)\n","        out = F.avg_pool2d(out4, 4)\n","        outf = out.view(out.size(0), -1)\n","        outt1 = self.linear1(outf)\n","        outt2 = self.linear2(outf)\n","        outt3 = self.linear3(outf)\n","        outt4 = self.linear4(outf)\n","        outt5 = self.linear5(outf)\n","        outt6 = self.linear6(outf)\n","        outt7 = self.linear7(outf)\n","        outt8 = self.linear8(outf)\n","        outt9 = self.linear9(outf)\n","        outt10 = self.linear10(outf)\n","        outt11 = self.linear11(outf)\n","\n","        return [outt1, outt2, outt3, outt4, outt5, outt6, outt7, outt8, outt9, outt10, outt11], outf, [out1, out2, out3, out4]\n","\n","def ResNet18(num_classes = 10, expansion = 1):\n","    return ResNet(BasicBlock, [2,2,2,2], num_classes, expansion)"]},{"cell_type":"code","execution_count":60,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T16:50:57.468430Z","iopub.status.busy":"2023-06-06T16:50:57.468125Z","iopub.status.idle":"2023-06-06T16:50:57.513673Z","shell.execute_reply":"2023-06-06T16:50:57.512888Z","shell.execute_reply.started":"2023-06-06T16:50:57.468397Z"},"trusted":true},"outputs":[],"source":["class LossNet(nn.Module):\n","    def __init__(self, feature_sizes=[32, 16, 8, 4], num_channels=[64, 128, 256, 512], interm_dim=128):\n","        super(LossNet, self).__init__()\n","        \n","        self.GAP1 = nn.AvgPool2d(feature_sizes[0])\n","        self.GAP2 = nn.AvgPool2d(feature_sizes[1])\n","        self.GAP3 = nn.AvgPool2d(feature_sizes[2])\n","        self.GAP4 = nn.AvgPool2d(feature_sizes[3])\n","\n","        self.FC1 = nn.Linear(num_channels[0]*9, interm_dim)\n","        self.FC2 = nn.Linear(num_channels[1]*9, interm_dim)\n","        self.FC3 = nn.Linear(num_channels[2]*9, interm_dim)\n","        self.FC4 = nn.Linear(num_channels[3]*9, interm_dim)\n","\n","        self.linear = nn.Linear(4 * interm_dim, 1)\n","    \n","    def forward(self, features):\n","        out1 = self.GAP1(features[0])\n","        out1 = out1.view(out1.size(0), -1)\n","        out1 = F.relu(self.FC1(out1))\n","\n","        out2 = self.GAP2(features[1])\n","        out2 = out2.view(out2.size(0), -1)\n","        out2 = F.relu(self.FC2(out2))\n","\n","        out3 = self.GAP3(features[2])\n","        out3 = out3.view(out3.size(0), -1)\n","        out3 = F.relu(self.FC3(out3))\n","\n","        out4 = self.GAP4(features[3])\n","        out4 = out4.view(out4.size(0), -1)\n","        out4 = F.relu(self.FC4(out4))\n","\n","        out = self.linear(torch.cat((out1, out2, out3, out4), 1))\n","        return out\n","\n","class View(nn.Module):\n","    def __init__(self, size):\n","        super(View, self).__init__()\n","        self.size = size\n","\n","    def forward(self, tensor):\n","        return tensor.view(self.size)\n","    \n","class VAE(nn.Module):\n","    \"\"\"Encoder-Decoder architecture for both WAE-MMD and WAE-GAN.\"\"\"\n","    def __init__(self, z_dim=32, nc=3, f_filt=4):\n","        super(VAE, self).__init__()\n","        self.z_dim = z_dim\n","        self.nc = nc\n","        self.f_filt = f_filt\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(nc, 128, 4, 2, 1, bias=False),              # B,  128, 32, 32\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(True),\n","            nn.Conv2d(128, 256, 4, 2, 1, bias=False),             # B,  256, 16, 16\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(True),\n","            nn.Conv2d(256, 512, 4, 2, 1, bias=False),             # B,  512,  8,  8\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(True),\n","            nn.Conv2d(512, 1024, self.f_filt, 2, 1, bias=False),            # B, 1024,  4,  4\n","            nn.BatchNorm2d(1024),\n","            nn.ReLU(True),\n","            View((-1, 1024*6*6)),     #1024*14*12                            # B, 1024*4*4\n","        )\n","\n","        self.fc_mu = nn.Linear(1024*6*6, z_dim)                            # B, z_dim\n","        self.fc_logvar = nn.Linear(1024*6*6, z_dim)                            # B, z_dim\n","        self.decoder = nn.Sequential(\n","            nn.Linear(z_dim + 1, 1024*6*6),                           # B, 1024*8*8\n","            View((-1, 1024, 6, 6)),                               # B, 1024,  8,  8\n","            nn.ConvTranspose2d(1024, 512, self.f_filt, 2, 1, bias=False),   # B,  512, 16, 16\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),    # B,  256, 32, 32\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),    # B,  128, 64, 64\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(128, nc, 2, 2),                       # B,   nc, 64, 64\n","        )\n","        self.weight_init()\n","\n","    def weight_init(self):\n","        for block in self._modules:\n","            try:\n","                for m in self._modules[block]:\n","                    kaiming_init(m)\n","            except:\n","                kaiming_init(block)\n","\n","    def forward(self, r, x):\n","        z = self._encode(x)\n","        mu, logvar = self.fc_mu(z), self.fc_logvar(z)\n","        z = self.reparameterize(mu, logvar)\n","        z = torch.cat([z,r],1)\n","        x_recon = self._decode(z)\n","\n","        return  x_recon, z, mu, logvar\n","\n","    def reparameterize(self, mu, logvar):\n","        stds = (0.5 * logvar).exp()\n","        epsilon = torch.randn(*mu.size())\n","        if mu.is_cuda:\n","            stds, epsilon = stds.cuda(), epsilon.cuda()\n","        latents = epsilon * stds + mu\n","        return latents\n","\n","    def _encode(self, x):\n","        return self.encoder(x)\n","\n","    def _decode(self, z):\n","        return self.decoder(z)\n","\n","class Discriminator(nn.Module):\n","    \"\"\"Adversary architecture(Discriminator) for WAE-GAN.\"\"\"\n","    def __init__(self, z_dim=10):\n","        super(Discriminator, self).__init__()\n","        self.z_dim = z_dim\n","        self.net = nn.Sequential(\n","            nn.Linear(z_dim + 1, 512),\n","            nn.ReLU(True),\n","            nn.Linear(512, 512),\n","            nn.ReLU(True),\n","            nn.Linear(512, 1),\n","            nn.Sigmoid()\n","        )\n","        self.weight_init()\n","\n","    def weight_init(self):\n","        for block in self._modules:\n","            for m in self._modules[block]:\n","                kaiming_init(m)\n","\n","    def forward(self, r,z):  \n","        z = torch.cat([z, r], 1)\n","        return self.net(z)\n","\n","def kaiming_init(m):\n","    if isinstance(m, (nn.Linear, nn.Conv2d)):\n","        init.kaiming_normal(m.weight)\n","        if m.bias is not None:\n","            m.bias.data.fill_(0)\n","    elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n","        m.weight.data.fill_(1)\n","        if m.bias is not None:\n","            m.bias.data.fill_(0)"]},{"cell_type":"code","execution_count":61,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T16:50:59.378472Z","iopub.status.busy":"2023-06-06T16:50:59.377712Z","iopub.status.idle":"2023-06-06T16:50:59.455024Z","shell.execute_reply":"2023-06-06T16:50:59.452294Z","shell.execute_reply.started":"2023-06-06T16:50:59.378419Z"},"trusted":true},"outputs":[],"source":["def LossPredLoss(input, target, margin=1.0, reduction='mean'):\n","    assert len(input) % 2 == 0, 'the batch size is not even.'\n","    assert input.shape == input.flip(0).shape\n","    criterion = nn.BCELoss()\n","    input = (input - input.flip(0))[:len(input)//2] # [l_1 - l_2B, l_2 - l_2B-1, ... , l_B - l_B+1], where batch_size = 2B\n","    target = (target - target.flip(0))[:len(target)//2]\n","    target = target.detach()\n","    diff = torch.sigmoid(input)\n","    one = torch.sign(torch.clamp(target, min=0)) # 1 operation which is defined by the authors\n","    \n","    if reduction == 'mean':\n","        loss = criterion(diff,one)\n","    elif reduction == 'none':\n","        loss = criterion(diff,one)\n","    else:\n","        NotImplementedError()\n","    \n","    return loss\n","\n","def test(models, epoch, method, criterion, dataloaders, args, mode='val'):\n","    \n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","\n","    models['backbone'].eval()\n","    if method == 'lloss':\n","        models['module'].eval()\n","    \n","    total = 0\n","    correct = [0]*args['NO_CLASSES']\n","    m = nn.LogSoftmax(dim=1)\n","    cm=[]\n","    for i in range(args['NO_CLASSES']):\n","        cm.append(np.zeros((2,2)))\n","\n","    individual_predictor_loss = [[] for i in range(args['NO_CLASSES'])]\n","    total_lloss = 0.0\n","    total_loss = 0.0\n","    total_pred_loss = 0.0\n","        \n","    with torch.no_grad():\n","        for (inputs, labels) in dataloaders[mode]:\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            scores, _, features = models['backbone'](inputs)\n","            labels = torch.reshape(labels, (labels.size(0), labels.size(2))).float()\n","            \n","            losses = []\n","            for i in range(args['NO_CLASSES']):\n","                s = m(scores[i])\n","                _,preds = torch.max(s, dim=1)\n","\n","                cm[i]+=confusion_matrix(labels[:,i].cpu().numpy(), preds.cpu().numpy())\n","                correct[i]+=(preds == labels[:,i]).sum().item()  \n","                \n","                l = criterion[i](s.float(), labels[:,i].long())\n","                losses.append(l)\n","                individual_predictor_loss[i].append(l)    \n","                \n","            total += labels.size(0)\n","            target_loss=torch.stack(losses).mean(dim=0)\n","            \n","            if method == 'lloss' or 'TA-VAAL':\n","                if epoch > args['EPOCHL']:\n","                    features[0] = features[0].detach()\n","                    features[1] = features[1].detach()\n","                    features[2] = features[2].detach()\n","                    features[3] = features[3].detach()\n","\n","                pred_loss = models['module'](features)\n","                pred_loss = pred_loss.view(pred_loss.size(0))\n","                m_module_loss   = LossPredLoss(pred_loss, target_loss, margin=args['MARGIN'])\n","\n","                total_lloss+= m_module_loss.item()\n","\n","                m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)        \n","                loss            = m_backbone_loss + args['WEIGHT'] * m_module_loss \n","                total_loss+=loss.item()\n","            else:\n","                m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)        \n","                loss            = m_backbone_loss\n","    \n","    for i in range(args['NO_CLASSES']):\n","        individual_predictor_loss[i] = torch.stack(individual_predictor_loss[i]).mean().item()            \n","    \n","    return (100 * np.array(correct)) / total, cm, individual_predictor_loss, total_lloss, total_loss\n","\n","iters = 0\n","def train_epoch(models, method, criterion, optimizers, dataloaders, epoch, args): #epoch_loss, num_tasks):\n","    \n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    models['backbone'].train()\n","    if method == 'lloss' or 'TA-VAAL':\n","        models['module'].train()\n","    global iters\n","    models['backbone'].train()\n","    models['module'].train()\n","    models['backbone'].to(device)\n","    models['module'].to(device)\n","    m = nn.LogSoftmax(dim=1)\n","    \n","    individual_predictor_loss = [[] for i in range(args['NO_CLASSES'])]\n","    total_lloss = 0.0\n","    total_loss = 0.0\n","    correct = [0 for i in range(args['NO_CLASSES'])]\n","    total=0\n","\n","    for data in tqdm(dataloaders['train'], leave=False, total=len(dataloaders['train'])):\n","        inputs = data[0].to(device)\n","        labels = data[1].to(device)\n","\n","        iters += 1\n","\n","        optimizers['backbone'].zero_grad()\n","        if method == 'lloss' or 'TA-VAAL':\n","            optimizers['module'].zero_grad()\n","        \n","        scores, _, features = models['backbone'](inputs)\n","        \n","        labels = torch.reshape(labels, (labels.size(0), labels.size(2))).float()\n","\n","        losses = []\n","        for i in range(args['NO_CLASSES']):\n","            s = m(scores[i])\n","            _,preds = torch.max(s, dim=1)\n","            correct[i]+=(preds == labels[:,i]).sum().item()  \n","            l = criterion[i](s.float(), labels[:,i].long())\n","            losses.append(l)\n","            individual_predictor_loss[i].append(l)\n","            \n","        total += labels.size(0)\n","        target_loss=torch.stack(losses).mean(dim=0)\n","\n","        if method == 'lloss' or 'TA-VAAL':\n","            if epoch > args['EPOCHL']:\n","                features[0] = features[0].detach()\n","                features[1] = features[1].detach()\n","                features[2] = features[2].detach()\n","                features[3] = features[3].detach()\n","                \n","            pred_loss = models['module'](features)\n","            pred_loss = pred_loss.view(pred_loss.size(0))\n","            m_module_loss   = LossPredLoss(pred_loss, target_loss, margin=args['MARGIN'])\n","            \n","            total_lloss+= m_module_loss.item()\n","\n","            m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)        \n","            loss            = m_backbone_loss + args['WEIGHT'] * m_module_loss \n","            total_loss+=loss.item()\n","        else:\n","            m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)        \n","            loss            = m_backbone_loss\n","            \n","        loss.backward()\n","        optimizers['backbone'].step()\n","        if method == 'lloss' or 'TA-VAAL':\n","            optimizers['module'].step()\n","            \n","    for i in range(args['NO_CLASSES']):\n","        individual_predictor_loss[i] = torch.stack(individual_predictor_loss[i]).mean().item()\n","            \n","    return loss, individual_predictor_loss, total_lloss, total_loss, (100 * np.array(correct)) / total\n","\n","def train(models, method, criterion, optimizers, schedulers, dataloaders, cycle, args):\n","    \n","    print('>> Train a Model.')\n","    best_acc = 0.\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","    num_tasks=args['NO_CLASSES']\n","    \n","    rows = []\n","    \n","    print(len(dataloaders['train']))\n","    for epoch in range(args['no_of_epochs']):\n","        row = [cycle, epoch]\n","\n","        best_loss = torch.tensor([0.5]).to(device)\n","        loss, individual_predictor_loss, total_lloss, total_loss, individual_acc = train_epoch(models, method, criterion, optimizers, dataloaders, epoch, args)\n","\n","        schedulers['backbone'].step()\n","        if method == 'lloss' or 'TA-VAAL':\n","            schedulers['module'].step()\n","            \n","        # logging individual_predictor_loss\n","        for i in range(num_tasks):\n","            args['writer-train'].add_scalar(str(cycle) + ' Individual predictor loss '+ str(i),\n","                    individual_predictor_loss[i], epoch)\n","        \n","        # logging total predictor loss\n","        args['writer-train'].add_scalar(str(cycle) + ' Total predictor loss ',\n","                    sum(individual_predictor_loss)/len(individual_predictor_loss), epoch)\n","        \n","        # logging total lloss\n","        args['writer-train'].add_scalar(str(cycle) + ' Total lloss ',\n","                    total_lloss/len(dataloaders['train']), epoch)\n","        \n","        # logging total loss\n","        args['writer-train'].add_scalar(str(cycle) + ' Total loss ',\n","                    total_loss/len(dataloaders['train']), epoch)\n","        \n","        \n","        ## logging predictor acc\n","        args['writer-train'].add_scalar(str(cycle) + ' Total predictor Acc ',\n","                    sum(individual_acc) / len(individual_acc), epoch)\n","        row.append(sum(individual_acc) / len(individual_acc))\n","\n","        # logging individual_predictor_acc\n","        for i in range(num_tasks):\n","            args['writer-train'].add_scalar(str(cycle) + ' Individual predictor acc ' + str(i),\n","                        individual_acc[i], epoch)\n","            row.append(individual_acc[i])\n","\n","        # Testing\n","        acc, cm, individual_predictor_loss, total_lloss, total_loss = test(models, epoch, method, criterion, dataloaders, args, mode='test')\n","        print('Epoch ' + str(epoch)+\": \"+'Mean Accuracy: ', acc.mean())\n","        \n","        \n","        # logging predictor_acc\n","        args['writer-val'].add_scalar(str(cycle) + ' Total predictor Acc ',\n","                    acc.mean(), epoch)\n","        row.append(acc.mean())\n","\n","        # logging individual_predictor_acc\n","        for i in range(num_tasks):\n","            args['writer-val'].add_scalar(str(cycle) + ' Individual predictor acc ' + str(i),\n","                        acc[i], epoch)\n","            row.append(acc[i])\n","        \n","        # logging individual_predictor_loss\n","        for i in range(num_tasks):\n","            args['writer-val'].add_scalar(str(cycle) + ' Individual predictor loss '+ str(i),\n","                    individual_predictor_loss[i], epoch)\n","            \n","        # logging total predictor loss\n","        args['writer-val'].add_scalar(str(cycle) + ' Total predictor loss ',\n","                    sum(individual_predictor_loss)/len(individual_predictor_loss), epoch)\n","            \n","        # logging total lloss\n","        args['writer-val'].add_scalar(str(cycle) + ' Total lloss ',\n","                    total_lloss/len(dataloaders['test']), epoch)\n","        \n","        # logging total loss\n","        args['writer-val'].add_scalar(str(cycle) + ' Total loss ',\n","                    total_loss/len(dataloaders['test']), epoch)      \n","        \n","        rows.append(row)\n","        \n","    with open(\"results_\" + args['group'] + '_' + str(args['attempt']) + \".csv\", 'a') as csvfile: \n","        csvwriter = csv.writer(csvfile) \n","\n","        # writing the data rows \n","        csvwriter.writerows(rows)\n","\n","            \n","#         if True and epoch % 20  == 0:\n","#             acc, cm = test(models, epoch, method, dataloaders, args, mode='test')\n","#             print('Mean Accuracy:', acc.mean())\n","#             for i in range(args['NO_CLASSES']):\n","#                 print('val_accuracy', i, ':', acc[i])\n","    print('>> Finished.')"]},{"cell_type":"code","execution_count":62,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T16:51:01.413833Z","iopub.status.busy":"2023-06-06T16:51:01.413420Z","iopub.status.idle":"2023-06-06T16:51:01.470753Z","shell.execute_reply":"2023-06-06T16:51:01.469292Z","shell.execute_reply.started":"2023-06-06T16:51:01.413784Z"},"trusted":true},"outputs":[],"source":["def read_data(dataloader, labels=True):\n","    if labels:\n","        while True:\n","            for img, label,_ in dataloader:\n","                yield img, label\n","    else:\n","        while True:\n","            for img, _, _ in dataloader:\n","                yield img\n","\n","def vae_loss(x, recon, mu, logvar, beta):\n","    mse_loss = nn.MSELoss()\n","    MSE = mse_loss(recon, x)\n","    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n","    KLD = KLD * beta\n","    return MSE + KLD\n","\n","def train_vaal(models, optimizers, labeled_dataloader, unlabeled_dataloader, cycle, args):\n","    \n","    vae = models['vae']\n","    discriminator = models['discriminator']\n","    task_model = models['backbone']\n","    ranker = models['module']\n","    \n","    task_model.eval()\n","    ranker.eval()\n","    vae.train()\n","    discriminator.train()\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","\n","    vae = vae.to(device)\n","    discriminator = discriminator.to(device)\n","    task_model = task_model.to(device)\n","    ranker = ranker.to(device)\n","\n","    adversary_param = 1\n","    beta          = 1\n","    num_adv_steps = 1\n","    num_vae_steps = 1\n","\n","    bce_loss = nn.BCELoss()\n","    \n","    labeled_data = read_data(labeled_dataloader)\n","    unlabeled_data = read_data(unlabeled_dataloader)\n","\n","    train_iterations = 1250 # int( (args['INCREMENTAL']*cycle+ args['SUBSET']) * args['EPOCHV'] / args['BATCH'] )\n","    print('Num of Iteration:', str(train_iterations))\n","    \n","    for iter_count in range(train_iterations):\n","        labeled_imgs, labels = next(labeled_data)\n","        unlabeled_imgs = next(unlabeled_data)[0]\n","        \n","        labeled_imgs = labeled_imgs.to(device)\n","        unlabeled_imgs = unlabeled_imgs.to(device)\n","        labels = labels.to(device)    \n","        \n","        if iter_count == 0 :\n","            r_l_0 = torch.from_numpy(np.random.uniform(0, 1, size=(labeled_imgs.shape[0],1))).type(torch.FloatTensor).to(device)\n","            r_u_0 = torch.from_numpy(np.random.uniform(0, 1, size=(unlabeled_imgs.shape[0],1))).type(torch.FloatTensor).to(device)\n","        else:\n","            with torch.no_grad():\n","                _,_,features_l = task_model(labeled_imgs)\n","                _,_,feature_u = task_model(unlabeled_imgs)\n","                r_l = ranker(features_l)\n","                r_u = ranker(feature_u)\n","        if iter_count == 0:\n","            r_l = r_l_0.detach()\n","            r_u = r_u_0.detach()\n","            r_l_s = r_l_0.detach()\n","            r_u_s = r_u_0.detach()\n","        else:\n","            r_l_s = torch.sigmoid(r_l).detach()\n","            r_u_s = torch.sigmoid(r_u).detach()   \n","            \n","        # VAE step\n","        for count in range(num_vae_steps): # num_vae_steps\n","            recon, _, mu, logvar = vae(r_l_s,labeled_imgs)\n","            unsup_loss = vae_loss(labeled_imgs, recon, mu, logvar, beta)\n","            unlab_recon, _, unlab_mu, unlab_logvar = vae(r_u_s,unlabeled_imgs)\n","            transductive_loss = vae_loss(unlabeled_imgs, \n","                    unlab_recon, unlab_mu, unlab_logvar, beta)\n","        \n","            labeled_preds = discriminator(r_l,mu)\n","            unlabeled_preds = discriminator(r_u,unlab_mu)\n","            \n","            lab_real_preds = torch.ones(labeled_imgs.size(0))\n","            unlab_real_preds = torch.ones(unlabeled_imgs.size(0))\n","                \n","            lab_real_preds = lab_real_preds.to(device)\n","            unlab_real_preds = unlab_real_preds.to(device)            \n","\n","            dsc_loss = bce_loss(labeled_preds[:,0], lab_real_preds) + \\\n","                       bce_loss(unlabeled_preds[:,0], unlab_real_preds)\n","            total_vae_loss = unsup_loss + transductive_loss + adversary_param * dsc_loss\n","            \n","            optimizers['vae'].zero_grad()\n","            total_vae_loss.backward()\n","            optimizers['vae'].step()\n","\n","            # sample new batch if needed to train the adversarial network\n","            if count < (num_vae_steps - 1):\n","                labeled_imgs, _ = next(labeled_data)\n","                unlabeled_imgs = next(unlabeled_data)[0]\n","                \n","                labeled_imgs = labeled_imgs.to(device)\n","                unlabeled_imgs = unlabeled_imgs.to(device)\n","                labels = labels.to(device)                \n","\n","        # Discriminator step\n","        for count in range(num_adv_steps):\n","            with torch.no_grad():\n","                _, _, mu, _ = vae(r_l_s,labeled_imgs)\n","                _, _, unlab_mu, _ = vae(r_u_s,unlabeled_imgs)\n","            \n","            labeled_preds = discriminator(r_l,mu)\n","            unlabeled_preds = discriminator(r_u,unlab_mu)\n","            \n","            lab_real_preds = torch.ones(labeled_imgs.size(0))\n","            unlab_fake_preds = torch.zeros(unlabeled_imgs.size(0))\n","\n","            lab_real_preds = lab_real_preds.to(device)\n","            unlab_fake_preds = unlab_fake_preds.to(device)            \n","            \n","            dsc_loss = bce_loss(labeled_preds[:,0], lab_real_preds) + \\\n","                       bce_loss(unlabeled_preds[:,0], unlab_fake_preds)\n","\n","            optimizers['discriminator'].zero_grad()\n","            dsc_loss.backward()\n","            optimizers['discriminator'].step()\n","\n","            # sample new batch if needed to train the adversarial network\n","            if count < (num_adv_steps-1):\n","                labeled_imgs, _ = next(labeled_data)\n","                unlabeled_imgs = next(unlabeled_data)[0]\n","\n","                labeled_imgs = labeled_imgs.to(device)\n","                unlabeled_imgs = unlabeled_imgs.to(device)\n","                labels = labels.to(device)                \n","                \n","            if iter_count % 50 == 0:\n","                # print(\"Iteration: \" + str(iter_count) + \"  vae_loss: \" + str(total_vae_loss.item()) + \" dsc_loss: \" +str(dsc_loss.item()))\n","                args['writer-train'].add_scalar(str(cycle) + ' Total VAE Loss ',\n","                        total_vae_loss.item(), iter_count)\n","                args['writer-train'].add_scalar(str(cycle) + ' Total DSC Loss ',\n","                        dsc_loss.item(), iter_count)\n","                \n","# Select the indices of the unlablled data according to the methods\n","def query_samples(model, method, data_unlabeled, subset, labeled_set, cycle, args):\n","    \n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","    if method == 'TA-VAAL':\n","        # Create unlabeled dataloader for the unlabeled subset\n","        unlabeled_loader = DataLoader(data_unlabeled, batch_size=args['BATCH'], \n","                                    sampler=SubsetSequentialSampler(subset), \n","                                    pin_memory=True)\n","        labeled_loader = DataLoader(data_unlabeled, batch_size=args['BATCH'], \n","                                    sampler=SubsetSequentialSampler(labeled_set), \n","                                    pin_memory=True)\n","        \n","        vae = VAE()\n","        discriminator = Discriminator(32)\n","     \n","        models      = {'backbone': model['backbone'], 'module': model['module'], 'vae': vae, 'discriminator': discriminator}\n","        \n","        optim_vae = optim.Adam(vae.parameters(), lr=5e-4)\n","        optim_discriminator = optim.Adam(discriminator.parameters(), lr=5e-4)\n","        optimizers = {'vae': optim_vae, 'discriminator':optim_discriminator}\n","\n","        train_vaal(models,optimizers, labeled_loader, unlabeled_loader, cycle+1, args)\n","        task_model = models['backbone']\n","        ranker = models['module']        \n","        all_preds, all_indices = [], []\n","\n","        for images, _, indices in unlabeled_loader:                       \n","            images = images.to(device)\n","            with torch.no_grad():\n","                _,_,features = task_model(images)\n","                r = ranker(features)\n","                _, _, mu, _ = vae(torch.sigmoid(r),images)\n","                preds = discriminator(r,mu)\n","\n","            preds = preds.cpu().data\n","            all_preds.extend(preds)\n","            all_indices.extend(indices)\n","\n","        all_preds = torch.stack(all_preds)\n","        all_preds = all_preds.view(-1)\n","        # need to multiply by -1 to be able to use torch.topk \n","        all_preds *= -1\n","        # select the points which the discriminator things are the most likely to be unlabeled\n","        _, arg = torch.sort(all_preds) \n","        \n","        torch.save(vae, 'saved_history/models/vae-' + args['group'] +'cycle-'+str(cycle)+'.pth')\n","        torch.save(discriminator, 'saved_history/models/discriminator-' + args['group'] +'cycle-'+str(cycle)+'.pth')\n","        \n","    return arg"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T16:51:03.841876Z","iopub.status.busy":"2023-06-06T16:51:03.841560Z","iopub.status.idle":"2023-06-06T16:51:03.886618Z","shell.execute_reply":"2023-06-06T16:51:03.885853Z","shell.execute_reply.started":"2023-06-06T16:51:03.841842Z"},"trusted":true},"outputs":[],"source":["# Main\n","def main(args):\n","    \n","    targets = ['image_id', 'Bald', 'Bangs', 'Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair', 'Receding_Hairline', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat' ]\n","\n","    if args['dataset'] == 'celeba':\n","        args['NO_CLASSES'] = len(targets) - 1\n","    \n","    method = args['method_type']\n","    results = open('results_'+str(args['method_type'])+\"_\"+args['dataset'] +'_main'+str(args['CYCLES'])+str(args['total'])+'.txt','w')\n","    print(\"Dataset: %s\"%args['dataset'])\n","    print(\"Method type:%s\"%method)\n","    \n","    if args['total']:\n","        args['TRIALS'] = 1\n","        args['CYCLES'] = 1\n","    else:\n","        args['CYCLES'] = args['CYCLES']\n","        \n","    # fields\n","    fields = ['cycle', 'epoch', 'total_train_pred_acc']\n","    for i in range(len(targets)-1):\n","        fields.append('train_pred_'+ str(i+1) + '_acc')\n","    fields.append('total_val_pred_acc')\n","    for i in range(len(targets)-1):\n","        fields.append('val_pred_'+ str(i+1) + '_acc')\n","    \n","    # name of csv file \n","    filename = \"results_\" + args['group'] + '_' + str(args['attempt']) + \".csv\"\n","\n","    # writing to csv file \n","    with open(filename, 'a') as csvfile: \n","        # creating a csv writer object \n","        csvwriter = csv.writer(csvfile) \n","        # writing the fields \n","        csvwriter.writerow(fields)     \n","        \n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","    for trial in range(args['trials']):\n","        \n","        # Load training and testing dataset\n","        data_train, data_unlabeled, data_test, adden, NO_CLASSES, no_train = load_dataset(args, targets)\n","        print('The entire datasize is {}'.format(len(data_train)))  \n","        \n","        ADDENDUM = adden\n","        NUM_TRAIN = no_train\n","        indices = list(range(NUM_TRAIN))\n","        random.shuffle(indices)\n","\n","        if args['total']:\n","            labeled_set= indices\n","        else:\n","            labeled_set = indices[:ADDENDUM]\n","            unlabeled_set = [x for x in indices if x not in labeled_set]\n","        \n","        train_loader = DataLoader(data_train, batch_size=args['BATCH'], \n","                                     sampler=SubsetRandomSampler(labeled_set), \n","                                     pin_memory=True, drop_last=True)\n","        test_loader  = DataLoader(data_test, batch_size=args['BATCH'],  drop_last=True)\n","        dataloaders  = {'train': train_loader, 'test': test_loader}\n","        \n","        np.save(\"saved_history/initial-labelled-\" + args['group'] + \".npy\", np.array(labeled_set))\n","        print('Len: ', len(train_loader), 'Len:', len(test_loader))\n","        \n","        for cycle in range(args['CYCLES']):\n","            print(cycle)\n","            \n","            if not args['total']:\n","                random.shuffle(unlabeled_set)\n","                subset = unlabeled_set[:args['SUBSET']]\n","                            \n","            resnet18    = ResNet18(num_classes=args['NO_CLASSES'], expansion=9).to(device)\n","            if method == 'lloss' or 'TA-VAAL':\n","                loss_module = LossNet().to(device)\n","\n","            models      = {'backbone': resnet18}\n","            if method =='lloss' or 'TA-VAAL':\n","                models = {'backbone': resnet18, 'module': loss_module}\n","            torch.backends.cudnn.benchmark = True\n","                        \n","            labels_l = []\n","            for i in labeled_set:\n","                labels_l.append(data_train[i][1])\n","\n","            counts = np.array(labels_l).reshape((-1, args['NO_CLASSES'])).sum(axis=0)\n","\n","            wts_0 = (counts / len(labels_l))\n","            wts_1 = 1 - (counts / len(labels_l))\n","            print(wts_0)\n","            \n","            # Loss, criterion and scheduler (re)initialization\n","            criterion      = [nn.NLLLoss(reduction='none', weight = torch.tensor([wts_0[i], wts_1[i]]).to(device)).float() for i in range(args['NO_CLASSES'])]\n","            optim_backbone = optim.SGD(models['backbone'].parameters(), lr=args['LR'], \n","                momentum=args['MOMENTUM'], weight_decay=args['WDECAY'])\n"," \n","            sched_backbone = lr_scheduler.MultiStepLR(optim_backbone, milestones=args['MILESTONES'])\n","            optimizers = {'backbone': optim_backbone}\n","            schedulers = {'backbone': sched_backbone}\n","            if method == 'lloss' or 'TA-VAAL':\n","                optim_module   = optim.SGD(models['module'].parameters(), lr=args['LR'], \n","                    momentum=args['MOMENTUM'], weight_decay=args['WDECAY'])\n","                sched_module   = lr_scheduler.MultiStepLR(optim_module, milestones=args['MILESTONES'])\n","                optimizers = {'backbone': optim_backbone, 'module': optim_module}\n","                schedulers = {'backbone': sched_backbone, 'module': sched_module} \n","\n","            # Training and testing\n","            train(models, method, criterion, optimizers, schedulers, dataloaders, cycle+1, args)\n","            acc, cm, individual_predictor_loss, total_lloss, total_loss = test(models, args['no_of_epochs'], method, criterion, dataloaders, args, mode='test')\n","            torch.save(models['backbone'], 'saved_history/models/predictor-backbone-' + args['group'] + 'cycle-'+str(cycle+1)+'.pth')\n","            torch.save(models['module'], 'saved_history/models/predictor-module-'+args['group']+'cycle-'+str(cycle+1)+'.pth')\n","            \n","            print('Trial {}/{} || Cycle {}/{} || Label set size {}'.format(trial+1, args['trials'], cycle+1, args['CYCLES'], len(labeled_set)))\n","\n","            for i in range(args['NO_CLASSES']):\n","                print('val_accuracy', i, ':', acc[i])\n","                print(cm[i])\n","                print(\"\")\n","            \n","            if cycle == (args['CYCLES']-1):\n","                # Reached final training cycle\n","                print(\"Finished.\")\n","                break\n","                \n","            # Get the indices of the unlabeled samples to train on next cycle\n","            arg = query_samples(models, method, data_unlabeled, subset, labeled_set, cycle, args)\n","            \n","            # Update the labeled dataset and the unlabeled dataset, respectively\n","            new_list = list(torch.tensor(subset)[arg][:args['INCREMENTAL']].numpy())\n","            labeled_set += list(torch.tensor(subset)[arg][-args['INCREMENTAL']:].numpy())\n","            listd = list(torch.tensor(subset)[arg][:-args['INCREMENTAL']].numpy()) \n","            unlabeled_set = listd + unlabeled_set[args['SUBSET']:]\n","            print(len(labeled_set), min(labeled_set), max(labeled_set))\n","            \n","            np.save(\"saved_history/labelled-\" + args['group'] + str(cycle) + \".npy\", np.array(labeled_set))\n","            \n","            # Create a new dataloader for the updated labeled dataset\n","            dataloaders['train'] = DataLoader(data_train, batch_size=args['BATCH'], \n","                                            sampler=SubsetRandomSampler(labeled_set), \n","                                            pin_memory=True, drop_last=True)\n","\n","    results.close()"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T16:51:09.333532Z","iopub.status.busy":"2023-06-06T16:51:09.333213Z","iopub.status.idle":"2023-06-06T16:51:09.338870Z","shell.execute_reply":"2023-06-06T16:51:09.337811Z","shell.execute_reply.started":"2023-06-06T16:51:09.333497Z"},"trusted":true},"outputs":[],"source":["splits = [0.1,0.15,0.2,0.25,0.3,0.35,0.4]"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T16:51:16.636299Z","iopub.status.busy":"2023-06-06T16:51:16.635989Z","iopub.status.idle":"2023-06-06T16:51:16.642732Z","shell.execute_reply":"2023-06-06T16:51:16.641629Z","shell.execute_reply.started":"2023-06-06T16:51:16.636266Z"},"trusted":true},"outputs":[],"source":["# change targets in main\n","# change attr. group name and in writer-train/val\n","# change len(targets) in args NO_CLASSES\n","# change outputs in Predictor \n","# Change lr (if)\n","# Change epochs (if)\n","# Change attempt number (if) and change attempt in writer-train/val"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T16:51:22.413348Z","iopub.status.busy":"2023-06-06T16:51:22.413035Z","iopub.status.idle":"2023-06-06T16:51:22.425183Z","shell.execute_reply":"2023-06-06T16:51:22.424340Z","shell.execute_reply.started":"2023-06-06T16:51:22.413314Z"},"trusted":true},"outputs":[{"ename":"TypeError","evalue":"descriptor to field 'tensorflow.TensorProto.dtype' doesn't apply to 'TensorProto' object","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorboard/compat/__init__.py:42\u001b[0m, in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtensorboard\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcompat\u001b[39;00m \u001b[39mimport\u001b[39;00m notf  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'notf' from 'tensorboard.compat' (/home/saurabh/.conda/envs/AL/lib/python3.9/site-packages/tensorboard/compat/__init__.py)","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[66], line 27\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[39m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmethod_type\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mTA-VAAL\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mceleba\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mgroup\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mhead\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtotal\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrials\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mCYCLES\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m7\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mADDENDUM\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m16277\u001b[39m, \u001b[39m## num of images increment\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mINCREMENTAL\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m8138\u001b[39m, \u001b[39m## \u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mBATCH\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m32\u001b[39m, \u001b[39m## \u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mSUBSET\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m20000\u001b[39m, \u001b[39m##\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mNO_CLASSES\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m11\u001b[39m, \u001b[39m##\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mLR\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1e-4\u001b[39m,\n\u001b[1;32m     14\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mWDECAY\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m5e-4\u001b[39m,\n\u001b[1;32m     15\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mMOMENTUM\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.9\u001b[39m,\n\u001b[1;32m     16\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mMILESTONES\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m160\u001b[39m, \u001b[39m240\u001b[39m],\n\u001b[1;32m     17\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mMARGIN\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1.0\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mWEIGHT\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1.0\u001b[39m,\n\u001b[1;32m     19\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mno_of_epochs\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m20\u001b[39m,\n\u001b[1;32m     20\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlambda_loss\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1.2\u001b[39m,\n\u001b[1;32m     21\u001b[0m     \u001b[39m'\u001b[39m\u001b[39ms_margin\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.1\u001b[39m,\n\u001b[1;32m     22\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mhidden_units\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m128\u001b[39m,\n\u001b[1;32m     23\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mdropout_rate\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0.3\u001b[39m,\n\u001b[1;32m     24\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mEPOCHV\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m10\u001b[39m, \n\u001b[1;32m     25\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mEPOCHL\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m12\u001b[39m, \n\u001b[1;32m     26\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mattempt\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[0;32m---> 27\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mwriter-train\u001b[39m\u001b[39m'\u001b[39m: SummaryWriter(\u001b[39m'\u001b[39;49m\u001b[39mlogs/TAVAAL-CELEBA-head-1-Train\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m     28\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mwriter-val\u001b[39m\u001b[39m'\u001b[39m: SummaryWriter(\u001b[39m'\u001b[39m\u001b[39mlogs/TAVAAL-CELEBA-head-1-Val\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     29\u001b[0m }\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py:247\u001b[0m, in \u001b[0;36mSummaryWriter.__init__\u001b[0;34m(self, log_dir, comment, purge_step, max_queue, flush_secs, filename_suffix)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39m# Initialize the file writers, but they can be cleared out on close\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[39m# and recreated later as needed.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_writer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_writers \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 247\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_file_writer()\n\u001b[1;32m    249\u001b[0m \u001b[39m# Create default bins for histograms, see generate_testdata.py in tensorflow/tensorboard\u001b[39;00m\n\u001b[1;32m    250\u001b[0m v \u001b[39m=\u001b[39m \u001b[39m1e-12\u001b[39m\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py:277\u001b[0m, in \u001b[0;36mSummaryWriter._get_file_writer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Returns the default FileWriter instance. Recreates it if closed.\"\"\"\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_writers \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_writer \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_writer \u001b[39m=\u001b[39m FileWriter(\n\u001b[1;32m    278\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_dir, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_queue, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mflush_secs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilename_suffix\n\u001b[1;32m    279\u001b[0m     )\n\u001b[1;32m    280\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mall_writers \u001b[39m=\u001b[39m {\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_writer\u001b[39m.\u001b[39mget_logdir(): \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_writer}\n\u001b[1;32m    281\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpurge_step \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/torch/utils/tensorboard/writer.py:76\u001b[0m, in \u001b[0;36mFileWriter.__init__\u001b[0;34m(self, log_dir, max_queue, flush_secs, filename_suffix)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[39m# Sometimes PosixPath is passed in and we need to coerce it to\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39m# a string in all cases\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m# TODO: See if we can remove this in the future if we are\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[39m# actually the ones passing in a PosixPath\u001b[39;00m\n\u001b[1;32m     75\u001b[0m log_dir \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(log_dir)\n\u001b[0;32m---> 76\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevent_writer \u001b[39m=\u001b[39m EventFileWriter(\n\u001b[1;32m     77\u001b[0m     log_dir, max_queue, flush_secs, filename_suffix\n\u001b[1;32m     78\u001b[0m )\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorboard/summary/writer/event_file_writer.py:72\u001b[0m, in \u001b[0;36mEventFileWriter.__init__\u001b[0;34m(self, logdir, max_queue_size, flush_secs, filename_suffix)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates a `EventFileWriter` and an event file to write to.\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[39mOn construction the summary writer creates a new event file in `logdir`.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m    pending events and summaries to disk.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logdir \u001b[39m=\u001b[39m logdir\n\u001b[0;32m---> 72\u001b[0m tf\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mmakedirs(logdir)\n\u001b[1;32m     73\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file_name \u001b[39m=\u001b[39m (\n\u001b[1;32m     74\u001b[0m     os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m     75\u001b[0m         logdir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[39m+\u001b[39m filename_suffix\n\u001b[1;32m     85\u001b[0m )  \u001b[39m# noqa E128\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_general_file_writer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mGFile(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file_name, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorboard/lazy.py:65\u001b[0m, in \u001b[0;36mlazy_load.<locals>.wrapper.<locals>.LazyModule.__getattr__\u001b[0;34m(self, attr_name)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, attr_name):\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(load_once(\u001b[39mself\u001b[39;49m), attr_name)\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorboard/lazy.py:97\u001b[0m, in \u001b[0;36m_memoize.<locals>.wrapper\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[39mwith\u001b[39;00m lock:\n\u001b[1;32m     96\u001b[0m         \u001b[39mif\u001b[39;00m cache\u001b[39m.\u001b[39mget(arg, nothing) \u001b[39mis\u001b[39;00m nothing:\n\u001b[0;32m---> 97\u001b[0m             cache[arg] \u001b[39m=\u001b[39m f(arg)\n\u001b[1;32m     98\u001b[0m \u001b[39mreturn\u001b[39;00m cache[arg]\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorboard/lazy.py:50\u001b[0m, in \u001b[0;36mlazy_load.<locals>.wrapper.<locals>.load_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m load_once\u001b[39m.\u001b[39mloading \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 50\u001b[0m     module \u001b[39m=\u001b[39m load_fn()\n\u001b[1;32m     51\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     load_once\u001b[39m.\u001b[39mloading \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorboard/compat/__init__.py:45\u001b[0m, in \u001b[0;36mtf\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 45\u001b[0m         \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\n\u001b[1;32m     47\u001b[0m         \u001b[39mreturn\u001b[39;00m tensorflow\n\u001b[1;32m     48\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorflow/__init__.py:37\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_sys\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_typing\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtools\u001b[39;00m \u001b[39mimport\u001b[39;00m module_util \u001b[39mas\u001b[39;00m _module_util\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlazy_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m LazyLoader \u001b[39mas\u001b[39;00m _LazyLoader\n\u001b[1;32m     40\u001b[0m \u001b[39m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001b[39;00m\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorflow/python/__init__.py:42\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39meager\u001b[39;00m \u001b[39mimport\u001b[39;00m context\n\u001b[1;32m     39\u001b[0m \u001b[39m# pylint: enable=wildcard-import\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m \u001b[39m# Bring in subpackages.\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m data\n\u001b[1;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[1;32m     44\u001b[0m \u001b[39m# from tensorflow.python import keras\u001b[39;00m\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorflow/python/data/__init__.py:21\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"`tf.data.Dataset` API for input pipelines.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39mSee [Importing Data](https://tensorflow.org/guide/data) for an overview.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m experimental\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m AUTOTUNE\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdataset_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorflow/python/data/experimental/__init__.py:97\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Experimental API for building input pipelines.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39mThis module contains experimental `Dataset` sources and transformations that can\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39m@@UNKNOWN_CARDINALITY\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     96\u001b[0m \u001b[39m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m \u001b[39mimport\u001b[39;00m service\n\u001b[1;32m     98\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatching\u001b[39;00m \u001b[39mimport\u001b[39;00m dense_to_ragged_batch\n\u001b[1;32m     99\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbatching\u001b[39;00m \u001b[39mimport\u001b[39;00m dense_to_sparse_batch\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorflow/python/data/experimental/service/__init__.py:419\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"API for using the tf.data service.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39mThis module contains:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[39m  job of ParameterServerStrategy).\u001b[39;00m\n\u001b[1;32m    417\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_service_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[1;32m    420\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_service_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m from_dataset_id\n\u001b[1;32m    421\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_service_ops\u001b[39;00m \u001b[39mimport\u001b[39;00m register_dataset\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/data_service_ops.py:22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprotobuf\u001b[39;00m \u001b[39mimport\u001b[39;00m data_service_pb2\n\u001b[1;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m tf2\n\u001b[0;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m compression_ops\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mservice\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_server_lib\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexperimental\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mservice\u001b[39;00m \u001b[39mimport\u001b[39;00m _pywrap_utils\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorflow/python/data/experimental/ops/compression_ops.py:16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Copyright 2020 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Ops for compressing and uncompressing dataset elements.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m structure\n\u001b[1;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m gen_experimental_dataset_ops \u001b[39mas\u001b[39;00m ged_ops\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompress\u001b[39m(element):\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorflow/python/data/util/structure.py:30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m type_spec\n\u001b[1;32m     29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m type_spec_registry\n\u001b[0;32m---> 30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m resource_variable_ops\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_array_ops\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mragged\u001b[39;00m \u001b[39mimport\u001b[39;00m ragged_tensor\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorflow/python/ops/resource_variable_ops.py:41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m errors\n\u001b[1;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m indexed_slices\n\u001b[0;32m---> 41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m meta_graph\n\u001b[1;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m ops\n\u001b[1;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_conversion_registry\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorflow/python/framework/meta_graph.py:34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m error_interpolation\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m graph_io\n\u001b[0;32m---> 34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m importer\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m op_def_registry\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m ops\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorflow/python/framework/importer.py:24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m device \u001b[39mas\u001b[39;00m pydev\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m errors\n\u001b[0;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m function\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m op_def_registry\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m ops\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorflow/python/framework/function.py:34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m array_ops\n\u001b[1;32m     33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m resource_variable_ops\n\u001b[0;32m---> 34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m variable_scope \u001b[39mas\u001b[39;00m vs\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m compat\n\u001b[1;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m function_utils\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorflow/python/ops/variable_scope.py:33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_shape\n\u001b[1;32m     32\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m array_ops\n\u001b[0;32m---> 33\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m init_ops\n\u001b[1;32m     34\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m resource_variable_ops\n\u001b[1;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m variables\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorflow/python/ops/init_ops.py:43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m linalg_ops_impl\n\u001b[1;32m     42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m math_ops\n\u001b[0;32m---> 43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m random_ops\n\u001b[1;32m     44\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecation\n\u001b[1;32m     45\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutil\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdeprecation\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecated\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorflow/python/ops/random_ops.py:25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mframework\u001b[39;00m \u001b[39mimport\u001b[39;00m tensor_util\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m array_ops\n\u001b[0;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m control_flow_ops\n\u001b[1;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m gen_random_ops\n\u001b[1;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m math_ops\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorflow/python/ops/control_flow_ops.py:42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m gen_control_flow_ops\n\u001b[1;32m     41\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m gen_functional_ops\n\u001b[0;32m---> 42\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m gen_logging_ops\n\u001b[1;32m     43\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m gen_math_ops\n\u001b[1;32m     44\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m math_ops\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorflow/python/ops/gen_logging_ops.py:312\u001b[0m\n\u001b[1;32m    308\u001b[0m   _result, \u001b[39m=\u001b[39m _result\n\u001b[1;32m    309\u001b[0m   \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m--> 312\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mimage_summary\u001b[39m(tag, tensor, max_images\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, bad_color\u001b[39m=\u001b[39m_execute\u001b[39m.\u001b[39;49mmake_tensor(\u001b[39m\"\"\"\u001b[39;49m\u001b[39mdtype: DT_UINT8 tensor_shape \u001b[39;49m\u001b[39m{\u001b[39;49m\u001b[39m dim \u001b[39;49m\u001b[39m{\u001b[39;49m\u001b[39m size: 4 } } int_val: 255 int_val: 0 int_val: 0 int_val: 255 \u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mbad_color\u001b[39;49m\u001b[39m\"\u001b[39;49m), name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    313\u001b[0m \u001b[39m  \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Outputs a `Summary` protocol buffer with images.\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \n\u001b[1;32m    315\u001b[0m \u001b[39m  The summary has up to `max_images` summary values containing images. The\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39m    A `Tensor` of type `string`.\u001b[39;00m\n\u001b[1;32m    363\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[1;32m    364\u001b[0m   _ctx \u001b[39m=\u001b[39m _context\u001b[39m.\u001b[39m_context \u001b[39mor\u001b[39;00m _context\u001b[39m.\u001b[39mcontext()\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:218\u001b[0m, in \u001b[0;36mmake_tensor\u001b[0;34m(v, arg_name)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(v, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    217\u001b[0m   pb \u001b[39m=\u001b[39m tensor_pb2\u001b[39m.\u001b[39mTensorProto()\n\u001b[0;32m--> 218\u001b[0m   text_format\u001b[39m.\u001b[39;49mMerge(v, pb)\n\u001b[1;32m    219\u001b[0m   \u001b[39mreturn\u001b[39;00m pb\n\u001b[1;32m    220\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    221\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mDon\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt know how to convert \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to a TensorProto for argument \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m    222\u001b[0m     (\u001b[39mrepr\u001b[39m(v), arg_name))\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/google/protobuf/text_format.py:719\u001b[0m, in \u001b[0;36mMerge\u001b[0;34m(text, message, allow_unknown_extension, allow_field_number, descriptor_pool, allow_unknown_field)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mMerge\u001b[39m(text,\n\u001b[1;32m    691\u001b[0m           message,\n\u001b[1;32m    692\u001b[0m           allow_unknown_extension\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    693\u001b[0m           allow_field_number\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    694\u001b[0m           descriptor_pool\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    695\u001b[0m           allow_unknown_field\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    696\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Parses a text representation of a protocol message into a message.\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \n\u001b[1;32m    698\u001b[0m \u001b[39m  Like Parse(), but allows repeated values for a non-repeated field, and uses\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[39m    ParseError: On text parsing problems.\u001b[39;00m\n\u001b[1;32m    718\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 719\u001b[0m   \u001b[39mreturn\u001b[39;00m MergeLines(\n\u001b[1;32m    720\u001b[0m       text\u001b[39m.\u001b[39;49msplit(\u001b[39mb\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(text, \u001b[39mbytes\u001b[39;49m) \u001b[39melse\u001b[39;49;00m \u001b[39mu\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m    721\u001b[0m       message,\n\u001b[1;32m    722\u001b[0m       allow_unknown_extension,\n\u001b[1;32m    723\u001b[0m       allow_field_number,\n\u001b[1;32m    724\u001b[0m       descriptor_pool\u001b[39m=\u001b[39;49mdescriptor_pool,\n\u001b[1;32m    725\u001b[0m       allow_unknown_field\u001b[39m=\u001b[39;49mallow_unknown_field)\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/google/protobuf/text_format.py:793\u001b[0m, in \u001b[0;36mMergeLines\u001b[0;34m(lines, message, allow_unknown_extension, allow_field_number, descriptor_pool, allow_unknown_field)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Parses a text representation of a protocol message into a message.\u001b[39;00m\n\u001b[1;32m    769\u001b[0m \n\u001b[1;32m    770\u001b[0m \u001b[39mSee Merge() for more details.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[39m  ParseError: On text parsing problems.\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    789\u001b[0m parser \u001b[39m=\u001b[39m _Parser(allow_unknown_extension,\n\u001b[1;32m    790\u001b[0m                  allow_field_number,\n\u001b[1;32m    791\u001b[0m                  descriptor_pool\u001b[39m=\u001b[39mdescriptor_pool,\n\u001b[1;32m    792\u001b[0m                  allow_unknown_field\u001b[39m=\u001b[39mallow_unknown_field)\n\u001b[0;32m--> 793\u001b[0m \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mMergeLines(lines, message)\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/google/protobuf/text_format.py:818\u001b[0m, in \u001b[0;36m_Parser.MergeLines\u001b[0;34m(self, lines, message)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Merges a text representation of a protocol message into a message.\"\"\"\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_allow_multiple_scalars \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 818\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ParseOrMerge(lines, message)\n\u001b[1;32m    819\u001b[0m \u001b[39mreturn\u001b[39;00m message\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/google/protobuf/text_format.py:837\u001b[0m, in \u001b[0;36m_Parser._ParseOrMerge\u001b[0;34m(self, lines, message)\u001b[0m\n\u001b[1;32m    835\u001b[0m tokenizer \u001b[39m=\u001b[39m Tokenizer(str_lines)\n\u001b[1;32m    836\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m tokenizer\u001b[39m.\u001b[39mAtEnd():\n\u001b[0;32m--> 837\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_MergeField(tokenizer, message)\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/google/protobuf/text_format.py:967\u001b[0m, in \u001b[0;36m_Parser._MergeField\u001b[0;34m(self, tokenizer, message)\u001b[0m\n\u001b[1;32m    964\u001b[0m         tokenizer\u001b[39m.\u001b[39mConsume(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    966\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 967\u001b[0m     merger(tokenizer, message, field)\n\u001b[1;32m    969\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# Proto field is unknown.\u001b[39;00m\n\u001b[1;32m    970\u001b[0m   \u001b[39massert\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mallow_unknown_extension \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mallow_unknown_field)\n","File \u001b[0;32m~/.conda/envs/AL/lib/python3.9/site-packages/google/protobuf/text_format.py:1131\u001b[0m, in \u001b[0;36m_Parser._MergeScalarField\u001b[0;34m(self, tokenizer, message, field)\u001b[0m\n\u001b[1;32m   1127\u001b[0m   \u001b[39mraise\u001b[39;00m tokenizer\u001b[39m.\u001b[39mParseErrorPreviousToken(\n\u001b[1;32m   1128\u001b[0m       \u001b[39m'\u001b[39m\u001b[39mMessage type \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m should not have multiple \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m fields.\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1129\u001b[0m       (message\u001b[39m.\u001b[39mDESCRIPTOR\u001b[39m.\u001b[39mfull_name, field\u001b[39m.\u001b[39mname))\n\u001b[1;32m   1130\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1131\u001b[0m   \u001b[39msetattr\u001b[39;49m(message, field\u001b[39m.\u001b[39;49mname, value)\n","\u001b[0;31mTypeError\u001b[0m: descriptor to field 'tensorflow.TensorProto.dtype' doesn't apply to 'TensorProto' object"]}],"source":["args = {\n","    'method_type': 'TA-VAAL', \n","    'dataset': 'celeba',\n","    'group': 'head',\n","    'total': False,\n","    'trials': 1,\n","    'CYCLES': 7,\n","    'ADDENDUM': 16277, ## num of images increment\n","    'INCREMENTAL': 8138, ## \n","    'BATCH': 32, ## \n","    'SUBSET': 20000, ##\n","    'NO_CLASSES': 11, ##\n","    'LR': 1e-4,\n","    'WDECAY': 5e-4,\n","    'MOMENTUM': 0.9,\n","    'MILESTONES': [160, 240],\n","    'MARGIN': 1.0,\n","    'WEIGHT': 1.0,\n","    'no_of_epochs': 20,\n","    'lambda_loss': 1.2,\n","    's_margin': 0.1,\n","    'hidden_units': 128,\n","    'dropout_rate': 0.3,\n","    'EPOCHV':10, \n","    'EPOCHL': 12, \n","    'attempt': 1,\n","    'writer-train': SummaryWriter('logs/TAVAAL-CELEBA-head-1-Train'),\n","    'writer-val': SummaryWriter('logs/TAVAAL-CELEBA-head-1-Val')\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T16:51:27.948414Z","iopub.status.busy":"2023-06-06T16:51:27.947775Z","iopub.status.idle":"2023-06-06T16:51:27.952926Z","shell.execute_reply":"2023-06-06T16:51:27.951956Z","shell.execute_reply.started":"2023-06-06T16:51:27.948361Z"},"trusted":true},"outputs":[],"source":["#Mouth       4 'Big_Lips', 'Mouth_Slightly_Open', 'Smiling', 'Wearing_Lipstick' 19 3 5e-4 1e-4\n","#Eyes        5 'Arched_Eyebrows', 'Bag_Under_Eyes', 'Bushy_Eyebrows', 'Eyeglasses', 'Narrow_Eyes' 19 1 5e-4 1e-4\n","#Face        6 'Attractive', 'Blurry', 'Heavy_Makeup', 'Oval_Face', 'Pale_Skin', 'Young'\n","#Facial Hair 5 '5_o_clock shadow', 'Goatee', 'Moustache', 'No_Beard', 'Sideburns'\n","#Head       11 'Bald', 'Bangs', 'Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair', 'Receding_Hairline', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T16:51:29.832633Z","iopub.status.busy":"2023-06-06T16:51:29.832284Z","iopub.status.idle":"2023-06-06T16:51:30.945926Z","shell.execute_reply":"2023-06-06T16:51:30.944634Z","shell.execute_reply.started":"2023-06-06T16:51:29.832596Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory logs: File exists\n"]}],"source":["!mkdir logs saved_history saved_history/models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-06T16:51:34.344499Z","iopub.status.busy":"2023-06-06T16:51:34.344129Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset: celeba\n","Method type:TA-VAAL\n","The entire datasize is 162770\n"]}],"source":["main(args)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# dataloader -----------> Done\n","# model_architecture \n","# model ----------------> Done\n","# query_samples \n","# hyperparameters\n","\n","# import torch, gc\n","# gc.collect()\n","# torch.cuda.empty_cache()\n","\n","# !nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# summary(a, [(3,96,96)])\n","# train_iterations = int( (args['INCREMENTAL']*cycle+ args['SUBSET']) * args['EPOCHV'] / args['BATCH'] )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# cycle, epoch, total_train_pred_acc, train_pred_1_acc, train_pred_2_acc, train_pred_3_acc, train_pred_4_acc, \n","# total_val_pred_acc, val_pred_1_acc, val_pred_2_acc, val_pred_3_acc, val_pred_4_acc"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
