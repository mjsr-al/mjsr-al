{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:01.772646Z","iopub.status.busy":"2023-06-21T07:04:01.771726Z","iopub.status.idle":"2023-06-21T07:04:01.776924Z","shell.execute_reply":"2023-06-21T07:04:01.775645Z","shell.execute_reply.started":"2023-06-21T07:04:01.772602Z"},"trusted":true},"outputs":[],"source":["# !git clone https://github.com/cubeyoung/TA-VAAL.git"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:02.137626Z","iopub.status.busy":"2023-06-21T07:04:02.137320Z","iopub.status.idle":"2023-06-21T07:04:02.141770Z","shell.execute_reply":"2023-06-21T07:04:02.140716Z","shell.execute_reply.started":"2023-06-21T07:04:02.137585Z"},"trusted":true},"outputs":[],"source":["# !python3 TA-VAAL/main.py -m TA-VAAL -d cifar10 -c 3"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## CelebA"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:03.096734Z","iopub.status.busy":"2023-06-21T07:04:03.096390Z","iopub.status.idle":"2023-06-21T07:04:03.101608Z","shell.execute_reply":"2023-06-21T07:04:03.100646Z","shell.execute_reply.started":"2023-06-21T07:04:03.096694Z"},"trusted":true},"outputs":[],"source":["# import os\n","# import math\n","# import random\n","# import torch\n","# import numpy as np\n","# from tqdm import tqdm\n","# import torch.nn as nn\n","# import torch.optim as optim\n","# import torch.nn.init as init\n","# import torch.nn.functional as F\n","# from torch.distributions import Normal\n","# from torch.nn.parameter import Parameter\n","# from torch.nn.modules.module import Module\n","# from torch.utils.data import DataLoader, Dataset\n","# import torch.optim.lr_scheduler as lr_scheduler\n","# from torch.utils.data.sampler import SubsetRandomSampler\n","# from torchvision.datasets import CIFAR100, CIFAR10, FashionMNIST, SVHN\n","# import torchvision.transforms as T\n","# import torchvision.models as models\n","# import argparse\n","# from sklearn.metrics import confusion_matrix"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:03.442882Z","iopub.status.busy":"2023-06-21T07:04:03.441896Z","iopub.status.idle":"2023-06-21T07:04:03.448345Z","shell.execute_reply":"2023-06-21T07:04:03.447524Z","shell.execute_reply.started":"2023-06-21T07:04:03.442829Z"},"trusted":true},"outputs":[],"source":["# class SubsetSequentialSampler(torch.utils.data.Sampler):\n","#     r\"\"\"Samples elements sequentially from a given list of indices, without replacement.\n","\n","#     Arguments:\n","#         indices (sequence): a sequence of indices\n","#     \"\"\"\n","\n","#     def __init__(self, indices):\n","#         self.indices = indices\n","\n","#     def __iter__(self):\n","#         return (self.indices[i] for i in range(len(self.indices)))\n","    \n","#     def __len__(self):\n","#         return len(self.indices)\n","\n","# class MyDataset(Dataset):\n","#     def __init__(self, dataset_name, train_flag, transf):\n","#         self.dataset_name = dataset_name\n","#         if self.dataset_name == \"cifar10\":\n","#             self.cifar10 = CIFAR10('../cifar10', train=train_flag, \n","#                                     download=True, transform=transf)\n","\n","#     def __getitem__(self, index):\n","#         if self.dataset_name == \"cifar10\":\n","#             data, target = self.cifar10[index]\n","#         return data, target, index\n","\n","#     def __len__(self):\n","#         if self.dataset_name == \"cifar10\":\n","#             return len(self.cifar10)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:03.733061Z","iopub.status.busy":"2023-06-21T07:04:03.732754Z","iopub.status.idle":"2023-06-21T07:04:03.739135Z","shell.execute_reply":"2023-06-21T07:04:03.738101Z","shell.execute_reply.started":"2023-06-21T07:04:03.733030Z"},"trusted":true},"outputs":[],"source":["# # Data\n","# def load_dataset(dataset):\n","\n","#     NUM_TRAIN = 50000 \n","#     BATCH     = 128 # B\n","#     SUBSET    = 10000 # M\n","#     ADDENDUM  = 1000 # K\n","#     TRIALS = 5\n","#     CYCLES = 10\n","    \n","#     train_transform = T.Compose([\n","#         T.RandomHorizontalFlip(),\n","#         T.RandomCrop(size=32, padding=4),\n","#         T.ToTensor(),\n","#         T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]) \n","#     ])\n","\n","#     test_transform = T.Compose([\n","#         T.ToTensor(),\n","#         T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]) \n","#     ])\n","\n","#     if dataset == 'cifar10': \n","#         data_train = CIFAR10('../cifar10', train=True, download=True, transform=train_transform)\n","#         data_unlabeled = MyDataset(dataset, True, test_transform)\n","#         data_test  = CIFAR10('../cifar10', train=False, download=True, transform=test_transform)\n","#         NO_CLASSES = 10\n","#         adden = ADDENDUM\n","#         NUM_TRAIN = len(data_train)        \n","#         no_train = NUM_TRAIN\n","\n","#     return data_train, data_unlabeled, data_test, adden, NO_CLASSES, no_train"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:04.031708Z","iopub.status.busy":"2023-06-21T07:04:04.031401Z","iopub.status.idle":"2023-06-21T07:04:04.039501Z","shell.execute_reply":"2023-06-21T07:04:04.038403Z","shell.execute_reply.started":"2023-06-21T07:04:04.031676Z"},"trusted":true},"outputs":[],"source":["# class BasicBlock(nn.Module):\n","#     expansion = 1\n","\n","#     def __init__(self, in_planes, planes, stride=1):\n","#         super(BasicBlock, self).__init__()\n","#         self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","#         self.bn1 = nn.BatchNorm2d(planes)\n","#         self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","#         self.bn2 = nn.BatchNorm2d(planes)\n","\n","#         self.shortcut = nn.Sequential()\n","#         if stride != 1 or in_planes != self.expansion*planes:\n","#             self.shortcut = nn.Sequential(\n","#                 nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","#                 nn.BatchNorm2d(self.expansion*planes)\n","#             )\n","\n","#     def forward(self, x):\n","#         out = F.relu(self.bn1(self.conv1(x)))\n","#         out = self.bn2(self.conv2(out))\n","#         out += self.shortcut(x)\n","#         out = F.relu(out)\n","#         return out\n","\n","# class ResNet(nn.Module):\n","#     def __init__(self, block, num_blocks, num_classes=10):\n","#         super(ResNet, self).__init__()\n","#         self.in_planes = 64\n","\n","#         self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","#         self.bn1 = nn.BatchNorm2d(64)\n","#         self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","#         self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","#         self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","#         self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","#         self.linear = nn.Linear(512*block.expansion, num_classes)\n","#         # self.linear2 = nn.Linear(1000, num_classes)\n","\n","#     def _make_layer(self, block, planes, num_blocks, stride):\n","#         strides = [stride] + [1]*(num_blocks-1)\n","#         layers = []\n","#         for stride in strides:\n","#             layers.append(block(self.in_planes, planes, stride))\n","#             self.in_planes = planes * block.expansion\n","#         return nn.Sequential(*layers)\n","\n","#     def forward(self, x):\n","#         out = F.relu(self.bn1(self.conv1(x)))\n","#         out1 = self.layer1(out)\n","#         out2 = self.layer2(out1)\n","#         out3 = self.layer3(out2)\n","#         out4 = self.layer4(out3)\n","#         out = F.avg_pool2d(out4, 4)\n","#         outf = out.view(out.size(0), -1)\n","#         # outl = self.linear(outf)\n","#         out = self.linear(outf)\n","#         return out, outf, [out1, out2, out3, out4]\n","\n","# def ResNet18(num_classes = 10):\n","#     return ResNet(BasicBlock, [2,2,2,2], num_classes)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:04.350557Z","iopub.status.busy":"2023-06-21T07:04:04.350242Z","iopub.status.idle":"2023-06-21T07:04:04.362381Z","shell.execute_reply":"2023-06-21T07:04:04.360942Z","shell.execute_reply.started":"2023-06-21T07:04:04.350526Z"},"trusted":true},"outputs":[],"source":["# class LossNet(nn.Module):\n","#     def __init__(self, feature_sizes=[32, 16, 8, 4], num_channels=[64, 128, 256, 512], interm_dim=128):\n","#         super(LossNet, self).__init__()\n","        \n","#         self.GAP1 = nn.AvgPool2d(feature_sizes[0])\n","#         self.GAP2 = nn.AvgPool2d(feature_sizes[1])\n","#         self.GAP3 = nn.AvgPool2d(feature_sizes[2])\n","#         self.GAP4 = nn.AvgPool2d(feature_sizes[3])\n","\n","#         self.FC1 = nn.Linear(num_channels[0], interm_dim)\n","#         self.FC2 = nn.Linear(num_channels[1], interm_dim)\n","#         self.FC3 = nn.Linear(num_channels[2], interm_dim)\n","#         self.FC4 = nn.Linear(num_channels[3], interm_dim)\n","\n","#         self.linear = nn.Linear(4 * interm_dim, 1)\n","    \n","#     def forward(self, features):\n","#         out1 = self.GAP1(features[0])\n","#         out1 = out1.view(out1.size(0), -1)\n","#         out1 = F.relu(self.FC1(out1))\n","\n","#         out2 = self.GAP2(features[1])\n","#         out2 = out2.view(out2.size(0), -1)\n","#         out2 = F.relu(self.FC2(out2))\n","\n","#         out3 = self.GAP3(features[2])\n","#         out3 = out3.view(out3.size(0), -1)\n","#         out3 = F.relu(self.FC3(out3))\n","\n","#         out4 = self.GAP4(features[3])\n","#         out4 = out4.view(out4.size(0), -1)\n","#         out4 = F.relu(self.FC4(out4))\n","\n","#         out = self.linear(torch.cat((out1, out2, out3, out4), 1))\n","#         return out\n","\n","# class View(nn.Module):\n","#     def __init__(self, size):\n","#         super(View, self).__init__()\n","#         self.size = size\n","\n","#     def forward(self, tensor):\n","#         return tensor.view(self.size)\n","\n","# class VAE(nn.Module):\n","#     \"\"\"Encoder-Decoder architecture for both WAE-MMD and WAE-GAN.\"\"\"\n","#     def __init__(self, z_dim=32, nc=3, f_filt=4):\n","#         super(VAE, self).__init__()\n","#         self.z_dim = z_dim\n","#         self.nc = nc\n","#         self.f_filt = f_filt\n","#         self.encoder = nn.Sequential(\n","#             nn.Conv2d(nc, 128, 4, 2, 1, bias=False),              # B,  128, 32, 32\n","#             nn.BatchNorm2d(128),\n","#             nn.ReLU(True),\n","#             nn.Conv2d(128, 256, 4, 2, 1, bias=False),             # B,  256, 16, 16\n","#             nn.BatchNorm2d(256),\n","#             nn.ReLU(True),\n","#             nn.Conv2d(256, 512, 4, 2, 1, bias=False),             # B,  512,  8,  8\n","#             nn.BatchNorm2d(512),\n","#             nn.ReLU(True),\n","#             nn.Conv2d(512, 1024, self.f_filt, 2, 1, bias=False),            # B, 1024,  4,  4\n","#             nn.BatchNorm2d(1024),\n","#             nn.ReLU(True),\n","#             View((-1, 1024*2*2)),                                 # B, 1024*4*4\n","#         )\n","\n","#         self.fc_mu = nn.Linear(1024*2*2, z_dim)                            # B, z_dim\n","#         self.fc_logvar = nn.Linear(1024*2*2, z_dim)                            # B, z_dim\n","#         self.decoder = nn.Sequential(\n","#             nn.Linear(z_dim + 1, 1024*4*4),                           # B, 1024*8*8\n","#             View((-1, 1024, 4, 4)),                               # B, 1024,  8,  8\n","#             nn.ConvTranspose2d(1024, 512, self.f_filt, 2, 1, bias=False),   # B,  512, 16, 16\n","#             nn.BatchNorm2d(512),\n","#             nn.ReLU(True),\n","#             nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),    # B,  256, 32, 32\n","#             nn.BatchNorm2d(256),\n","#             nn.ReLU(True),\n","#             nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),    # B,  128, 64, 64\n","#             nn.BatchNorm2d(128),\n","#             nn.ReLU(True),\n","#             nn.ConvTranspose2d(128, nc, 1),                       # B,   nc, 64, 64\n","#         )\n","#         self.weight_init()\n","\n","#     def weight_init(self):\n","#         for block in self._modules:\n","#             try:\n","#                 for m in self._modules[block]:\n","#                     kaiming_init(m)\n","#             except:\n","#                 kaiming_init(block)\n","\n","#     def forward(self, r,x):\n","#         z = self._encode(x)\n","#         mu, logvar = self.fc_mu(z), self.fc_logvar(z)\n","#         z = self.reparameterize(mu, logvar)\n","#         z = torch.cat([z,r],1)\n","#         x_recon = self._decode(z)\n","\n","#         return x_recon, z, mu, logvar\n","\n","#     def reparameterize(self, mu, logvar):\n","#         stds = (0.5 * logvar).exp()\n","#         epsilon = torch.randn(*mu.size())\n","#         if mu.is_cuda:\n","#             stds, epsilon = stds.cuda(), epsilon.cuda()\n","#         latents = epsilon * stds + mu\n","#         return latents\n","\n","#     def _encode(self, x):\n","#         return self.encoder(x)\n","\n","#     def _decode(self, z):\n","#         return self.decoder(z)\n","\n","# class Discriminator(nn.Module):\n","#     \"\"\"Adversary architecture(Discriminator) for WAE-GAN.\"\"\"\n","#     def __init__(self, z_dim=10):\n","#         super(Discriminator, self).__init__()\n","#         self.z_dim = z_dim\n","#         self.net = nn.Sequential(\n","#             nn.Linear(z_dim + 1, 512),\n","#             nn.ReLU(True),\n","#             nn.Linear(512, 512),\n","#             nn.ReLU(True),\n","#             nn.Linear(512, 1),\n","#             nn.Sigmoid()\n","#         )\n","#         self.weight_init()\n","\n","#     def weight_init(self):\n","#         for block in self._modules:\n","#             for m in self._modules[block]:\n","#                 kaiming_init(m)\n","\n","#     def forward(self, r,z):  \n","#         z = torch.cat([z, r], 1)\n","#         return self.net(z)\n","\n","# def kaiming_init(m):\n","#     if isinstance(m, (nn.Linear, nn.Conv2d)):\n","#         init.kaiming_normal(m.weight)\n","#         if m.bias is not None:\n","#             m.bias.data.fill_(0)\n","#     elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n","#         m.weight.data.fill_(1)\n","#         if m.bias is not None:\n","#             m.bias.data.fill_(0)"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:04.672481Z","iopub.status.busy":"2023-06-21T07:04:04.672092Z","iopub.status.idle":"2023-06-21T07:04:04.683508Z","shell.execute_reply":"2023-06-21T07:04:04.682449Z","shell.execute_reply.started":"2023-06-21T07:04:04.672440Z"},"trusted":true},"outputs":[],"source":["# # Loss Prediction Loss\n","# def LossPredLoss(input, target, margin=1.0, reduction='mean'):\n","#     assert len(input) % 2 == 0, 'the batch size is not even.'\n","#     assert input.shape == input.flip(0).shape\n","#     criterion = nn.BCELoss()\n","#     input = (input - input.flip(0))[:len(input)//2] # [l_1 - l_2B, l_2 - l_2B-1, ... , l_B - l_B+1], where batch_size = 2B\n","#     target = (target - target.flip(0))[:len(target)//2]\n","#     target = target.detach()\n","#     diff = torch.sigmoid(input)\n","#     one = torch.sign(torch.clamp(target, min=0)) # 1 operation which is defined by the authors\n","    \n","#     if reduction == 'mean':\n","#         loss = criterion(diff,one)\n","#     elif reduction == 'none':\n","#         loss = criterion(diff,one)\n","#     else:\n","#         NotImplementedError()\n","    \n","#     return loss\n","\n","\n","# def test(models, epoch, method, dataloaders, mode='val'):\n","    \n","#     CUDA_VISIBLE_DEVICES = 0\n","#     BATCH     = 128 # B\n","#     MARGIN = 1.0 # xi\n","#     WEIGHT = 1.0 # lambda\n","    \n","#     assert mode == 'val' or mode == 'test'\n","#     models['backbone'].eval()\n","#     if method == 'lloss':\n","#         models['module'].eval()\n","    \n","#     total = 0\n","#     correct = 0\n","#     with torch.no_grad():\n","#         for (inputs, labels) in dataloaders[mode]:\n","#             with torch.cuda.device(CUDA_VISIBLE_DEVICES):\n","#                 inputs = inputs.cuda()\n","#                 labels = labels.cuda()\n","\n","#             scores, _, _ = models['backbone'](inputs)\n","#             _, preds = torch.max(scores.data, 1)\n","#             total += labels.size(0)\n","#             correct += (preds == labels).sum().item()\n","    \n","#     return 100 * correct / total\n","\n","# iters = 0\n","# def train_epoch(models, method, criterion, optimizers, dataloaders, epoch, epoch_loss):\n","    \n","#     CUDA_VISIBLE_DEVICES = 0\n","#     BATCH     = 128 # B\n","#     MARGIN = 1.0 # xi\n","#     WEIGHT = 1.0 # lambda\n","    \n","#     models['backbone'].train()\n","#     if method == 'lloss' or 'TA-VAAL':\n","#         models['module'].train()\n","#     global iters\n","#     for data in tqdm(dataloaders['train'], leave=False, total=len(dataloaders['train'])):\n","#         with torch.cuda.device(CUDA_VISIBLE_DEVICES):\n","#             inputs = data[0].cuda()\n","#             labels = data[1].cuda()\n","\n","#         iters += 1\n","\n","#         optimizers['backbone'].zero_grad()\n","#         if method == 'lloss' or 'TA-VAAL':\n","#             optimizers['module'].zero_grad()\n","\n","#         scores, _, features = models['backbone'](inputs) \n","#         target_loss = criterion(scores, labels)\n","\n","#         if method == 'lloss' or 'TA-VAAL':\n","#             if epoch > epoch_loss:\n","#                 features[0] = features[0].detach()\n","#                 features[1] = features[1].detach()\n","#                 features[2] = features[2].detach()\n","#                 features[3] = features[3].detach()\n","\n","#             pred_loss = models['module'](features)\n","#             pred_loss = pred_loss.view(pred_loss.size(0))\n","#             m_module_loss   = LossPredLoss(pred_loss, target_loss, margin=MARGIN)\n","#             m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)        \n","#             loss            = m_backbone_loss + WEIGHT * m_module_loss \n","#         else:\n","#             m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)        \n","#             loss            = m_backbone_loss\n","\n","#         loss.backward()\n","#         optimizers['backbone'].step()\n","#         if method == 'lloss' or 'TA-VAAL':\n","#             optimizers['module'].step()\n","#     return loss\n","\n","# def train(models, method, criterion, optimizers, schedulers, dataloaders, num_epochs, epoch_loss):\n","#     print('>> Train a Model.')\n","#     best_acc = 0.\n","    \n","#     for epoch in range(num_epochs):\n","\n","#         best_loss = torch.tensor([0.5]).cuda()\n","#         loss = train_epoch(models, method, criterion, optimizers, dataloaders, epoch, epoch_loss)\n","\n","#         schedulers['backbone'].step()\n","#         if method == 'lloss' or 'TA-VAAL':\n","#             schedulers['module'].step()\n","\n","#         if True and epoch % 20  == 7:\n","#             acc = test(models, epoch, method, dataloaders, mode='test')\n","#             # acc = test(models, dataloaders, mc, 'test')\n","#             if best_acc < acc:\n","#                 best_acc = acc\n","#                 print('Val Acc: {:.3f} \\t Best Acc: {:.3f}'.format(acc, best_acc))\n","#     print('>> Finished.')"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:04.985916Z","iopub.status.busy":"2023-06-21T07:04:04.985541Z","iopub.status.idle":"2023-06-21T07:04:05.001917Z","shell.execute_reply":"2023-06-21T07:04:05.000849Z","shell.execute_reply.started":"2023-06-21T07:04:04.985863Z"},"trusted":true},"outputs":[],"source":["# def read_data(dataloader, labels=True):\n","#     if labels:\n","#         while True:\n","#             for img, label,_ in dataloader:\n","#                 yield img, label\n","#     else:\n","#         while True:\n","#             for img, _, _ in dataloader:\n","#                 yield img\n","\n","# def vae_loss(x, recon, mu, logvar, beta):\n","#     mse_loss = nn.MSELoss()\n","#     MSE = mse_loss(recon, x)\n","#     KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n","#     KLD = KLD * beta\n","#     return MSE + KLD\n","\n","# def train_vaal(models, optimizers, labeled_dataloader, unlabeled_dataloader, cycle):\n","    \n","#     CUDA_VISIBLE_DEVICES = 0\n","#     NUM_TRAIN = 50000 # N \\Fashion MNIST 60000, cifar 10/100 50000\n","#     # NUM_VAL   = 50000 - NUM_TRAIN\n","#     BATCH     = 128 # B\n","#     SUBSET    = 10000 # M\n","#     ADDENDUM  = 1000 # K\n","\n","#     MARGIN = 1.0 # xi\n","#     WEIGHT = 1.0 # lambda\n","\n","#     TRIALS = 5\n","#     CYCLES = 10\n","\n","#     EPOCH = 200\n","#     EPOCH_GCN = 200\n","#     LR = 1e-1\n","#     LR_GCN = 1e-3\n","#     MILESTONES = [160, 240]\n","#     EPOCHL = 120#20 #120 # After 120 epochs, stop \n","#     EPOCHV = 100 # VAAL number of epochs\n","\n","#     MOMENTUM = 0.9\n","#     WDECAY = 5e-4#2e-3# 5e-4\n","    \n","#     vae = models['vae']\n","#     discriminator = models['discriminator']\n","#     task_model = models['backbone']\n","#     ranker = models['module']\n","    \n","#     task_model.eval()\n","#     ranker.eval()\n","#     vae.train()\n","#     discriminator.train()\n","#     with torch.cuda.device(CUDA_VISIBLE_DEVICES):\n","#         vae = vae.cuda()\n","#         discriminator = discriminator.cuda()\n","#         task_model = task_model.cuda()\n","#         ranker = ranker.cuda()\n","#     adversary_param = 1\n","#     beta          = 1\n","#     num_adv_steps = 1\n","#     num_vae_steps = 1\n","\n","#     bce_loss = nn.BCELoss()\n","    \n","#     labeled_data = read_data(labeled_dataloader)\n","#     unlabeled_data = read_data(unlabeled_dataloader)\n","\n","#     train_iterations = int( (ADDENDUM*cycle+ SUBSET) * EPOCHV / BATCH )\n","    \n","#     for iter_count in range(train_iterations):\n","#         labeled_imgs, labels = next(labeled_data)\n","#         unlabeled_imgs = next(unlabeled_data)[0]\n","\n","#         with torch.cuda.device(CUDA_VISIBLE_DEVICES):\n","#             labeled_imgs = labeled_imgs.cuda()\n","#             unlabeled_imgs = unlabeled_imgs.cuda()\n","#             labels = labels.cuda()\n","#         if iter_count == 0 :\n","#             r_l_0 = torch.from_numpy(np.random.uniform(0, 1, size=(labeled_imgs.shape[0],1))).type(torch.FloatTensor).cuda()\n","#             r_u_0 = torch.from_numpy(np.random.uniform(0, 1, size=(unlabeled_imgs.shape[0],1))).type(torch.FloatTensor).cuda()\n","#         else:\n","#             with torch.no_grad():\n","#                 _,_,features_l = task_model(labeled_imgs)\n","#                 _,_,feature_u = task_model(unlabeled_imgs)\n","#                 r_l = ranker(features_l)\n","#                 r_u = ranker(feature_u)\n","#         if iter_count == 0:\n","#             r_l = r_l_0.detach()\n","#             r_u = r_u_0.detach()\n","#             r_l_s = r_l_0.detach()\n","#             r_u_s = r_u_0.detach()\n","#         else:\n","#             r_l_s = torch.sigmoid(r_l).detach()\n","#             r_u_s = torch.sigmoid(r_u).detach()                 \n","#         # VAE step\n","#         for count in range(num_vae_steps): # num_vae_steps\n","#             recon, _, mu, logvar = vae(r_l_s,labeled_imgs)\n","#             unsup_loss = vae_loss(labeled_imgs, recon, mu, logvar, beta)\n","#             unlab_recon, _, unlab_mu, unlab_logvar = vae(r_u_s,unlabeled_imgs)\n","#             transductive_loss = vae_loss(unlabeled_imgs, \n","#                     unlab_recon, unlab_mu, unlab_logvar, beta)\n","        \n","#             labeled_preds = discriminator(r_l,mu)\n","#             unlabeled_preds = discriminator(r_u,unlab_mu)\n","            \n","#             lab_real_preds = torch.ones(labeled_imgs.size(0))\n","#             unlab_real_preds = torch.ones(unlabeled_imgs.size(0))\n","                \n","#             with torch.cuda.device(CUDA_VISIBLE_DEVICES):\n","#                 lab_real_preds = lab_real_preds.cuda()\n","#                 unlab_real_preds = unlab_real_preds.cuda()\n","\n","#             dsc_loss = bce_loss(labeled_preds[:,0], lab_real_preds) + \\\n","#                        bce_loss(unlabeled_preds[:,0], unlab_real_preds)\n","#             total_vae_loss = unsup_loss + transductive_loss + adversary_param * dsc_loss\n","            \n","#             optimizers['vae'].zero_grad()\n","#             total_vae_loss.backward()\n","#             optimizers['vae'].step()\n","\n","#             # sample new batch if needed to train the adversarial network\n","#             if count < (num_vae_steps - 1):\n","#                 labeled_imgs, _ = next(labeled_data)\n","#                 unlabeled_imgs = next(unlabeled_data)[0]\n","\n","#                 with torch.cuda.device(CUDA_VISIBLE_DEVICES):\n","#                     labeled_imgs = labeled_imgs.cuda()\n","#                     unlabeled_imgs = unlabeled_imgs.cuda()\n","#                     labels = labels.cuda()\n","\n","#         # Discriminator step\n","#         for count in range(num_adv_steps):\n","#             with torch.no_grad():\n","#                 _, _, mu, _ = vae(r_l_s,labeled_imgs)\n","#                 _, _, unlab_mu, _ = vae(r_u_s,unlabeled_imgs)\n","            \n","#             labeled_preds = discriminator(r_l,mu)\n","#             unlabeled_preds = discriminator(r_u,unlab_mu)\n","            \n","#             lab_real_preds = torch.ones(labeled_imgs.size(0))\n","#             unlab_fake_preds = torch.zeros(unlabeled_imgs.size(0))\n","\n","#             with torch.cuda.device(CUDA_VISIBLE_DEVICES):\n","#                 lab_real_preds = lab_real_preds.cuda()\n","#                 unlab_fake_preds = unlab_fake_preds.cuda()\n","            \n","#             dsc_loss = bce_loss(labeled_preds[:,0], lab_real_preds) + \\\n","#                        bce_loss(unlabeled_preds[:,0], unlab_fake_preds)\n","\n","#             optimizers['discriminator'].zero_grad()\n","#             dsc_loss.backward()\n","#             optimizers['discriminator'].step()\n","\n","#             # sample new batch if needed to train the adversarial network\n","#             if count < (num_adv_steps-1):\n","#                 labeled_imgs, _ = next(labeled_data)\n","#                 unlabeled_imgs = next(unlabeled_data)[0]\n","\n","#                 with torch.cuda.device(CUDA_VISIBLE_DEVICES):\n","#                     labeled_imgs = labeled_imgs.cuda()\n","#                     unlabeled_imgs = unlabeled_imgs.cuda()\n","#                     labels = labels.cuda()\n","#             if iter_count % 100 == 0:\n","#                 print(\"Iteration: \" + str(iter_count) + \"  vae_loss: \" + str(total_vae_loss.item()) + \" dsc_loss: \" +str(dsc_loss.item()))\n","\n","# # Select the indices of the unlablled data according to the methods\n","# def query_samples(model, method, data_unlabeled, subset, labeled_set, cycle, args):\n","    \n","#     CUDA_VISIBLE_DEVICES = 0\n","#     NUM_TRAIN = 50000 # N \\Fashion MNIST 60000, cifar 10/100 50000\n","#     # NUM_VAL   = 50000 - NUM_TRAIN\n","#     BATCH     = 128 # B\n","#     SUBSET    = 10000 # M\n","#     ADDENDUM  = 1000 # K\n","\n","#     MARGIN = 1.0 # xi\n","#     WEIGHT = 1.0 # lambda\n","\n","#     TRIALS = 5\n","#     CYCLES = 10\n","\n","#     EPOCH = 200\n","#     EPOCH_GCN = 200\n","#     LR = 1e-1\n","#     LR_GCN = 1e-3\n","#     MILESTONES = [160, 240]\n","#     EPOCHL = 120#20 #120 # After 120 epochs, stop \n","#     EPOCHV = 100 # VAAL number of epochs\n","\n","#     MOMENTUM = 0.9\n","#     WDECAY = 5e-4#2e-3# 5e-4\n","\n","#     if method == 'Random':\n","#         arg = np.random.randint(SUBSET, size=SUBSET)\n","\n","#     if method == 'TA-VAAL':\n","#         # Create unlabeled dataloader for the unlabeled subset\n","#         unlabeled_loader = DataLoader(data_unlabeled, batch_size=BATCH, \n","#                                     sampler=SubsetSequentialSampler(subset), \n","#                                     pin_memory=True)\n","#         labeled_loader = DataLoader(data_unlabeled, batch_size=BATCH, \n","#                                     sampler=SubsetSequentialSampler(labeled_set), \n","#                                     pin_memory=True)\n","        \n","#         vae = VAE()\n","#         discriminator = Discriminator(32)\n","     \n","#         models      = {'backbone': model['backbone'], 'module': model['module'],'vae': vae, 'discriminator': discriminator}\n","        \n","#         optim_vae = optim.Adam(vae.parameters(), lr=5e-4)\n","#         optim_discriminator = optim.Adam(discriminator.parameters(), lr=5e-4)\n","#         optimizers = {'vae': optim_vae, 'discriminator':optim_discriminator}\n","\n","#         train_vaal(models,optimizers, labeled_loader, unlabeled_loader, cycle+1)\n","#         task_model = models['backbone']\n","#         ranker = models['module']        \n","#         all_preds, all_indices = [], []\n","\n","#         for images, _, indices in unlabeled_loader:                       \n","#             images = images.cuda()\n","#             with torch.no_grad():\n","#                 _,_,features = task_model(images)\n","#                 r = ranker(features)\n","#                 _, _, mu, _ = vae(torch.sigmoid(r),images)\n","#                 preds = discriminator(r,mu)\n","\n","#             preds = preds.cpu().data\n","#             all_preds.extend(preds)\n","#             all_indices.extend(indices)\n","\n","#         all_preds = torch.stack(all_preds)\n","#         all_preds = all_preds.view(-1)\n","#         # need to multiply by -1 to be able to use torch.topk \n","#         all_preds *= -1\n","#         # select the points which the discriminator things are the most likely to be unlabeled\n","#         _, arg = torch.sort(all_preds) \n","#     return arg"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:05.287725Z","iopub.status.busy":"2023-06-21T07:04:05.287383Z","iopub.status.idle":"2023-06-21T07:04:05.299244Z","shell.execute_reply":"2023-06-21T07:04:05.297980Z","shell.execute_reply.started":"2023-06-21T07:04:05.287690Z"},"trusted":true},"outputs":[],"source":["# # Main\n","# def main(args):\n","    \n","#     CUDA_VISIBLE_DEVICES = 0\n","#     NUM_TRAIN = 50000 # N \\Fashion MNIST 60000, cifar 10/100 50000\n","#     # NUM_VAL   = 50000 - NUM_TRAIN\n","#     BATCH     = 128 # B\n","#     SUBSET    = 10000 # M\n","#     ADDENDUM  = 1000 # K\n","\n","#     MARGIN = 1.0 # xi\n","#     WEIGHT = 1.0 # lambda\n","\n","#     TRIALS = 5\n","#     CYCLES = 10\n","\n","#     EPOCH = 200\n","#     EPOCH_GCN = 200\n","#     LR = 1e-1\n","#     LR_GCN = 1e-3\n","#     MILESTONES = [160, 240]\n","#     EPOCHL = 120#20 #120 # After 120 epochs, stop \n","#     EPOCHV = 100 # VAAL number of epochs\n","\n","#     MOMENTUM = 0.9\n","#     WDECAY = 5e-4#2e-3# 5e-4\n","    \n","#     ## ------------------------------------------------------------------------------------------\n","    \n","#     method = args['method_type']\n","#     methods = ['Random', 'UncertainGCN', 'CoreGCN', 'CoreSet', 'lloss','VAAL','TA-VAAL']\n","#     datasets = ['cifar10','cifar10im', 'cifar100', 'fashionmnist','svhn']\n","#     assert method in methods, 'No method %s! Try options %s'%(method, methods)\n","#     assert args['dataset'] in datasets, 'No dataset %s! Try options %s'%(args['dataset'], datasets)\n","#     '''\n","#     method_type: 'Random', 'UncertainGCN', 'CoreGCN', 'CoreSet', 'lloss','VAAL','TA-VAAL'\n","#     '''\n","#     results = open('results_'+str(args['method_type'])+\"_\"+args['dataset'] +'_main'+str(args['cycles'])+str(args['total'])+'.txt','w')\n","#     print(\"Dataset: %s\"%args['dataset'])\n","#     print(\"Method type:%s\"%method)\n","    \n","#     if args['total']:\n","#         TRIALS = 1\n","#         CYCLES = 1\n","#     else:\n","#         CYCLES = args['cycles']\n","    \n","#     for trial in range(TRIALS):\n","\n","#         # Load training and testing dataset\n","#         data_train, data_unlabeled, data_test, adden, NO_CLASSES, no_train = load_dataset(args['dataset'])\n","#         print('The entire datasize is {}'.format(len(data_train)))       \n","#         ADDENDUM = adden\n","#         NUM_TRAIN = no_train\n","#         indices = list(range(NUM_TRAIN))\n","#         random.shuffle(indices)\n","\n","#         if args['total']:\n","#             labeled_set= indices\n","#         else:\n","#             labeled_set = indices[:ADDENDUM]\n","#             unlabeled_set = [x for x in indices if x not in labeled_set]\n","\n","#         train_loader = DataLoader(data_train, batch_size=BATCH, \n","#                                     sampler=SubsetRandomSampler(labeled_set), \n","#                                     pin_memory=True, drop_last=True)\n","#         test_loader  = DataLoader(data_test, batch_size=BATCH)\n","#         dataloaders  = {'train': train_loader, 'test': test_loader}\n","        \n","#         for cycle in range(CYCLES):\n","            \n","#             # Randomly sample 10000 unlabeled data points\n","#             if not args['total']:\n","#                 random.shuffle(unlabeled_set)\n","#                 subset = unlabeled_set[:SUBSET]\n","                \n","#             # Model - create new instance for every cycle so that it resets\n","#             with torch.cuda.device(CUDA_VISIBLE_DEVICES):\n","#                 #resnet18    = vgg11().cuda() \n","#                 resnet18    = ResNet18(num_classes=NO_CLASSES).cuda()\n","#                 if method == 'lloss' or 'TA-VAAL':\n","#                     #loss_module = LossNet(feature_sizes=[16,8,4,2], num_channels=[128,128,256,512]).cuda()\n","#                     loss_module = LossNet().cuda()\n","\n","#             models      = {'backbone': resnet18}\n","#             if method =='lloss' or 'TA-VAAL':\n","#                 models = {'backbone': resnet18, 'module': loss_module}\n","#             torch.backends.cudnn.benchmark = True\n","            \n","#             # Loss, criterion and scheduler (re)initialization\n","#             criterion      = nn.CrossEntropyLoss(reduction='none')\n","#             optim_backbone = optim.SGD(models['backbone'].parameters(), lr=LR, \n","#                 momentum=MOMENTUM, weight_decay=WDECAY)\n"," \n","#             sched_backbone = lr_scheduler.MultiStepLR(optim_backbone, milestones=MILESTONES)\n","#             optimizers = {'backbone': optim_backbone}\n","#             schedulers = {'backbone': sched_backbone}\n","#             if method == 'lloss' or 'TA-VAAL':\n","#                 optim_module   = optim.SGD(models['module'].parameters(), lr=LR, \n","#                     momentum=MOMENTUM, weight_decay=WDECAY)\n","#                 sched_module   = lr_scheduler.MultiStepLR(optim_module, milestones=MILESTONES)\n","#                 optimizers = {'backbone': optim_backbone, 'module': optim_module}\n","#                 schedulers = {'backbone': sched_backbone, 'module': sched_module}\n","                \n","#             # Training and testing\n","#             train(models, method, criterion, optimizers, schedulers, dataloaders, args['no_of_epochs'], EPOCHL)\n","#             acc = test(models, EPOCH, method, dataloaders, mode='test')\n","#             print('Trial {}/{} || Cycle {}/{} || Label set size {}: Test acc {}'.format(trial+1, TRIALS, cycle+1, CYCLES, len(labeled_set), acc))\n","#             np.array([method, trial+1, TRIALS, cycle+1, CYCLES, len(labeled_set), acc]).tofile(results, sep=\" \")\n","#             results.write(\"\\n\")\n","            \n","#             if cycle == (CYCLES-1):\n","#                 # Reached final training cycle\n","#                 print(\"Finished.\")\n","#                 break\n","                \n","#             # Get the indices of the unlabeled samples to train on next cycle\n","#             arg = query_samples(models, method, data_unlabeled, subset, labeled_set, cycle, args)\n","\n","#             # Update the labeled dataset and the unlabeled dataset, respectively\n","#             new_list = list(torch.tensor(subset)[arg][:ADDENDUM].numpy())\n","#             # print(len(new_list), min(new_list), max(new_list))\n","#             labeled_set += list(torch.tensor(subset)[arg][-ADDENDUM:].numpy())\n","#             listd = list(torch.tensor(subset)[arg][:-ADDENDUM].numpy()) \n","#             unlabeled_set = listd + unlabeled_set[SUBSET:]\n","#             print(len(labeled_set), min(labeled_set), max(labeled_set))\n","#             # Create a new dataloader for the updated labeled dataset\n","#             dataloaders['train'] = DataLoader(data_train, batch_size=BATCH, \n","#                                             sampler=SubsetRandomSampler(labeled_set), \n","#                                             pin_memory=True)\n","\n","#     results.close()\n","    \n","#     return loss_module, resnet18"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:05.607460Z","iopub.status.busy":"2023-06-21T07:04:05.607111Z","iopub.status.idle":"2023-06-21T07:04:05.612871Z","shell.execute_reply":"2023-06-21T07:04:05.611777Z","shell.execute_reply.started":"2023-06-21T07:04:05.607425Z"},"trusted":true},"outputs":[],"source":["# args = {\n","#     'lambda_loss': 1.2,\n","#     's_margin': 0.1,\n","#     'hidden_units': 128,\n","#     'dropout_rate': 0.3,\n","#     'dataset': 'cifar10', # cifar100, fashionmnist, svhn\n","#     'no_of_epochs': 200,\n","#     'method_type': 'TA-VAAL', \n","#     'cycles': 10,\n","#     'total': False\n","# }"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:05.929932Z","iopub.status.busy":"2023-06-21T07:04:05.929571Z","iopub.status.idle":"2023-06-21T07:04:05.934853Z","shell.execute_reply":"2023-06-21T07:04:05.933653Z","shell.execute_reply.started":"2023-06-21T07:04:05.929896Z"},"trusted":true},"outputs":[],"source":["#main(args)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## TA-VAAL Cifar10"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:06.623400Z","iopub.status.busy":"2023-06-21T07:04:06.623064Z","iopub.status.idle":"2023-06-21T07:04:09.739999Z","shell.execute_reply":"2023-06-21T07:04:09.739062Z","shell.execute_reply.started":"2023-06-21T07:04:06.623366Z"},"trusted":true},"outputs":[],"source":["import os\n","import math\n","import random\n","import torch\n","import numpy as np\n","from tqdm import tqdm\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.init as init\n","import torch.nn.functional as F\n","from torch.distributions import Normal\n","from torch.nn.parameter import Parameter\n","from torch.nn.modules.module import Module\n","from torch.utils.data import DataLoader, Dataset\n","import torch.optim.lr_scheduler as lr_scheduler\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torchvision.datasets import CIFAR100, CIFAR10, FashionMNIST, SVHN\n","import torchvision.transforms as T\n","import torchvision.models as models\n","import argparse\n","from sklearn.metrics import confusion_matrix\n","from torch.utils.tensorboard import SummaryWriter\n","import csv"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:09.742566Z","iopub.status.busy":"2023-06-21T07:04:09.742206Z","iopub.status.idle":"2023-06-21T07:04:09.755805Z","shell.execute_reply":"2023-06-21T07:04:09.754433Z","shell.execute_reply.started":"2023-06-21T07:04:09.742470Z"},"trusted":true},"outputs":[],"source":["class SubsetSequentialSampler(torch.utils.data.Sampler):\n","    r\"\"\"Samples elements sequentially from a given list of indices, without replacement.\n","\n","    Arguments:\n","        indices (sequence): a sequence of indices\n","    \"\"\"\n","\n","    def __init__(self, indices):\n","        self.indices = indices\n","\n","    def __iter__(self):\n","        return (self.indices[i] for i in range(len(self.indices)))\n","    \n","    def __len__(self):\n","        return len(self.indices)\n","    \n","class MyDataset(Dataset):\n","    def __init__(self, dataset_name, train_flag, transf):\n","        self.dataset_name = dataset_name\n","        if self.dataset_name == \"cifar10\":\n","            self.cifar10 = CIFAR10('../cifar10', train=train_flag, \n","                                    download=True, transform=transf)\n","\n","    def __getitem__(self, index):\n","        if self.dataset_name == \"cifar10\":\n","            data, target = self.cifar10[index]\n","        return data, target, index\n","\n","    def __len__(self):\n","        if self.dataset_name == \"cifar10\":\n","            return len(self.cifar10)"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:09.757215Z","iopub.status.busy":"2023-06-21T07:04:09.756976Z","iopub.status.idle":"2023-06-21T07:04:09.770221Z","shell.execute_reply":"2023-06-21T07:04:09.768786Z","shell.execute_reply.started":"2023-06-21T07:04:09.757185Z"},"trusted":true},"outputs":[],"source":["def load_dataset(args):\n","    \n","    train_transform = T.Compose([\n","        T.RandomHorizontalFlip(),\n","        T.RandomCrop(size=32, padding=4),\n","        T.ToTensor(),\n","        T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]) \n","    ])\n","\n","    test_transform = T.Compose([\n","        T.ToTensor(),\n","        T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]) \n","    ])\n","\n","    if args['dataset'] == 'cifar10': \n","        data_train = CIFAR10('../cifar10', train=True, download=True, transform=train_transform)\n","        data_unlabeled = MyDataset(args['dataset'], True, test_transform)\n","        data_test  = CIFAR10('../cifar10', train=False, download=True, transform=test_transform)\n","        NO_CLASSES = 10\n","        adden = args['ADDENDUM']\n","        NUM_TRAIN = len(data_train)        \n","        no_train = NUM_TRAIN\n","        \n","    return data_train, data_unlabeled, data_test, adden, NO_CLASSES, no_train"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:09.774587Z","iopub.status.busy":"2023-06-21T07:04:09.773890Z","iopub.status.idle":"2023-06-21T07:04:09.808075Z","shell.execute_reply":"2023-06-21T07:04:09.807347Z","shell.execute_reply.started":"2023-06-21T07:04:09.774542Z"},"trusted":true},"outputs":[],"source":["class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","        self.num_classes = num_classes\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        \n","        self.linear1 = nn.Linear(512*block.expansion, 2)\n","        self.linear2 = nn.Linear(512*block.expansion, 2)\n","        self.linear3 = nn.Linear(512*block.expansion, 2)\n","        self.linear4 = nn.Linear(512*block.expansion, 2)\n","        self.linear5 = nn.Linear(512*block.expansion, 2)\n","        self.linear6 = nn.Linear(512*block.expansion, 2)\n","        self.linear7 = nn.Linear(512*block.expansion, 2)\n","        self.linear8 = nn.Linear(512*block.expansion, 2)\n","        self.linear9 = nn.Linear(512*block.expansion, 2)\n","        self.linear10 = nn.Linear(512*block.expansion,2)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out1 = self.layer1(out)\n","        out2 = self.layer2(out1)\n","        out3 = self.layer3(out2)\n","        out4 = self.layer4(out3)\n","        out = F.avg_pool2d(out4, 4)\n","        outf = out.view(out.size(0), -1)\n","        \n","        outt1 = self.linear1(outf)          \n","        outt2 = self.linear2(outf)          \n","        outt3 = self.linear3(outf)          \n","        outt4 = self.linear4(outf)          \n","        outt5 = self.linear5(outf)          \n","        outt6 = self.linear6(outf)          \n","        outt7 = self.linear7(outf)          \n","        outt8 = self.linear8(outf)          \n","        outt9 = self.linear9(outf)          \n","        outt10 = self.linear10(outf)          \n","        \n","        return [outt1, outt2, outt3, outt4, outt5, outt6, outt7, outt8, outt9, outt10], outf, [out1, out2, out3, out4]\n","\n","def ResNet18(num_classes = 10):\n","    return ResNet(BasicBlock, [2,2,2,2], num_classes)"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:09.809801Z","iopub.status.busy":"2023-06-21T07:04:09.809434Z","iopub.status.idle":"2023-06-21T07:04:09.855956Z","shell.execute_reply":"2023-06-21T07:04:09.854815Z","shell.execute_reply.started":"2023-06-21T07:04:09.809768Z"},"trusted":true},"outputs":[],"source":["class LossNet(nn.Module):\n","    def __init__(self, feature_sizes=[32, 16, 8, 4], num_channels=[64, 128, 256, 512], interm_dim=128):\n","        super(LossNet, self).__init__()\n","        \n","        self.GAP1 = nn.AvgPool2d(feature_sizes[0])\n","        self.GAP2 = nn.AvgPool2d(feature_sizes[1])\n","        self.GAP3 = nn.AvgPool2d(feature_sizes[2])\n","        self.GAP4 = nn.AvgPool2d(feature_sizes[3])\n","\n","        self.FC1 = nn.Linear(num_channels[0], interm_dim)\n","        self.FC2 = nn.Linear(num_channels[1], interm_dim)\n","        self.FC3 = nn.Linear(num_channels[2], interm_dim)\n","        self.FC4 = nn.Linear(num_channels[3], interm_dim)\n","\n","        self.linear = nn.Linear(4 * interm_dim, 1)\n","    \n","    def forward(self, features):\n","        out1 = self.GAP1(features[0])\n","        out1 = out1.view(out1.size(0), -1)\n","        out1 = F.relu(self.FC1(out1))\n","\n","        out2 = self.GAP2(features[1])\n","        out2 = out2.view(out2.size(0), -1)\n","        out2 = F.relu(self.FC2(out2))\n","\n","        out3 = self.GAP3(features[2])\n","        out3 = out3.view(out3.size(0), -1)\n","        out3 = F.relu(self.FC3(out3))\n","\n","        out4 = self.GAP4(features[3])\n","        out4 = out4.view(out4.size(0), -1)\n","        out4 = F.relu(self.FC4(out4))\n","\n","        out = self.linear(torch.cat((out1, out2, out3, out4), 1))\n","        return out\n","\n","class View(nn.Module):\n","    def __init__(self, size):\n","        super(View, self).__init__()\n","        self.size = size\n","\n","    def forward(self, tensor):\n","        return tensor.view(self.size)\n","    \n","class VAE(nn.Module):\n","    \"\"\"Encoder-Decoder architecture for both WAE-MMD and WAE-GAN.\"\"\"\n","    def __init__(self, z_dim=32, nc=3, f_filt=4):\n","        super(VAE, self).__init__()\n","        self.z_dim = z_dim\n","        self.nc = nc\n","        self.f_filt = f_filt\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(nc, 128, 4, 2, 1, bias=False),              # B,  128, 32, 32\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(True),\n","            nn.Conv2d(128, 256, 4, 2, 1, bias=False),             # B,  256, 16, 16\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(True),\n","            nn.Conv2d(256, 512, 4, 2, 1, bias=False),             # B,  512,  8,  8\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(True),\n","            nn.Conv2d(512, 1024, self.f_filt, 2, 1, bias=False),            # B, 1024,  4,  4\n","            nn.BatchNorm2d(1024),\n","            nn.ReLU(True),\n","            View((-1, 1024*2*2)),                                 # B, 1024*4*4\n","        )\n","\n","        self.fc_mu = nn.Linear(1024*2*2, z_dim)                            # B, z_dim\n","        self.fc_logvar = nn.Linear(1024*2*2, z_dim)                            # B, z_dim\n","        self.decoder = nn.Sequential(\n","            nn.Linear(z_dim + 1, 1024*4*4),                           # B, 1024*8*8\n","            View((-1, 1024, 4, 4)),                               # B, 1024,  8,  8\n","            nn.ConvTranspose2d(1024, 512, self.f_filt, 2, 1, bias=False),   # B,  512, 16, 16\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),    # B,  256, 32, 32\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),    # B,  128, 64, 64\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(128, nc, 1),                       # B,   nc, 64, 64\n","        )\n","        self.weight_init()\n","\n","    def weight_init(self):\n","        for block in self._modules:\n","            try:\n","                for m in self._modules[block]:\n","                    kaiming_init(m)\n","            except:\n","                kaiming_init(block)\n","\n","    def forward(self, r,x):\n","        z = self._encode(x)\n","        mu, logvar = self.fc_mu(z), self.fc_logvar(z)\n","        z = self.reparameterize(mu, logvar)\n","        z = torch.cat([z,r],1)\n","        x_recon = self._decode(z)\n","\n","        return x_recon, z, mu, logvar\n","\n","    def reparameterize(self, mu, logvar):\n","        stds = (0.5 * logvar).exp()\n","        epsilon = torch.randn(*mu.size())\n","        if mu.is_cuda:\n","            stds, epsilon = stds.cuda(), epsilon.cuda()\n","        latents = epsilon * stds + mu\n","        return latents\n","\n","    def _encode(self, x):\n","        return self.encoder(x)\n","\n","    def _decode(self, z):\n","        return self.decoder(z)\n","\n","class Discriminator(nn.Module):\n","    \"\"\"Adversary architecture(Discriminator) for WAE-GAN.\"\"\"\n","    def __init__(self, z_dim=10):\n","        super(Discriminator, self).__init__()\n","        self.z_dim = z_dim\n","        self.net = nn.Sequential(\n","            nn.Linear(z_dim + 1, 512),\n","            nn.ReLU(True),\n","            nn.Linear(512, 512),\n","            nn.ReLU(True),\n","            nn.Linear(512, 1),\n","            nn.Sigmoid()\n","        )\n","        self.weight_init()\n","\n","    def weight_init(self):\n","        for block in self._modules:\n","            for m in self._modules[block]:\n","                kaiming_init(m)\n","\n","    def forward(self, r,z):  \n","        z = torch.cat([z, r], 1)\n","        return self.net(z)\n","\n","def kaiming_init(m):\n","    if isinstance(m, (nn.Linear, nn.Conv2d)):\n","        init.kaiming_normal(m.weight)\n","        if m.bias is not None:\n","            m.bias.data.fill_(0)\n","    elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n","        m.weight.data.fill_(1)\n","        if m.bias is not None:\n","            m.bias.data.fill_(0)"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:09.858128Z","iopub.status.busy":"2023-06-21T07:04:09.857878Z","iopub.status.idle":"2023-06-21T07:04:09.928938Z","shell.execute_reply":"2023-06-21T07:04:09.927996Z","shell.execute_reply.started":"2023-06-21T07:04:09.858097Z"},"trusted":true},"outputs":[],"source":["# Loss Prediction Loss\n","def LossPredLoss(input, target, margin=1.0, reduction='mean'):\n","    assert len(input) % 2 == 0, 'the batch size is not even.'\n","    assert input.shape == input.flip(0).shape\n","    criterion = nn.BCELoss()\n","    input = (input - input.flip(0))[:len(input)//2] # [l_1 - l_2B, l_2 - l_2B-1, ... , l_B - l_B+1], where batch_size = 2B\n","    target = (target - target.flip(0))[:len(target)//2]\n","    target = target.detach()\n","    diff = torch.sigmoid(input)\n","    one = torch.sign(torch.clamp(target, min=0)) # 1 operation which is defined by the authors\n","    \n","    if reduction == 'mean':\n","        loss = criterion(diff,one)\n","    elif reduction == 'none':\n","        loss = criterion(diff,one)\n","    else:\n","        NotImplementedError()\n","    \n","    return loss\n","\n","def test(models, epoch, method, criterion, dataloaders, args, mode='val'):\n","    \n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")     \n","    \n","    models['backbone'].eval()\n","    if method == 'lloss':\n","        models['module'].eval()\n","    \n","    total = 0\n","    correct = [0 for i in range(args['NO_CLASSES'])]\n","    m = nn.LogSoftmax(dim=1)\n","    cm=[]\n","    for i in range(args['NO_CLASSES']):\n","        cm.append(np.zeros((2,2)))\n","\n","    individual_predictor_loss = [[] for i in range(args['NO_CLASSES'])]\n","    total_lloss = 0.0\n","    total_loss = 0.0\n","    total_pred_loss = 0.0        \n","\n","    with torch.no_grad():\n","        for (inputs, labels) in dataloaders[mode]:\n","            \n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            scores, _, features = models['backbone'](inputs)\n","            labels = torch.nn.functional.one_hot(labels, num_classes=args['NO_CLASSES']).float()\n","\n","            losses = []\n","            for i in range(args['NO_CLASSES']):\n","                s = m(scores[i])\n","                _,preds = torch.max(s, dim=1)\n","\n","                cm[i]+=confusion_matrix(labels[:,i].cpu().numpy(), preds.cpu().numpy())\n","                correct[i]+=(preds == labels[:,i]).sum().item()  \n","                \n","                l = criterion[i](s.float(), labels[:,i].long())\n","                losses.append(l)\n","                individual_predictor_loss[i].append(l)  \n","                \n","            total += labels.size(0)\n","            target_loss=torch.stack(losses).mean(dim=0)\n","            \n","            if method == 'lloss' or 'TA-VAAL':\n","                if epoch > args['EPOCHL']:\n","                    features[0] = features[0].detach()\n","                    features[1] = features[1].detach()\n","                    features[2] = features[2].detach()\n","                    features[3] = features[3].detach()\n","\n","                pred_loss = models['module'](features)\n","                pred_loss = pred_loss.view(pred_loss.size(0))\n","                m_module_loss   = LossPredLoss(pred_loss, target_loss, margin=args['MARGIN'])\n","\n","                total_lloss+= m_module_loss.item()\n","\n","                m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)        \n","                loss            = m_backbone_loss + args['WEIGHT'] * m_module_loss \n","\n","                total_loss+=loss.item()\n","            else:\n","                m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)        \n","                loss            = m_backbone_loss \n","              \n","    for i in range(args['NO_CLASSES']):\n","        individual_predictor_loss[i] = torch.cat(individual_predictor_loss[i]).mean().item()       \n","            \n","    return (100 * np.array(correct)) / total, cm, individual_predictor_loss, total_lloss, total_loss\n","\n","iters = 0\n","def train_epoch(models, method, criterion, optimizers, dataloaders, epoch, args):\n","        \n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    models['backbone'].train()\n","    if method == 'lloss' or 'TA-VAAL':\n","        models['module'].train()\n","    global iters\n","    models['backbone'].train()\n","    models['module'].train()\n","    models['backbone'].to(device)\n","    models['module'].to(device)\n","    m = nn.LogSoftmax(dim=1)\n","    \n","    individual_predictor_loss = [[] for i in range(args['NO_CLASSES'])]\n","    total_lloss = 0.0\n","    total_loss = 0.0\n","    correct = [0 for i in range(args['NO_CLASSES'])]\n","    total=0\n","    \n","    for data in dataloaders['train']:\n","        inputs = data[0].to(device)\n","        labels = data[1].to(device)\n","\n","        iters += 1\n","\n","        optimizers['backbone'].zero_grad()\n","        if method == 'lloss' or 'TA-VAAL':\n","            optimizers['module'].zero_grad()\n","        \n","        scores, _, features = models['backbone'](inputs)\n","        labels = torch.nn.functional.one_hot(labels, num_classes=args['NO_CLASSES']).float()\n","\n","        losses = []\n","        for i in range(args['NO_CLASSES']):\n","            s = m(scores[i])\n","            _,preds = torch.max(s, dim=1)\n","            \n","            correct[i]+=(preds == labels[:,i]).sum().item() \n","            \n","            l = criterion[i](s.float(), labels[:,i].long())\n","            losses.append(l)\n","            individual_predictor_loss[i].append(l)\n","        \n","        total += labels.size(0)\n","        target_loss=torch.stack(losses).mean(dim=0)\n","\n","        if method == 'lloss' or 'TA-VAAL':\n","            if epoch > args['EPOCHL']:\n","                features[0] = features[0].detach()\n","                features[1] = features[1].detach()\n","                features[2] = features[2].detach()\n","                features[3] = features[3].detach()\n","\n","            pred_loss = models['module'](features)\n","            pred_loss = pred_loss.view(pred_loss.size(0))\n","            m_module_loss   = LossPredLoss(pred_loss, target_loss, margin=args['MARGIN'])\n","            \n","            total_lloss+= m_module_loss.item()\n","            \n","            m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)        \n","            loss            = m_backbone_loss + args['WEIGHT'] * m_module_loss \n","            \n","            total_loss+=loss.item()\n","        else:\n","            m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)        \n","            loss            = m_backbone_loss\n","            \n","        loss.backward()\n","        optimizers['backbone'].step()\n","        if method == 'lloss' or 'TA-VAAL':\n","            optimizers['module'].step()\n","    \n","    for i in range(args['NO_CLASSES']):\n","        individual_predictor_loss[i] = torch.stack(individual_predictor_loss[i]).mean().item()\n","    \n","    return loss, individual_predictor_loss, total_lloss, total_loss, (100 * np.array(correct)) / total\n","\n","def train(models, method, criterion, optimizers, schedulers, dataloaders, cycle, args):\n","    \n","    rows=[]\n","    print('>> Train a Model.')\n","    best_acc = 0.\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","    num_tasks=args['NO_CLASSES']\n","    \n","    print(len(dataloaders['train']))\n","    for epoch in range(args['no_of_epochs']):\n","        row = [cycle, epoch]\n","        \n","        best_loss = torch.tensor([0.5]).to(device)\n","        loss,  individual_predictor_loss, total_lloss, total_loss, individual_acc = train_epoch(models, method, criterion, optimizers, dataloaders, epoch, args)\n","        \n","        schedulers['backbone'].step()\n","        if method == 'lloss' or 'TA-VAAL':\n","            schedulers['module'].step()\n","        \n","        # logging individual_predictor_loss\n","        for i in range(num_tasks):\n","            args['writer-train'].add_scalar(str(cycle) + ' Individual predictor loss '+ str(i),\n","                    individual_predictor_loss[i], epoch)\n","        \n","        # logging total predictor loss\n","        args['writer-train'].add_scalar(str(cycle) + ' Total predictor loss ',\n","                    sum(individual_predictor_loss)/len(individual_predictor_loss), epoch)\n","        \n","        # logging total lloss\n","        args['writer-train'].add_scalar(str(cycle) + ' Total lloss ',\n","                    total_lloss/len(dataloaders['train']), epoch)\n","        \n","        # logging total loss\n","        args['writer-train'].add_scalar(str(cycle) + ' Total loss ',\n","                    total_loss/len(dataloaders['train']), epoch)\n","        \n","        ## logging predictor acc\n","        args['writer-train'].add_scalar(str(cycle) + ' Total predictor Acc ',\n","                    sum(individual_acc) / len(individual_acc), epoch)\n","        row.append(sum(individual_acc) / len(individual_acc))\n","        \n","        # logging individual_predictor_acc\n","        for i in range(num_tasks):\n","            args['writer-train'].add_scalar(str(cycle) + ' Individual predictor acc ' + str(i),\n","                        individual_acc[i], epoch)\n","            row.append(individual_acc[i])\n","\n","        # Testing\n","        acc, cm, individual_predictor_loss, total_lloss, total_loss  = test(models, epoch, method, criterion, dataloaders, args, mode='test')\n","        print('Epoch ' + str(epoch)+\": \"+'Mean Accuracy: ', acc.mean())\n","\n","        # logging predictor_acc\n","        args['writer-val'].add_scalar(str(cycle) + ' Total predictor Acc ',\n","                    acc.mean(), epoch)\n","        row.append(acc.mean())\n","        \n","        # logging individual_predictor_acc\n","        for i in range(num_tasks):\n","            args['writer-val'].add_scalar(str(cycle) + ' Individual predictor acc ' + str(i),\n","                        acc[i], epoch)\n","            row.append(acc[i])\n","        \n","        # logging individual_predictor_loss\n","        for i in range(num_tasks):\n","            args['writer-val'].add_scalar(str(cycle) + ' Individual predictor loss '+ str(i),\n","                    individual_predictor_loss[i], epoch)\n","        \n","        # logging total predictor loss\n","        args['writer-val'].add_scalar(str(cycle) + ' Total predictor loss ',\n","                    sum(individual_predictor_loss)/len(individual_predictor_loss), epoch)\n","        \n","        # logging total lloss\n","        args['writer-val'].add_scalar(str(cycle) + ' Total lloss ',\n","                    total_lloss/len(dataloaders['test']), epoch)\n","        \n","        # logging total loss\n","        args['writer-val'].add_scalar(str(cycle) + ' Total loss ',\n","                    total_loss/len(dataloaders['test']), epoch)      \n","        \n","        rows.append(row)\n","        \n","    with open(\"results_\" + str(args['attempt']) + '_cifar10' + \".csv\", 'a') as csvfile: \n","        csvwriter = csv.writer(csvfile) \n","        csvwriter.writerows(rows)\n","        \n","#         if True and epoch % 20  == 0:\n","#             acc, cm = test(models, epoch, method, dataloaders, args, mode='test')\n","#             print('Mean Accuracy:', acc.mean())\n","\n","    print('>> Finished.')"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:10.401743Z","iopub.status.busy":"2023-06-21T07:04:10.401451Z","iopub.status.idle":"2023-06-21T07:04:10.453327Z","shell.execute_reply":"2023-06-21T07:04:10.452342Z","shell.execute_reply.started":"2023-06-21T07:04:10.401713Z"},"trusted":true},"outputs":[],"source":["def read_data(dataloader, labels=True):\n","    if labels:\n","        while True:\n","            for img, label,_ in dataloader:\n","                yield img, label\n","    else:\n","        while True:\n","            for img, _, _ in dataloader:\n","                yield img\n","\n","def vae_loss(x, recon, mu, logvar, beta):\n","    mse_loss = nn.MSELoss()\n","    MSE = mse_loss(recon, x)\n","    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n","    KLD = KLD * beta\n","    return MSE + KLD\n","\n","def train_vaal(models, optimizers, labeled_dataloader, unlabeled_dataloader, cycle, args):\n","    \n","    vae = models['vae']\n","    discriminator = models['discriminator']\n","    task_model = models['backbone']\n","    ranker = models['module']\n","    \n","    task_model.eval()\n","    ranker.eval()\n","    vae.train()\n","    discriminator.train()\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","\n","    vae = vae.to(device)\n","    discriminator = discriminator.to(device)\n","    task_model = task_model.to(device)\n","    ranker = ranker.to(device)\n","\n","    adversary_param = 1\n","    beta          = 1\n","    num_adv_steps = 1\n","    num_vae_steps = 1\n","\n","    bce_loss = nn.BCELoss()\n","    \n","    labeled_data = read_data(labeled_dataloader)\n","    unlabeled_data = read_data(unlabeled_dataloader)\n","\n","    train_iterations = int( (args['INCREMENTAL']*cycle+ args['SUBSET']) * args['EPOCHV'] / args['BATCH'] )\n","    print('Num of Iteration:', str(train_iterations))\n","    \n","    for iter_count in range(train_iterations):\n","        labeled_imgs, labels = next(labeled_data)\n","        unlabeled_imgs = next(unlabeled_data)[0]\n","        \n","        labeled_imgs = labeled_imgs.to(device)\n","        unlabeled_imgs = unlabeled_imgs.to(device)\n","        labels = labels.to(device)    \n","        \n","        if iter_count == 0 :\n","            r_l_0 = torch.from_numpy(np.random.uniform(0, 1, size=(labeled_imgs.shape[0],1))).type(torch.FloatTensor).to(device)\n","            r_u_0 = torch.from_numpy(np.random.uniform(0, 1, size=(unlabeled_imgs.shape[0],1))).type(torch.FloatTensor).to(device)\n","        else:\n","            with torch.no_grad():\n","                _,_,features_l = task_model(labeled_imgs)\n","                _,_,feature_u = task_model(unlabeled_imgs)\n","                r_l = ranker(features_l)\n","                r_u = ranker(feature_u)\n","        if iter_count == 0:\n","            r_l = r_l_0.detach()\n","            r_u = r_u_0.detach()\n","            r_l_s = r_l_0.detach()\n","            r_u_s = r_u_0.detach()\n","        else:\n","            r_l_s = torch.sigmoid(r_l).detach()\n","            r_u_s = torch.sigmoid(r_u).detach()   \n","            \n","        # VAE step\n","        for count in range(num_vae_steps): # num_vae_steps\n","            recon, _, mu, logvar = vae(r_l_s,labeled_imgs)\n","            unsup_loss = vae_loss(labeled_imgs, recon, mu, logvar, beta)\n","            unlab_recon, _, unlab_mu, unlab_logvar = vae(r_u_s,unlabeled_imgs)\n","            transductive_loss = vae_loss(unlabeled_imgs, \n","                    unlab_recon, unlab_mu, unlab_logvar, beta)\n","        \n","            labeled_preds = discriminator(r_l,mu)\n","            unlabeled_preds = discriminator(r_u,unlab_mu)\n","            \n","            lab_real_preds = torch.ones(labeled_imgs.size(0))\n","            unlab_real_preds = torch.ones(unlabeled_imgs.size(0))\n","                \n","            lab_real_preds = lab_real_preds.to(device)\n","            unlab_real_preds = unlab_real_preds.to(device)            \n","\n","            dsc_loss = bce_loss(labeled_preds[:,0], lab_real_preds) + \\\n","                       bce_loss(unlabeled_preds[:,0], unlab_real_preds)\n","            total_vae_loss = unsup_loss + transductive_loss + adversary_param * dsc_loss\n","            \n","            optimizers['vae'].zero_grad()\n","            total_vae_loss.backward()\n","            optimizers['vae'].step()\n","\n","            # sample new batch if needed to train the adversarial network\n","            if count < (num_vae_steps - 1):\n","                labeled_imgs, _ = next(labeled_data)\n","                unlabeled_imgs = next(unlabeled_data)[0]\n","                \n","                labeled_imgs = labeled_imgs.to(device)\n","                unlabeled_imgs = unlabeled_imgs.to(device)\n","                labels = labels.to(device)                \n","\n","        # Discriminator step\n","        for count in range(num_adv_steps):\n","            with torch.no_grad():\n","                _, _, mu, _ = vae(r_l_s,labeled_imgs)\n","                _, _, unlab_mu, _ = vae(r_u_s,unlabeled_imgs)\n","            \n","            labeled_preds = discriminator(r_l,mu)\n","            unlabeled_preds = discriminator(r_u,unlab_mu)\n","            \n","            lab_real_preds = torch.ones(labeled_imgs.size(0))\n","            unlab_fake_preds = torch.zeros(unlabeled_imgs.size(0))\n","\n","            lab_real_preds = lab_real_preds.to(device)\n","            unlab_fake_preds = unlab_fake_preds.to(device)            \n","            \n","            dsc_loss = bce_loss(labeled_preds[:,0], lab_real_preds) + \\\n","                       bce_loss(unlabeled_preds[:,0], unlab_fake_preds)\n","\n","            optimizers['discriminator'].zero_grad()\n","            dsc_loss.backward()\n","            optimizers['discriminator'].step()\n","\n","            # sample new batch if needed to train the adversarial network\n","            if count < (num_adv_steps-1):\n","                labeled_imgs, _ = next(labeled_data)\n","                unlabeled_imgs = next(unlabeled_data)[0]\n","\n","                labeled_imgs = labeled_imgs.to(device)\n","                unlabeled_imgs = unlabeled_imgs.to(device)\n","                labels = labels.to(device)                \n","                \n","            if iter_count % 50 == 0:\n","                # print(\"Iteration: \" + str(iter_count) + \"  vae_loss: \" + str(total_vae_loss.item()) + \" dsc_loss: \" +str(dsc_loss.item()))\n","                args['writer-train'].add_scalar(str(cycle) + ' Total VAE Loss ',\n","                        total_vae_loss.item(), iter_count)\n","                args['writer-train'].add_scalar(str(cycle) + ' Total DSC Loss ',\n","                        dsc_loss.item(), iter_count)\n","        \n","\n","# Select the indices of the unlablled data according to the methods\n","def query_samples(model, method, data_unlabeled, subset, labeled_set, cycle, args):\n","    \n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","    if method == 'TA-VAAL':\n","        # Create unlabeled dataloader for the unlabeled subset\n","        unlabeled_loader = DataLoader(data_unlabeled, batch_size=args['BATCH'], \n","                                    sampler=SubsetSequentialSampler(subset), \n","                                    pin_memory=True)\n","        labeled_loader = DataLoader(data_unlabeled, batch_size=args['BATCH'], \n","                                    sampler=SubsetSequentialSampler(labeled_set), \n","                                    pin_memory=True)\n","        \n","        vae = VAE()\n","        discriminator = Discriminator(32)\n","     \n","        models      = {'backbone': model['backbone'], 'module': model['module'], 'vae': vae, 'discriminator': discriminator}\n","        \n","        optim_vae = optim.Adam(vae.parameters(), lr=5e-4)\n","        optim_discriminator = optim.Adam(discriminator.parameters(), lr=5e-4)\n","        optimizers = {'vae': optim_vae, 'discriminator':optim_discriminator}\n","\n","        train_vaal(models,optimizers, labeled_loader, unlabeled_loader, cycle+1, args)\n","        task_model = models['backbone']\n","        ranker = models['module']        \n","        all_preds, all_indices = [], []\n","\n","        for images, _, indices in unlabeled_loader:                       \n","            images = images.to(device)\n","            with torch.no_grad():\n","                _,_,features = task_model(images)\n","                r = ranker(features)\n","                _, _, mu, _ = vae(torch.sigmoid(r),images)\n","                preds = discriminator(r,mu)\n","\n","            preds = preds.cpu().data\n","            all_preds.extend(preds)\n","            all_indices.extend(indices)\n","\n","        all_preds = torch.stack(all_preds)\n","        all_preds = all_preds.view(-1)\n","        # need to multiply by -1 to be able to use torch.topk \n","        all_preds *= -1\n","        # select the points which the discriminator things are the most likely to be unlabeled\n","        _, arg = torch.sort(all_preds) \n","        \n","        torch.save(vae, 'saved_history/models/vae-cycle-'+str(cycle)+'.pth')\n","        torch.save(discriminator, 'saved_history/models/discriminator-cycle-'+str(cycle)+'.pth')\n","    \n","    if method == 'Random':\n","        arg = np.random.randint(10000, size=10000)\n","        \n","    return arg"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:11.113694Z","iopub.status.busy":"2023-06-21T07:04:11.113328Z","iopub.status.idle":"2023-06-21T07:04:11.156815Z","shell.execute_reply":"2023-06-21T07:04:11.155779Z","shell.execute_reply.started":"2023-06-21T07:04:11.113656Z"},"trusted":true},"outputs":[],"source":["# Main\n","def main(args):\n","    method = args['method_type']\n","    results = open('results_'+str(args['method_type'])+\"_\"+args['dataset'] +'_main'+str(args['CYCLES'])+str(args['total'])+'.txt','w')\n","    print(\"Dataset: %s\"%args['dataset'])\n","    print(\"Method type:%s\"%method)\n","    \n","    if args['total']:\n","        args['TRIALS'] = 1\n","        args['CYCLES'] = 1\n","    else:\n","        args['CYCLES'] = args['CYCLES']\n","        \n","    # fields\n","    fields = ['cycle', 'epoch', 'total_train_pred_acc']\n","    for i in range(10):\n","        fields.append('train_pred_'+ str(i+1) + '_acc')\n","    fields.append('total_val_pred_acc')\n","    for i in range(10):\n","        fields.append('val_pred_'+ str(i+1) + '_acc')\n","    \n","    filename = \"results_\" + str(args['attempt']) + '_cifar10' + \".csv\"\n","    with open(filename, 'a') as csvfile: \n","        csvwriter = csv.writer(csvfile) \n","        csvwriter.writerow(fields)     \n","\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","    \n","    for trial in range(args['trials']):\n","        # Load training and testing dataset\n","        data_train, data_unlabeled, data_test, adden, NO_CLASSES, no_train = load_dataset(args)\n","        print('The entire datasize is {}'.format(len(data_train)))       \n","        \n","        ADDENDUM = adden\n","        NUM_TRAIN = no_train\n","        indices = list(range(NUM_TRAIN))\n","        random.shuffle(indices)\n","\n","        if args['total']:\n","            labeled_set= indices\n","        else:\n","            labeled_set = indices[:ADDENDUM]\n","            unlabeled_set = [x for x in indices if x not in labeled_set]\n","        \n","        train_loader = DataLoader(data_train, batch_size=args['BATCH'], \n","                                    sampler=SubsetRandomSampler(labeled_set), \n","                                    pin_memory=True, drop_last=True)\n","        test_loader  = DataLoader(data_test, batch_size=args['BATCH'])\n","        dataloaders  = {'train': train_loader, 'test': test_loader}\n","        \n","        np.save(\"saved_history/initial-labelled.npy\", np.array(labeled_set))\n","        \n","        for cycle in range(args['CYCLES']):\n","            print(cycle)\n","            \n","            # Randomly sample 10000 unlabeled data points\n","            if not args['total']:\n","                random.shuffle(unlabeled_set)\n","                subset = unlabeled_set[:args['SUBSET']]\n","            \n","            # Model - create new instance for every cycle so that it resets\n","            resnet18    = ResNet18(num_classes=NO_CLASSES).to(device)\n","            if method == 'lloss' or 'TA-VAAL':\n","                loss_module = LossNet().to(device)\n","\n","            models      = {'backbone': resnet18}\n","            if method =='lloss' or 'TA-VAAL':\n","                models = {'backbone': resnet18, 'module': loss_module}\n","            torch.backends.cudnn.benchmark = True\n","            \n","            labels_l = []\n","            for i in labeled_set:\n","                labels_l.append(data_train[i][1])\n","\n","            unique, counts = np.unique(labels_l, return_counts=True)\n","            wts_0 = (counts / len(labels_l))\n","            wts_1 = 1 - (counts / len(labels_l))\n","            print(wts_0)\n","            assert len(unique) == NO_CLASSES, 'The number of labels is not %s'%(str(NO_CLASSES)) \n","            \n","            # Loss, criterion and scheduler (re)initialization\n","            criterion      = [nn.NLLLoss(reduction='none', weight = torch.tensor([wts_0[i], wts_1[i]]).to(device)).float() for i in range(NO_CLASSES)]\n","            optim_backbone = optim.SGD(models['backbone'].parameters(), lr=args['LR'], \n","                momentum=args['MOMENTUM'], weight_decay=args['WDECAY'])\n"," \n","            sched_backbone = lr_scheduler.MultiStepLR(optim_backbone, milestones=args['MILESTONES'])\n","            optimizers = {'backbone': optim_backbone}\n","            schedulers = {'backbone': sched_backbone}\n","            if method == 'lloss' or 'TA-VAAL':\n","                optim_module   = optim.SGD(models['module'].parameters(), lr=args['LR'], \n","                    momentum=args['MOMENTUM'], weight_decay=args['WDECAY'])\n","                sched_module   = lr_scheduler.MultiStepLR(optim_module, milestones=args['MILESTONES'])\n","                optimizers = {'backbone': optim_backbone, 'module': optim_module}\n","                schedulers = {'backbone': sched_backbone, 'module': sched_module} \n","\n","            # Training and testing\n","            train(models, method, criterion, optimizers, schedulers, dataloaders, cycle+1, args)\n","            acc, cm, individual_predictor_loss, total_lloss, total_loss = test(models, args['no_of_epochs'], method, criterion, dataloaders, args, mode='test')\n","            torch.save(models['backbone'], 'saved_history/models/predictor-backbone-cycle-'+str(cycle+1)+'.pth')\n","            torch.save(models['module'], 'saved_history/models/predictor-module-cycle-'+str(cycle+1)+'.pth')\n","\n","            print('Trial {}/{} || Cycle {}/{} || Label set size {}'.format(trial+1, args['trials'], cycle+1, args['CYCLES'], len(labeled_set)))\n","\n","            for i in range(args['NO_CLASSES']):\n","                print('val_accuracy', i, ':', acc[i])\n","                print(cm[i])\n","                print(\"\")\n","\n","            # Get the indices of the unlabeled samples to train on next cycle\n","            if cycle == (args['CYCLES']-1):\n","                # Reached final training cycle\n","                print(\"Finished.\")\n","                break\n","                \n","            # Get the indices of the unlabeled samples to train on next cycle\n","            arg = query_samples(models, method, data_unlabeled, subset, labeled_set, cycle, args)                \n","            \n","            # Update the labeled dataset and the unlabeled dataset, respectively\n","            new_list = list(torch.tensor(subset)[arg][:args['INCREMENTAL']].numpy())\n","            labeled_set += list(torch.tensor(subset)[arg][-args['INCREMENTAL']:].numpy())\n","            listd = list(torch.tensor(subset)[arg][:-args['INCREMENTAL']].numpy()) \n","            unlabeled_set = listd + unlabeled_set[args['SUBSET']:]\n","            print(len(labeled_set), min(labeled_set), max(labeled_set))\n","            \n","            np.save(\"saved_history/labelled\" + str(cycle) + \".npy\", np.array(labeled_set))\n","            \n","            # Create a new dataloader for the updated labeled dataset\n","            dataloaders['train'] = DataLoader(data_train, batch_size=args['BATCH'], \n","                                            sampler=SubsetRandomSampler(labeled_set), \n","                                            pin_memory=True, drop_last=True)\n","\n","    results.close()"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:12.550060Z","iopub.status.busy":"2023-06-21T07:04:12.549719Z","iopub.status.idle":"2023-06-21T07:04:12.555530Z","shell.execute_reply":"2023-06-21T07:04:12.554285Z","shell.execute_reply.started":"2023-06-21T07:04:12.550026Z"},"trusted":true},"outputs":[],"source":["splits = [0.1,0.15,0.2,0.25,0.3,0.35,0.4]"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:19.016869Z","iopub.status.busy":"2023-06-21T07:04:19.016530Z","iopub.status.idle":"2023-06-21T07:04:23.753794Z","shell.execute_reply":"2023-06-21T07:04:23.752936Z","shell.execute_reply.started":"2023-06-21T07:04:19.016834Z"},"trusted":true},"outputs":[],"source":["args = {\n","    'method_type': 'Random', # TA-VAAL \n","    'dataset': 'cifar10',\n","    'total': False,\n","    'trials': 1,\n","    'CYCLES': 7,\n","    'ADDENDUM': 5000,\n","    'INCREMENTAL': 2500,\n","    'BATCH': 256,\n","    'SUBSET': 10000,\n","    'NO_CLASSES': 10,\n","    'LR': 1e-3,\n","    'WDECAY': 5e-4,\n","    'MOMENTUM': 0.9,\n","    'MILESTONES': [160, 240],###\n","    'MARGIN': 1.0,\n","    'WEIGHT': 1.0,\n","    'no_of_epochs': 100,\n","    'lambda_loss': 1.2,\n","    's_margin': 0.1,\n","    'hidden_units': 128,\n","    'dropout_rate': 0.3,\n","    'EPOCHV':50,\n","    'EPOCHL': 60,\n","    'attempt': 1,\n","    'writer-train': SummaryWriter('logs/TAVAAL-CIFAR10-1-Train'),\n","    'writer-val': SummaryWriter('logs/TAVAAL-CIFAR10-1-Val')\n","}"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:23.757052Z","iopub.status.busy":"2023-06-21T07:04:23.756339Z","iopub.status.idle":"2023-06-21T07:04:24.868788Z","shell.execute_reply":"2023-06-21T07:04:24.867683Z","shell.execute_reply.started":"2023-06-21T07:04:23.756998Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory logs: File exists\n"]}],"source":["!mkdir logs saved_history saved_history/models"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-21T07:04:24.872794Z","iopub.status.busy":"2023-06-21T07:04:24.872169Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset: cifar10\n","Method type:Random\n","Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../cifar10/cifar-10-python.tar.gz\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc3527bcafcd48048f746967a2d149f2","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Extracting ../cifar10/cifar-10-python.tar.gz to ../cifar10\n","Files already downloaded and verified\n","Files already downloaded and verified\n","The entire datasize is 50000\n","0\n","[0.1082 0.0942 0.098  0.1044 0.1022 0.1014 0.0992 0.0976 0.0974 0.0974]\n",">> Train a Model.\n","19\n"]}],"source":["main(args)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-06-05T09:10:01.130183Z","iopub.status.idle":"2022-06-05T09:10:01.130882Z","shell.execute_reply":"2022-06-05T09:10:01.130665Z","shell.execute_reply.started":"2022-06-05T09:10:01.130638Z"},"trusted":true},"outputs":[],"source":["# a=torch.nn.functional.one_hot(k[1])\n","# b=torch.cat(s, dim=1)\n","# torch.nn.functional.binary_cross_entropy(torch.sigmoid(b).cpu(), a.float().cpu())\n","\n","# tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
