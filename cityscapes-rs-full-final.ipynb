{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip3 install tensorflow==2.9.0 tensorflow-gpu\n# !pip3 uninstall tensorflow --yes\n# !pip3 uninstall tensorflow-gpu\n# ! pip3 install tensorflow-gpu","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-21T07:16:28.741518Z","iopub.execute_input":"2023-06-21T07:16:28.741927Z","iopub.status.idle":"2023-06-21T07:16:28.770346Z","shell.execute_reply.started":"2023-06-21T07:16:28.741892Z","shell.execute_reply":"2023-06-21T07:16:28.769388Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!rm -rf *","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:29.310629Z","iopub.execute_input":"2023-06-21T07:16:29.311878Z","iopub.status.idle":"2023-06-21T07:16:30.427649Z","shell.execute_reply.started":"2023-06-21T07:16:29.311832Z","shell.execute_reply":"2023-06-21T07:16:30.426127Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport random as random\nfrom PIL import Image\nimport os\nimport datetime\nimport cv2\nimport glob\nimport json\nfrom types import MethodType\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\n\nimport six\n\nimport keras\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Softmax, Reshape, Input, Flatten, Conv2DTranspose, Conv2D, GlobalAveragePooling2D,BatchNormalization, Multiply, Dot, Lambda, MaxPooling2D, ReLU, Dropout, Concatenate, ZeroPadding2D, UpSampling2D, Activation\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.models import Model, model_from_json\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.utils import plot_model, to_categorical\nfrom tensorflow.keras.callbacks import ModelCheckpoint, LambdaCallback, ReduceLROnPlateau, CSVLogger, TensorBoard\nfrom tensorflow.keras import backend as K\nimport sklearn\nimport tensorflow_addons as tfa\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:31.985051Z","iopub.execute_input":"2023-06-21T07:16:31.985481Z","iopub.status.idle":"2023-06-21T07:16:43.124373Z","shell.execute_reply.started":"2023-06-21T07:16:31.985443Z","shell.execute_reply":"2023-06-21T07:16:43.122986Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n\nTensorFlow Addons (TFA) has ended development and introduction of new features.\nTFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\nPlease modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n\nFor more information see: https://github.com/tensorflow/addons/issues/2807 \n\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\ntf.get_logger().setLevel('INFO')\nimport logging\ntf.get_logger().setLevel(logging.ERROR)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:43.126685Z","iopub.execute_input":"2023-06-21T07:16:43.127638Z","iopub.status.idle":"2023-06-21T07:16:43.134335Z","shell.execute_reply.started":"2023-06-21T07:16:43.127597Z","shell.execute_reply":"2023-06-21T07:16:43.133378Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from tensorflow.python.framework import ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.keras import backend as K\nimport tensorflow.keras as keras","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:43.135828Z","iopub.execute_input":"2023-06-21T07:16:43.136790Z","iopub.status.idle":"2023-06-21T07:16:43.150283Z","shell.execute_reply.started":"2023-06-21T07:16:43.136756Z","shell.execute_reply":"2023-06-21T07:16:43.148940Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# categories_1 = {\n#     'flat': [0,1],\n#     'construction': [2,3,4],\n#     'object': [5,6,7],\n#     'nature': [8,9],\n#     'human': [11,12],\n#     'vehicle':[13,14,15,16,17,18]\n# }","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:43.153304Z","iopub.execute_input":"2023-06-21T07:16:43.153640Z","iopub.status.idle":"2023-06-21T07:16:43.162235Z","shell.execute_reply.started":"2023-06-21T07:16:43.153614Z","shell.execute_reply":"2023-06-21T07:16:43.161122Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"hyperparameters={\n    'cls': 'human',\n    'targets': ['image_id'],\n    'height': 128, # 512 \n    'width': 256 , # 1024\n    'channels': 3, \n    'batch_size': 8, \n    'epochs': 1, \n    'num_tasks': 7, ### MODIFY\n    'initializer': 'he_uniform', \n    'reg_lambda': 1e-3, \n    'output': [2]*7, ### MODIFY === [2]*number of tasks\n    'attrPerTask': 2,\n    'lr': 1e-4, ### MODIFY\n    'is_trained': False, \n    'dropout_prob': 0.3,\n    'enable_cs': False, \n    'enable_sluice': False,\n    'initial_percent':0.1,\n    'initial_train_epoch': 7, ### NEED TO SEE----- Done\n    'increment_train_epoch': 7, ### NEED TO SEE --- Done\n    'uncertainity_repeat': 5, \n    'num_uncertain_elements': 148, \n    'additional_epoch': 5, ### NEED TO SEE\n    'pretraining_epochs': 3, ### -- make it 2 or originally 1 - Done\n    'train_initial_batches': 74, #6103, ### NEED TO SEE ------------- Done\n    'enable_additional': False,\n    'additional_attr_count':2,\n    'all_updates':False,\n    'initial_percent_val': 0.9, ### Added Now\n    'split_index': 0 ### Added Now\n    }","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:43.163484Z","iopub.execute_input":"2023-06-21T07:16:43.163830Z","iopub.status.idle":"2023-06-21T07:16:43.176957Z","shell.execute_reply.started":"2023-06-21T07:16:43.163802Z","shell.execute_reply":"2023-06-21T07:16:43.175665Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def preprocess(hyperparameters):\n    \n    train_files = pd.DataFrame(glob.glob(\"../input/cityscapes-train/*/*/*.png\"), columns=['idx'])\n    val_files = pd.DataFrame(glob.glob(\"../input/cityscapes/val/*.png\"), columns=['idx'])\n    \n    train = train_files[:(len(train_files)//hyperparameters['batch_size'])*hyperparameters['batch_size']]\n    val = val_files[:(len(val_files)//hyperparameters['batch_size'])*hyperparameters['batch_size']]\n    \n    return (train, val)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:43.178842Z","iopub.execute_input":"2023-06-21T07:16:43.179301Z","iopub.status.idle":"2023-06-21T07:16:43.196338Z","shell.execute_reply.started":"2023-06-21T07:16:43.179261Z","shell.execute_reply":"2023-06-21T07:16:43.195113Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"(train, val) = preprocess(hyperparameters)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:43.199088Z","iopub.execute_input":"2023-06-21T07:16:43.199751Z","iopub.status.idle":"2023-06-21T07:16:43.616000Z","shell.execute_reply.started":"2023-06-21T07:16:43.199707Z","shell.execute_reply":"2023-06-21T07:16:43.615201Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Parameters\nparams = {'dim': (hyperparameters['height'],hyperparameters['width']),\n          'batch_size': hyperparameters['batch_size'],\n          'n_channels': hyperparameters['channels'],\n          'shuffle': True}\n\n# Datasets\npartition = {}\npartition['train'] = train.idx.values\npartition['val'] =  val.idx.values\n\ntasks=['flat', 'construction', 'object', 'sky', 'nature', 'human', 'vehicle']\n\nlabels = {}\nfor i in partition['train']:\n    k = i.split(\"/\")\n    p=[]\n    for t in tasks:\n        p.append(\"/\".join(k[0:2]+['cityscapestasklabels/cityscapes/'+t+'/'+'train'] + [k[-1]]))\n    labels[i]=p\n    \nfor i in partition['val']:\n    k = i.split(\"/\")\n    p=[]\n    for t in tasks:\n        p.append(\"/\".join(k[0:2]+['cityscapestasklabels/cityscapes/'+t+'/'+'val'] + [k[-1]]))\n    labels[i]=p","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:43.617422Z","iopub.execute_input":"2023-06-21T07:16:43.617999Z","iopub.status.idle":"2023-06-21T07:16:43.677087Z","shell.execute_reply.started":"2023-06-21T07:16:43.617959Z","shell.execute_reply":"2023-06-21T07:16:43.675853Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def augment_using_layers(images, masks, size=None, val=False):\n    \n    h_s = size[0]\n    w_s = size[1]\n    \n    def aug(height=h_s, width=w_s, validation=val):\n        \n        resize = tf.keras.layers.Resizing(height, width, interpolation='nearest')\n        \n        flip = tf.keras.layers.RandomFlip(mode=\"horizontal\")\n        \n        rota = tf.keras.layers.RandomRotation(0.1, fill_mode='wrap', interpolation='nearest')\n        \n        trans = tf.keras.layers.RandomTranslation(height_factor=(-0.1, 0.1),\n                                            width_factor=(-0.1, 0.1), \n                                            fill_mode='wrap',interpolation='nearest')\n        \n        if validation:\n            layers = [resize] \n        else:\n            layers = [resize, flip, rota]\n        aug_model = tf.keras.Sequential(layers)\n\n        return aug_model\n    \n    aug = aug()\n    \n    if masks is not None:\n        mask = tf.stack(masks, -1)\n        mask = tf.cast(mask, 'float32')\n        images_mask = tf.concat([images, mask], -1)   \n    else:\n        images_mask = tf.cast(images, 'float32')\n    \n    images_mask = aug(images_mask)  \n    \n    if masks is not None:\n        image = images_mask[:,:,0:3]\n        labels=[]\n        for i in range(hyperparameters['num_tasks']):\n            labels.append(images_mask[:, :, 3+i])\n        return image/255.0, labels\n    else:\n        image = images_mask[:,:,0:3]\n        mask = None\n        return image/255.0, None","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:43.678348Z","iopub.execute_input":"2023-06-21T07:16:43.678660Z","iopub.status.idle":"2023-06-21T07:16:43.692056Z","shell.execute_reply.started":"2023-06-21T07:16:43.678634Z","shell.execute_reply":"2023-06-21T07:16:43.690886Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"class DataGenerator(keras.utils.Sequence):\n\n    def __init__(self, list_IDs, labels, batch_size=32, dim=(512,512), n_channels=3,shuffle=True, is_validation=False):\n        self.dim = dim\n        self.batch_size = batch_size\n        self.labels = labels\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.shuffle = shuffle\n        self.is_validation = is_validation\n        self.on_epoch_end()\n\n    def on_epoch_end(self):\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        y1 = np.empty((self.batch_size, *self.dim), dtype=int)\n        y2 = np.empty((self.batch_size, *self.dim), dtype=int)\n        y3 = np.empty((self.batch_size, *self.dim), dtype=int)\n        y4 = np.empty((self.batch_size, *self.dim), dtype=int)\n        y5 = np.empty((self.batch_size, *self.dim), dtype=int)\n        y6 = np.empty((self.batch_size, *self.dim), dtype=int)\n        y7 = np.empty((self.batch_size, *self.dim), dtype=int)\n        \n        # print(list_IDs_temp)\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            # Store sample\n            if type(ID) == list or type(ID) == np.ndarray:\n                k = ID[0]\n            else:\n                k=ID\n                \n            #print(i, ID)\n            #print(self.labels[k])\n            #print(\" \")\n            #print()\n            \n            # read image\n            img = cv2.imread(k)\n\n            # read labels        \n            img1 = cv2.imread(self.labels[k][0])[:,:,0]\n            img2 = cv2.imread(self.labels[k][1])[:,:,0]\n            img3 = cv2.imread(self.labels[k][2])[:,:,0]\n            img4 = cv2.imread(self.labels[k][3])[:,:,0]\n            img5 = cv2.imread(self.labels[k][4])[:,:,0]\n            img6 = cv2.imread(self.labels[k][5])[:,:,0]\n            img7 = cv2.imread(self.labels[k][6])[:,:,0]\n\n            labs=[img1,img2,img3,img4,img5,img6,img7]\n\n            # aug. dataset\n            img_1, labs_1 = augment_using_layers(img, labs, (hyperparameters['height'], hyperparameters['width']), self.is_validation)\n            \n            X[i,] = img_1\n            \n            y1[i, ] = labs_1[0]\n            y2[i, ] = labs_1[1]\n            y3[i, ] = labs_1[2]\n            y4[i, ] = labs_1[3]\n            y5[i, ] = labs_1[4]\n            y6[i, ] = labs_1[5]\n            y7[i, ] = labs_1[6]\n\n        return X, [y1, y2, y3, y4, y5, y6, y7]\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X, Y = self.__data_generation(list_IDs_temp)\n\n        return X, Y","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:43.697037Z","iopub.execute_input":"2023-06-21T07:16:43.697837Z","iopub.status.idle":"2023-06-21T07:16:43.721692Z","shell.execute_reply.started":"2023-06-21T07:16:43.697790Z","shell.execute_reply":"2023-06-21T07:16:43.720754Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class MultipleGenerator(keras.utils.Sequence):\n    \n    def __init__(self, list_IDs_1, list_IDs_2, labels, batch_size=32, dim=(512,512), n_channels=3,shuffle=True):\n        self.dim = dim\n        self.batch_size = batch_size\n        self.labels = labels\n        self.list_IDs_1 = list_IDs_1\n        self.list_IDs_2 = list_IDs_2\n        self.n_channels = n_channels\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def on_epoch_end(self):\n        self.indexes_1 = np.arange(len(self.list_IDs_1))\n        self.indexes_2 = np.arange(len(self.list_IDs_2))\n\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes_1)\n            np.random.shuffle(self.indexes_2)\n    \n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs_1) / self.batch_size))\n\n    def __data_generation(self, list_IDs_temp_1, list_IDs_temp_2):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X1 = np.empty((self.batch_size, *self.dim, self.n_channels))\n        X2 = np.empty((self.batch_size, *self.dim, self.n_channels))\n        y1 = np.empty((self.batch_size, *self.dim), dtype=int)\n        y2 = np.empty((self.batch_size, *self.dim), dtype=int)\n        y3 = np.empty((self.batch_size, *self.dim), dtype=int)\n        y4 = np.empty((self.batch_size, *self.dim), dtype=int)\n        y5 = np.empty((self.batch_size, *self.dim), dtype=int)\n        y6 = np.empty((self.batch_size, *self.dim), dtype=int)\n        y7 = np.empty((self.batch_size, *self.dim), dtype=int)       \n        \n        # Generate data\n        for i, ID in enumerate(list_IDs_temp_1):      \n            \n            # Store sample\n            img = cv2.imread(ID[0])\n            \n            img1 = cv2.imread(self.labels[ID[0]][0])[:,:,0]\n            img2 = cv2.imread(self.labels[ID[0]][1])[:,:,0]\n            img3 = cv2.imread(self.labels[ID[0]][2])[:,:,0]\n            img4 = cv2.imread(self.labels[ID[0]][3])[:,:,0]\n            img5 = cv2.imread(self.labels[ID[0]][4])[:,:,0]\n            img6 = cv2.imread(self.labels[ID[0]][5])[:,:,0]\n            img7 = cv2.imread(self.labels[ID[0]][6])[:,:,0]\n            \n            labs=[img1,img2,img3,img4,img5,img6,img7]\n            \n            # aug. dataset\n            img_1, labs_1 = augment_using_layers(img, labs, (hyperparameters['height'], hyperparameters['width']))            \n            X1[i,] = img_1\n                        \n            y1[i, ] = labs_1[0]\n            y2[i, ] = labs_1[1]\n            y3[i, ] = labs_1[2]\n            y4[i, ] = labs_1[3]\n            y5[i, ] = labs_1[4]\n            y6[i, ] = labs_1[5]\n            y7[i, ] = labs_1[6]\n            \n        for i, ID in enumerate(list_IDs_temp_2):\n            \n            img = cv2.imread(ID[0])\n            img_1, labs_1 = augment_using_layers(img, None, (hyperparameters['height'], hyperparameters['width']))            \n            \n            X2[i,] = img_1\n\n        return X1, X2, [y1, y2, y3, y4, y5, y6, y7]\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes_1 = self.indexes_1[index*self.batch_size:(index+1)*self.batch_size]\n        indexes_2 = self.indexes_2[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp_1 = [self.list_IDs_1[k] for k in indexes_1]\n        list_IDs_temp_2 = [self.list_IDs_2[k] for k in indexes_2]\n\n        # Generate data\n        X1, X2, Y = self.__data_generation(list_IDs_temp_1, list_IDs_temp_2)\n\n        return X1, X2, Y","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:43.724493Z","iopub.execute_input":"2023-06-21T07:16:43.724937Z","iopub.status.idle":"2023-06-21T07:16:43.751350Z","shell.execute_reply.started":"2023-06-21T07:16:43.724893Z","shell.execute_reply":"2023-06-21T07:16:43.750315Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# from tf.keras.callbacks import LambdaCallback\n# [LambdaCallback(on_epoch_end=generator.on_epoch_end)]","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:43.753103Z","iopub.execute_input":"2023-06-21T07:16:43.753487Z","iopub.status.idle":"2023-06-21T07:16:43.766553Z","shell.execute_reply.started":"2023-06-21T07:16:43.753458Z","shell.execute_reply":"2023-06-21T07:16:43.765609Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"(train, val) = preprocess(hyperparameters)\n\n# Generators\ntrain_gen_full = DataGenerator(partition['train'], \n                                   labels, \n                                   dim = (hyperparameters['height'],hyperparameters['width']),\n                                   batch_size= hyperparameters['batch_size'],\n                                   n_channels = hyperparameters['channels'],\n                                   shuffle = True)\n\nval_gen = DataGenerator(partition['val'], \n                                   labels, \n                                   dim = (hyperparameters['height'],hyperparameters['width']),\n                                   batch_size= hyperparameters['batch_size'],\n                                   n_channels =hyperparameters['channels'],\n                                   shuffle = False,\n                                   is_validation=True)\n\npretrain_gen = MultipleGenerator(train.values, \n                                      val.values, \n                                      labels,\n                                      batch_size= hyperparameters['batch_size'],\n                                      dim = (hyperparameters['height'],hyperparameters['width']),\n                                      n_channels = hyperparameters['channels'],\n                                      shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:43.769352Z","iopub.execute_input":"2023-06-21T07:16:43.769997Z","iopub.status.idle":"2023-06-21T07:16:43.815294Z","shell.execute_reply.started":"2023-06-21T07:16:43.769965Z","shell.execute_reply":"2023-06-21T07:16:43.813987Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# for i in labs_1:\n#     print((i.numpy()==0).sum() + (i.numpy()==1).sum())\n#     if ((i.numpy()==0).sum() + (i.numpy()==1).sum()) !=32768:\n#         print(1)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:43.816799Z","iopub.execute_input":"2023-06-21T07:16:43.817447Z","iopub.status.idle":"2023-06-21T07:16:43.821813Z","shell.execute_reply.started":"2023-06-21T07:16:43.817405Z","shell.execute_reply":"2023-06-21T07:16:43.820551Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"## Architecture","metadata":{}},{"cell_type":"code","source":"def discriminator():\n    \n    X = Input((720,), name='input_disc')\n    \n    x = Dense(units = 512, activation = None, name='dense_1_disc')(X)\n    x = BatchNormalization(name = 'bn_1_disc')(x)\n    x = ReLU(name='relu_1_disc')(x)\n    x = Dense(units = 512, activation = None, name='dense_2_disc')(x)\n    x = BatchNormalization(name = 'bn_2_disc')(x)\n    x = ReLU(name='relu_2_disc')(x)\n    x = Dense(units = 1, activation = 'sigmoid', name='output_disc')(x)\n    \n    return X,x","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:43.823250Z","iopub.execute_input":"2023-06-21T07:16:43.824466Z","iopub.status.idle":"2023-06-21T07:16:43.835452Z","shell.execute_reply.started":"2023-06-21T07:16:43.824425Z","shell.execute_reply":"2023-06-21T07:16:43.834082Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def variationalAutoEncoder(embed_size=360):\n    \n    img_input = Input(shape=(hyperparameters['height'],hyperparameters['width'],hyperparameters['channels']), name = 'input_vae')\n\n    x = Conv2D(64, (3, 3), activation='relu', padding='same',\n               name='conv_1_vae')(img_input)\n    x = Conv2D(64, (3, 3), activation='relu', padding='same',\n               name='conv_2_vae')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='pooling_1_vae')(x)\n    \n    # Block 2\n    x = Conv2D(128, (3, 3), activation='relu', padding='same',\n               name='conv_3_vae')(x)\n    x = Conv2D(128, (3, 3), activation='relu', padding='same',\n               name='conv_4_vae')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='pooling_2_vae')(x)\n    \n    # Block 3\n    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n               name='conv_5_vae')(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n               name='conv_6_vae')(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n               name='conv_7_vae')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='pooling_3_vae')(x)\n    \n    # Block 4\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='conv_8_vae')(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='conv_9_vae')(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='conv_10_vae')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='pooling_4_vae')(x)\n\n    # Block 5\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='conv_11_vae')(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='conv_12_vae')(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='conv_13_vae')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='pooling_5_vae')(x)\n    f5 = x\n    \n    x = Conv2D(32, (1, 1), activation='relu', padding='same',\n               name='conv_14_vae')(x)    \n    \n    x = Flatten(name='flatten_1_vae')(x)\n    mu = Dense(units = embed_size, activation = None, name='dense_1_vae')(x)\n    \n    x = Dense(units = 1024, activation = None, name='dense_2_vae')(mu)\n    x = Reshape((4,8,32), name='reshape_vae')(x)\n    x = Conv2DTranspose(512, kernel_size=(1,1), padding='same', use_bias=False, name='conv_15_vae')(x)\n    \n    x = (ZeroPadding2D((1, 1), name='zero_padding_1_vae'))(x)\n    x = (Conv2D(512, (3, 3), padding='valid', name='conv_16_vae'))(x)\n    x = (BatchNormalization(name = 'bn_1_vae'))(x)\n    x = tf.keras.layers.ReLU(name='relu_vae_1')(x)\n    \n    x = (UpSampling2D((2, 2), name='upsampling_1_vae'))(x)\n    x = (ZeroPadding2D((1, 1), name='zero_padding_2_vae'))(x)\n    x = (Conv2D(256, (3, 3), padding='valid' ,name='conv_17_vae'))(x)\n    x = (BatchNormalization(name = 'bn_2_vae'))(x)\n    x = tf.keras.layers.ReLU(name='relu_vae_2')(x)\n\n    x = (UpSampling2D((2, 2),  name='upsampling_2_vae'))(x)\n    x = (ZeroPadding2D((1, 1),  name='zero_padding_3_vae'))(x)\n    x = (Conv2D(128, (3, 3), padding='valid', name='conv_18_vae'))(x)\n    x = (BatchNormalization(name = 'bn_3_vae'))(x)\n    x = tf.keras.layers.ReLU(name='relu_vae_3')(x)\n\n    x = (UpSampling2D((2, 2),  name='upsampling_3_vae'))(x)\n    x = (ZeroPadding2D((1, 1),  name='zero_padding_4_vae'))(x)\n    x = (Conv2D(128, (3, 3), padding='valid', name='conv_19_vae'))(x)\n    x = (BatchNormalization(name = 'bn_4_vae'))(x)\n    x = tf.keras.layers.ReLU(name='relu_vae_4')(x)\n\n    x = (UpSampling2D((2, 2),  name='upsampling_4_vae'))(x)\n    x = (ZeroPadding2D((1, 1),  name='zero_padding_5_vae'))(x)\n    x = (Conv2D(3, (3, 3), padding='valid', name='conv_20_vae'))(x)\n    x = (BatchNormalization(name = 'bn_5_vae'))(x)\n    x = tf.keras.layers.ReLU(name='relu_vae_5')(x)\n\n    x = (UpSampling2D((2, 2),  name='upsampling_5_vae'))(x)\n    x = (ZeroPadding2D((1, 1),  name='zero_padding_6_vae'))(x)\n    x = (Conv2D(3, (3, 3), padding='valid', name='conv_21_vae'))(x)\n    x = (BatchNormalization(name = 'bn_6_vae'))(x)\n    x = tf.keras.layers.ReLU(name='relu_vae_6')(x)\n\n    return img_input, x, mu","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:43.837183Z","iopub.execute_input":"2023-06-21T07:16:43.837872Z","iopub.status.idle":"2023-06-21T07:16:43.868552Z","shell.execute_reply.started":"2023-06-21T07:16:43.837841Z","shell.execute_reply":"2023-06-21T07:16:43.867553Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# # defining my generator\n# X, z, mu = variationalAutoEncoder()\n# vae = Model(inputs = X, outputs = [z,mu])\n# vae.compile(optimizer = keras.optimizers.RMSprop(learning_rate=hyperparameters['lr'],\n#                                                  clipnorm=1.0 ))","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:43.870087Z","iopub.execute_input":"2023-06-21T07:16:43.870531Z","iopub.status.idle":"2023-06-21T07:16:43.887157Z","shell.execute_reply.started":"2023-06-21T07:16:43.870499Z","shell.execute_reply":"2023-06-21T07:16:43.886053Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def get_vgg_encoder(input_height=512, input_width=1024, pretrained='imagenet', channels=3):\n\n    assert input_height % 32 == 0\n    assert input_width % 32 == 0\n\n    img_input = Input(shape=(input_height, input_width, channels), name = 'input_pred')\n\n    x = Conv2D(64, (3, 3), activation='relu', padding='same',\n               name='block1_conv1')(img_input)\n    x = Conv2D(64, (3, 3), activation='relu', padding='same',\n               name='block1_conv2')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n    \n    f1 = x\n    # Block 2\n    x = Conv2D(128, (3, 3), activation='relu', padding='same',\n               name='block2_conv1')(x)\n    x = Conv2D(128, (3, 3), activation='relu', padding='same',\n               name='block2_conv2')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n    f2 = x\n\n    # Block 3\n    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n               name='block3_conv1')(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n               name='block3_conv2')(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same',\n               name='block3_conv3')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n    f3 = x\n\n    # Block 4\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block4_conv1')(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block4_conv2')(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block4_conv3')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n    f4 = x\n\n    # Block 5\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block5_conv1')(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block5_conv2')(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same',\n               name='block5_conv3')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n    f5 = x\n\n    return img_input, [f1, f2, f3, f4, f5]","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:43.888637Z","iopub.execute_input":"2023-06-21T07:16:43.889501Z","iopub.status.idle":"2023-06-21T07:16:43.907105Z","shell.execute_reply.started":"2023-06-21T07:16:43.889454Z","shell.execute_reply":"2023-06-21T07:16:43.905853Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def segnet_decoder(f, n_classes, n_up=3):\n\n    assert n_up >= 2\n    \n    ## added ---------------------------------------------------------------\n    f = Conv2D(360, (1, 1), padding='valid', name='conv_1_pred')(f) \n    f = GlobalAveragePooling2D(name = \"gap_1_pred\")(f)\n    \n    f = Dense(units = 360, activation = None, name='dense_1_pred')(f)  \n    g = f\n    f = Dense(units = 64*128*1, activation = None, name='dense_2_pred')(f)  \n    f = Reshape((64,128,1), name='reshape_1_pred')(f)\n    f = (Conv2D(256, (1, 1), padding='valid', name='conv_2_pred'))(f)\n    ## --------------------------------------------------------------------\n\n    o = f\n    o = (ZeroPadding2D((1, 1), name = 'zero_1_pred'))(o)\n    o = (Conv2D(512, (3, 3), padding='valid', name='conv_3_pred'))(o)\n    o = (BatchNormalization(name = 'bn_1_pred'))(o)\n    \n    o = tf.keras.layers.SpatialDropout2D(0.3)(o)\n\n    o = (UpSampling2D((2, 2), name = 'upsampling_1_pred'))(o)\n    o = (ZeroPadding2D((1, 1), name = 'zero_2_pred'))(o)\n    o = (Conv2D(256, (3, 3), padding='valid', name='conv_4_pred'))(o)\n    o = (BatchNormalization(name = 'bn_2_pred'))(o)\n    \n#     o = (UpSampling2D((2, 2), name = 'upsampling_2_pred'))(o)\n#     o = (ZeroPadding2D((1, 1), name = 'zero_3_pred'))(o)\n#     o = (Conv2D(128, (3, 3), padding='valid', name = 'conv_5_pred'))(o)\n#     o = (BatchNormalization(name = 'bn_3_pred'))(o)\n\n#     o = (UpSampling2D((2, 2), name = 'upsampling_3_pred'))(o)\n#     o = (ZeroPadding2D((1, 1), name = 'zero_4_pred'))(o)\n#     o = (Conv2D(64, (3, 3), padding='valid', name = 'conv_6_pred'))(o)\n#     o = (BatchNormalization(name = 'bn_4_pred'))(o)\n    \n    t1 = []\n    for i in range(hyperparameters['num_tasks']):\n        t1.append(Conv2D(1, (3, 3), padding='same', name = 'conv_7_'+ str(i) +'_pred', activation='sigmoid')(o))\n    \n    return t1, g","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:43.908664Z","iopub.execute_input":"2023-06-21T07:16:43.909186Z","iopub.status.idle":"2023-06-21T07:16:43.923828Z","shell.execute_reply.started":"2023-06-21T07:16:43.909128Z","shell.execute_reply":"2023-06-21T07:16:43.923050Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def get_segmentation_model(input, output):\n\n    img_input = input\n    o1,o2 = output\n\n#     o_shape = Model(img_input, [o1,o2]).output_shape\n#     i_shape = Model(img_input, [o1,o2]).input_shape\n    \n#     IMAGE_ORDERING = 'channels_last'\n#     output_height = o_shape[0][1]\n#     output_width = o_shape[0][2]\n#     input_height = i_shape[1]\n#     input_width = i_shape[2]\n#     n_classes = o_shape[0][3]\n    \n    # o1 = (Reshape((output_height*output_width, -1), name='reshape_2_pred'))(o1)\n\n    # o1 = (tf.keras.layers.Softmax(name='activation_1_pred'))(o1)\n    model = Model(img_input, [o1, o2])\n    \n    # model.output_width = output_width\n    # model.output_height = output_height\n    \n#     model.n_classes = n_classes\n#     model.input_height = input_height\n#     model.input_width = input_width\n#     model.model_name = \"\"\n\n    return model\n\ndef vgg_segnet(n_classes, input_height=416, input_width=608, encoder_level=3, channels=3):\n\n    model = _segnet(n_classes, get_vgg_encoder,  input_height=input_height,\n                    input_width=input_width, encoder_level=encoder_level, channels=channels)\n    model.model_name = \"vgg_segnet\"\n    return model\n\ndef _segnet(n_classes, encoder,  input_height=512, input_width=512,\n            encoder_level=3, channels=3):\n\n    img_input, levels = encoder(\n        input_height=input_height,  input_width=input_width, channels=channels)\n\n    feat = levels[encoder_level]\n    o = segnet_decoder(feat, n_classes, n_up=3)\n    model = get_segmentation_model(img_input, o)\n\n    return model","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:43.925344Z","iopub.execute_input":"2023-06-21T07:16:43.925874Z","iopub.status.idle":"2023-06-21T07:16:43.941238Z","shell.execute_reply.started":"2023-06-21T07:16:43.925833Z","shell.execute_reply":"2023-06-21T07:16:43.940281Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# pred_model = vgg_segnet(n_classes = hyperparameters['num_tasks'], input_height=hyperparameters['height'], input_width=hyperparameters['width'], encoder_level=2, channels=hyperparameters['channels'])\n# pred_model.compile(optimizer = keras.optimizers.SGD(learning_rate=hyperparameters['lr'],\n#                                                 clipnorm=1.0 ))","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:43.942654Z","iopub.execute_input":"2023-06-21T07:16:43.943080Z","iopub.status.idle":"2023-06-21T07:16:43.954867Z","shell.execute_reply.started":"2023-06-21T07:16:43.943044Z","shell.execute_reply":"2023-06-21T07:16:43.953796Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"## Active Learning","metadata":{}},{"cell_type":"code","source":"class ActiveLearning(keras.Model):\n    def __init__(self, discriminator, generator, predictor, hyperparameters, trackers):\n        super(ActiveLearning, self).__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.predictor = predictor\n        self.hyperparameters = hyperparameters\n        self.trackers = trackers\n\n    def compile(self, d_optimizer, g_optimizer, p_optimizer):\n        super(ActiveLearning, self).compile()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.p_optimizer = p_optimizer\n                \n    def train_step(self, real_images):\n        \n        # get labelled_x, unlabelled_x and labelled_y\n        \n        x = real_images\n        labelled_x = x[0]\n        unlabelled_x = x[1]\n        labelled_y = x[2]\n\n        ##### TRAIN THE PREDICTOR #####\n        \n        with tf.GradientTape() as tape:\n            labelled_prediction_y,_ = self.predictor(labelled_x, training=True)\n\n            re_1=[]\n            for i in range(7):\n                re_1.append(tf.reshape(labelled_y[i], (hyperparameters['batch_size'], -1)))\n\n            re_2 = []\n            for i in range(7):\n                re_2.append(tf.reshape(labelled_prediction_y[i], (hyperparameters['batch_size'], -1)))\n\n            weights = [{1: 1.0123, 0: 0.9879},\n             {0: 0.6377, 1: 2.3142},\n             {0: 0.5089, 1: 28.4156},\n             {0: 0.5185, 1: 14.0129},\n             {0: 0.5894, 1: 3.2964},\n             {0: 0.5060, 1: 41.6062},\n             {0: 0.5394, 1: 6.8425}]\n\n            w = []\n            for i in range(7):\n                wts = np.zeros((re_1[i].shape))\n                for j in range(re_1[i].shape[0]):\n                    t = re_1[i][j].numpy()\n                    t = t.astype('float64')\n                    wts[j][t==0] = weights[i][0]\n                    wts[j][t==1] = weights[i][1]\n\n                wts=tf.convert_to_tensor(wts)\n                predictor_loss = weighted_cross_entropy(re_1[i], re_2[i], sample_weights=wts)\n                w.append(predictor_loss)\n            w = tf.convert_to_tensor(w)\n            w = tf.math.reduce_mean(w, axis=1) +  tf.cast(dice_coef_loss(re_1, re_2), tf.float32)\n\n            #l = tfa.losses.SigmoidFocalCrossEntropy()\n            # predictor_loss = l(re_1, re_2)\n            #w = tf.math.reduce_mean(predictor_loss)\n\n        trainable_vars = self.predictor.trainable_variables\n        gradients = tape.gradient(w, trainable_vars)\n\n        # Update weights\n        self.p_optimizer.apply_gradients(zip(gradients, trainable_vars))        \n        \n#         # Compute output and latents\n#         weights = {'0': 0.555, \n#                    '1': 5.017}\n#         with tf.GradientTape() as tape:\n#             labelled_prediction_y, _ = self.predictor(labelled_x, training=True)\n# #             re_1 = tf.reshape(labelled_y, (hyperparameters['batch_size'], -1))\n# #             index = tf.where(tf.not_equal(re_1, tf.constant(255, dtype=tf.int64)))\n# #             a1 = tf.gather_nd(re_1, index)\n# #             re_2 = tf.reshape(labelled_prediction_y, (hyperparameters['batch_size'], -1, hyperparameters['num_tasks']))\n# #             a2 = tf.gather_nd(re_2, index) ## -----> check\n# #             a1 = tf.convert_to_tensor(tf.keras.utils.to_categorical(a1, num_classes=hyperparameters['num_tasks']))    \n# #             predictor_loss = soft_dice_loss(a1, a2)\n#             re_1 = tf.reshape(labelled_y, (hyperparameters['batch_size'], -1))\n#             index = tf.where(tf.not_equal(re_1, tf.constant(255, dtype=tf.int64)))\n#             a1 = tf.gather_nd(re_1, index)\n#             re_2 = tf.reshape(labelled_prediction_y, (hyperparameters['batch_size'], -1, hyperparameters['num_tasks']))\n#             a2 = tf.gather_nd(re_2, index)\n\n#             wts = np.ones((a1.shape))\n#             for i in range(a1.shape[0]):\n#                 if a1[i] == 0:\n#                     wts[i] = weights['0']\n#                 elif a1[i] == 1:\n#                     wts[i] = weights['1']\n#             wts=tf.convert_to_tensor(wts)\n\n#             l = tf.keras.losses.SparseCategoricalCrossentropy(reduction = tf.keras.losses.Reduction.NONE)\n#             predictor_loss = l(a1, a2, sample_weight=wts) # ----> 1\n#             predictor_loss = tf.math.reduce_mean(predictor_loss)\n            \n#             # l = tf.keras.losses.SparseCategoricalCrossentropy(reduction = 'auto')\n#             # predictor_loss = l(a1, a2) # ----> 1        \n\n        \n#         # Compute gradients\n#         trainable_vars = self.predictor.trainable_variables\n#         gradients = tape.gradient(predictor_loss, trainable_vars)\n        \n#         # Update weights\n#         self.p_optimizer.apply_gradients(zip(gradients, trainable_vars)) \n        \n        # ------------------------------------------------------------------------------------------------\n        \n        ##### TRAIN THE GENERATOR #####\n        \n        # Create labels for VAE\n        labelled_disc_true = np.ones((self.hyperparameters['batch_size'],1))\n        unlabelled_disc_fake = np.ones((self.hyperparameters['batch_size'],1))\n        \n        # Compute VAE outputs\n        with tf.GradientTape() as tape:\n            # Compute generator o/p\n            labelled_vae_y, labelled_vae_latent = self.generator(labelled_x)\n            unlabelled_vae_y, unlabelled_vae_latent = self.generator(unlabelled_x)\n            \n            # Calculate loss for VAE\n            labelled_vae_loss = keras.losses.mean_squared_error(labelled_x, labelled_vae_y) # ----> 2\n            unlabelled_vae_loss = keras.losses.mean_squared_error(unlabelled_x, unlabelled_vae_y) # ----> 2\n            \n            vae_loss = labelled_vae_loss + unlabelled_vae_loss #+ (self.advisory_param*disc_loss)\n        \n        # Compute gradients\n        trainable_vars = self.generator.trainable_variables\n        gradients = tape.gradient(vae_loss, trainable_vars)\n        \n        # Update weights\n        self.g_optimizer.apply_gradients(zip(gradients, trainable_vars))         \n        \n        # ------------------------------------------------------------------------------------------------\n        \n        ##### TRAIN THE DISCRIMINATOR #####\n        \n        # Create disc labels\n        labelled_disc_true = np.ones((self.hyperparameters['batch_size'],1))\n        unlabelled_disc_true = np.zeros((self.hyperparameters['batch_size'],1))\n        \n        # Compute VAE latents\n        _, labelled_vae_latent = self.generator(labelled_x, training = False)\n        _, unlabelled_vae_latent = self.generator(unlabelled_x, training = False)\n        \n        # Compute predictor latents\n        _, labelled_predictor_latent = self.predictor(labelled_x, training=False)\n        _, unlabelled_predictor_latent = self.predictor(unlabelled_x, training=False)\n        \n#         labelled_predictor_latent = math_ops.mean(ops.convert_to_tensor(labelled_predictor_latent), axis=0)\n#         unlabelled_predictor_latent = math_ops.mean(ops.convert_to_tensor(unlabelled_predictor_latent), axis=0)\n        \n        # Join vae and predictor latents\n        labelled_disc_in = tf.concat([labelled_vae_latent,labelled_predictor_latent],axis=1)\n        unlabelled_disc_in = tf.concat([unlabelled_vae_latent,unlabelled_predictor_latent],axis=1)\n        \n        # Compute disc output\n        with tf.GradientTape() as tape:\n            labelled_disc_y = self.discriminator(labelled_disc_in,training=True)\n            unlabelled_disc_y = self.discriminator(unlabelled_disc_in,training=True)\n            \n            labelled_disc_loss = keras.losses.binary_crossentropy(labelled_disc_true, labelled_disc_y) # ----> 3\n            unlabelled_dic_loss = keras.losses.binary_crossentropy(unlabelled_disc_true, unlabelled_disc_y) # ----> 3\n            \n            disc_loss = labelled_disc_loss + unlabelled_dic_loss\n        \n        # Compute gradients\n        trainable_vars = self.discriminator.trainable_variables\n        gradients = tape.gradient(disc_loss, trainable_vars)\n        \n        # Update weights\n        self.d_optimizer.apply_gradients(zip(gradients, trainable_vars)) \n    \n        # ------------------------------------------------------------------------------------------------\n        \n        # Computing Metrics\n        \n        # For predictor\n#         wts = np.ones((a1.shape))\n#         for i in range(a1.shape[0]):\n#             if a1[i] == 0:\n#                 wts[i] = weights['0']\n#             elif a1[i] == 1:\n#                 wts[i] = weights['1']\n#         wts=tf.convert_to_tensor(wts)\n        \n#         predictor_loss = l(a1, a2, sample_weight=wts) # ----> 1\n#         predictor_loss = tf.math.reduce_mean(predictor_loss)#soft_dice_loss(a1, a2)\n#         predictor_loss = l(re_1, re_2)\n#         predictor_loss = tf.math.reduce_mean(predictor_loss)\n        self.trackers['loss_tracker_predictor'].update_state(w) # predictor_loss\n        \n#         re_1 = tf.reshape(labelled_y, (self.hyperparameters['batch_size'], -1))\n#         index = tf.where(tf.not_equal(re_1, tf.constant(255, dtype=tf.int64)))\n#         a1 = tf.gather_nd(re_1, index)\n#         re_2 = tf.reshape(labelled_prediction_y, (hyperparameters['batch_size'], -1, hyperparameters['num_tasks']))\n#         a2 = tf.gather_nd(re_2, index) ## -----> check\n        \n        # self.trackers['acc_metric_predictor'].update_state(a1, a2)\n        self.trackers['acc_metric_predictor'].update_state(re_1, re_2)\n        \n        # f1 for predictor\n\n#         a2 = np.argmax(a2.numpy(), axis=1)\n#         a1 = a1.numpy()\n#         l1 = f1_score(a1, a2, average='weighted', zero_division=1)\n#         l2 = f1_score(a1, a2, labels=list(range(0,self.hyperparameters['num_tasks'])), average=None, zero_division=1)\n#         cm = confusion_matrix(a1,a2, labels=list(range(0,self.hyperparameters['num_tasks'])))\n#         FP = cm.sum(axis=0) - np.diag(cm)  \n#         FN = cm.sum(axis=1) - np.diag(cm)\n#         TP = np.diag(cm)\n#         TN = cm.sum() - (FP + FN + TP)\n        \n#         self.trackers['f1_predictor'].update_state(l1)\n#         self.trackers['IOU_predictor'].update_state(a1,a2)\n#         rec = sklearn.metrics.recall_score(a1,a2, average=None, labels=list(range(0,self.hyperparameters['num_tasks'])), zero_division=1)\n        for task in range(self.hyperparameters['num_tasks']):\n            # self.trackers['individual_f1_predictor'][task].update_state(l2[task])\n            acc=tf.keras.metrics.BinaryAccuracy()\n            accuracy = acc(re_1[task], re_2[task])\n            #acc = rec[task]\n            #self.trackers['individual_acc_predictor'][task].update_state(acc)\n            self.trackers['individual_acc_predictor'][task].update_state(accuracy)\n            self.trackers['individual_IOU_predictor'][task].update_state(re_1[task], tf.cast(re_2[task]>=0.5, tf.int64))\n        \n        # For VAE\n        self.trackers['loss_tracker_generator'].update_state(labelled_x, labelled_vae_y)\n        self.trackers['loss_tracker_generator'].update_state(unlabelled_x, unlabelled_vae_y)\n        # For Discriminator\n        self.trackers['loss_tracker_disc'].update_state(labelled_disc_true,labelled_disc_y)\n        self.trackers['loss_tracker_disc'].update_state(unlabelled_disc_true,unlabelled_disc_y)\n        self.trackers['acc_tracker_disc'].update_state(labelled_disc_true,labelled_disc_y)\n        self.trackers['acc_tracker_disc'].update_state(unlabelled_disc_true,unlabelled_disc_y)\n            \n        ret_dic = {\"loss_predictor_total\": self.trackers['loss_tracker_predictor'].result(), # loss_tracker_predictor.result(), \n                   \"acc_predictor\":self.trackers['acc_metric_predictor'].result(), # acc_metric_predictor.result(), \n                   #'f1_predictor': self.trackers['f1_predictor'].result(),\n                   \"loss_VAE\":  self.trackers['loss_tracker_generator'].result(), # loss_tracker_generator.result(),\n                   \"loss_disc\": self.trackers['loss_tracker_disc'].result(), # loss_tracker_disc.result(),\n                   \"acc_disc\": self.trackers['acc_tracker_disc'].result(), # acc_tracker_disc.result()\n                   #\"IOU_predictor\": self.trackers['IOU_predictor'].result()}\n                  }\n\n        for i in range(hyperparameters['num_tasks']):\n            ret_dic[\"acc_predictor_\"+str(i)] = self.trackers['individual_acc_predictor'][i].result() # individual_acc_metric_predictor[i].result()\n            ret_dic[\"IOU_predictor_\"+str(i)] = self.trackers['individual_IOU_predictor'][i].result() # individual_acc_metric_predictor[i].result()\n            #ret_dic[\"f1_predictor_\"+str(i)] = self.trackers['individual_f1_predictor'][i].result() # individual_acc_metric_predictor[i].result()\n\n        return ret_dic\n    \n    def call(self, x):\n        return\n    \n    def test_step(self, real_images):\n        \n        x = real_images\n        labelled_x = x[0]\n        labelled_y = x[1]\n        \n        # Predictor step\n        labelled_prediction_y, labelled_predictor_latent = self.predictor(labelled_x, training=False)\n        \n        # Generator step\n        labelled_vae_y, labelled_vae_latent = self.generator(labelled_x, training=False)\n        \n        # Discriminator step\n#         labelled_predictor_latent = math_ops.mean(ops.convert_to_tensor(labelled_predictor_latent), axis=0)\n        labelled_disc_in = tf.concat([labelled_vae_latent,labelled_predictor_latent],axis=1)\n        \n        labelled_disc_y = self.discriminator(labelled_disc_in,training=False)\n        \n        # Updating metrics\n        \n        # For Predictor\n#         re_1 = tf.reshape(labelled_y, (hyperparameters['batch_size'], -1))\n#         index = tf.where(tf.not_equal(re_1, tf.constant(255, dtype=tf.int64)))\n#         a1 = tf.gather_nd(re_1, index)\n#         re_2 = tf.reshape(labelled_prediction_y, (hyperparameters['batch_size'], -1, hyperparameters['num_tasks']))\n#         a2 = tf.gather_nd(re_2, index) ## -----> check\n#         a1 = tf.convert_to_tensor(tf.keras.utils.to_categorical(a1, num_classes=hyperparameters['num_tasks']))    \n#         predictor_loss = soft_dice_loss(a1, a2)\n\n#         re_1 = tf.reshape(labelled_y, (hyperparameters['batch_size'], -1))\n#         index = tf.where(tf.not_equal(re_1, tf.constant(255, dtype=tf.int64)))\n#         a1 = tf.gather_nd(re_1, index)\n#         re_2 = tf.reshape(labelled_prediction_y, (hyperparameters['batch_size'], -1, hyperparameters['num_tasks']))\n#         a2 = tf.gather_nd(re_2, index)\n\n#         l = tf.keras.losses.SparseCategoricalCrossentropy(reduction = 'auto')\n#         predictor_loss = l(a1, a2) # ----> 1 \n#         self.trackers['loss_tracker_predictor'].update_state(a1,a2)\n\n#         re_1 = tf.reshape(labelled_y, (self.hyperparameters['batch_size'], -1))\n#         index = tf.where(tf.not_equal(re_1, tf.constant(255, dtype=tf.int64)))\n#         a1 = tf.gather_nd(re_1, index)\n#         re_2 = tf.reshape(labelled_prediction_y, (hyperparameters['batch_size'], -1, hyperparameters['num_tasks']))\n#         a2 = tf.gather_nd(re_2, index) ## -----> check\n        \n#         self.trackers['acc_metric_predictor'].update_state(a1, a2)\n\n        re_1=[]\n        for i in range(7):\n            re_1.append(tf.reshape(labelled_y[i], (hyperparameters['batch_size'], -1)))\n\n        re_2 = []\n        for i in range(7):\n            re_2.append(tf.reshape(labelled_prediction_y[i], (hyperparameters['batch_size'], -1)))\n            \n        w = []\n        for i in range(7):\n            wts = np.ones((re_1[i].shape))\n            wts=tf.convert_to_tensor(wts)\n            predictor_loss = weighted_cross_entropy(re_1[i], re_2[i], sample_weights=wts)\n            w.append(predictor_loss)\n        w = tf.convert_to_tensor(w)\n        w = tf.math.reduce_mean(w, axis=1) +  tf.cast(dice_coef_loss(re_1, re_2), tf.float32)\n        \n\n#         l = tf.keras.losses.BinaryCrossentropy(from_logits=False, reduction = tf.keras.losses.Reduction.NONE)\n#         predictor_loss = l(re_1, re_2) # ----> \n#         predictor_loss = tf.math.reduce_mean(predictor_loss)\n\n#         l = tfa.losses.SigmoidFocalCrossEntropy()\n#         predictor_loss = l(re_1, re_2)\n#         w = tf.math.reduce_mean(predictor_loss)\n#         w = dice_coef_loss(re_1, re_2)\n        self.trackers['loss_tracker_predictor'].update_state(w)\n        self.trackers['acc_metric_predictor'].update_state(re_1, re_2)\n        \n        # f1 for predictor\n#         a2 = np.argmax(a2.numpy(), axis=1)\n#         a1 = a1.numpy()\n#         l1 = f1_score(a1, a2, average='weighted', zero_division=1)\n#         l2 = f1_score(a1, a2, labels=list(range(0,self.hyperparameters['num_tasks'])), average=None, zero_division=1)\n#         cm = confusion_matrix(a1,a2, labels=list(range(0,self.hyperparameters['num_tasks'])))\n#         FP = cm.sum(axis=0) - np.diag(cm)  \n#         FN = cm.sum(axis=1) - np.diag(cm)\n#         TP = np.diag(cm)\n#         TN = cm.sum() - (FP + FN + TP)\n        \n        #self.trackers['f1_predictor'].update_state(l1)\n        #self.trackers['IOU_predictor'].update_state(a1,a2)\n        #rec = sklearn.metrics.recall_score(a1,a2, average=None, labels=list(range(0,self.hyperparameters['num_tasks'])), zero_division=1)\n        for task in range(self.hyperparameters['num_tasks']):\n            #self.trackers['individual_f1_predictor'][task].update_state(l2[task])\n            # acc = rec[task]\n            # self.trackers['individual_acc_predictor'][task].update_state(acc)\n            acc=tf.keras.metrics.BinaryAccuracy()\n            accuracy = acc(re_1[task], re_2[task])\n            #acc = rec[task]\n            #self.trackers['individual_acc_predictor'][task].update_state(acc)\n            self.trackers['individual_acc_predictor'][task].update_state(accuracy)\n            self.trackers['individual_IOU_predictor'][task].update_state(re_1[task], tf.cast(re_2[task]>=0.5, tf.int64))\n\n        self.trackers['loss_tracker_generator'].update_state(labelled_x, labelled_vae_y)\n        \n        # For Discriminator\n        labelled_disc_true = np.ones((self.hyperparameters['batch_size'],1))\n        self.trackers['loss_tracker_disc'].update_state(labelled_disc_true,labelled_disc_y)\n        self.trackers['acc_tracker_disc'].update_state(labelled_disc_true,labelled_disc_y)\n        \n        ret_dic = {\"loss_predictor_total\": self.trackers['loss_tracker_predictor'].result(), # loss_tracker_predictor.result(), \n                   \"acc_predictor\":self.trackers['acc_metric_predictor'].result(), # acc_metric_predictor.result(), \n                   #'f1_predictor': self.trackers['f1_predictor'].result(),\n                   \"loss_VAE\":  self.trackers['loss_tracker_generator'].result(), # loss_tracker_generator.result(),\n                   \"loss_disc\": self.trackers['loss_tracker_disc'].result(), # loss_tracker_disc.result(),\n                   \"acc_disc\": self.trackers['acc_tracker_disc'].result(), # acc_tracker_disc.result()\n                   #\"IOU_predictor\": self.trackers['IOU_predictor'].result()}\n                  }\n        \n        for i in range(hyperparameters['num_tasks']):\n            ret_dic[\"acc_predictor_\"+str(i)] = self.trackers['individual_acc_predictor'][i].result() # individual_acc_metric_predictor[i].result()\n            #ret_dic[\"f1_predictor_\"+str(i)] = self.trackers['individual_f1_predictor'][i].result() # individual_acc_metric_predictor[i].result()\n            ret_dic[\"IOU_predictor_\"+str(i)] = self.trackers['individual_IOU_predictor'][i].result() # individual_acc_metric_predictor[i].result()\n       \n        return ret_dic        \n    \n    def predict_step(self, real_images):\n        unlabelled_x, unlabelled_y = real_images\n        \n        # Predictor step\n        unlabelled_prediction_y, unlabelled_predictor_latent = self.predictor(unlabelled_x, training=False)\n        \n        # Generator step\n        unlabelled_vae_y, unlabelled_vae_latent = self.generator(unlabelled_x, training=False)\n        \n        # Discriminator step\n#         unlabelled_predictor_latent = math_ops.mean(ops.convert_to_tensor(unlabelled_predictor_latent), axis=0)\n        unlabelled_disc_in = tf.concat([unlabelled_vae_latent,unlabelled_predictor_latent],axis=1)\n        \n        unlabelled_disc_y = self.discriminator(unlabelled_disc_in,training=False)\n        \n        return unlabelled_prediction_y #, unlabelled_disc_y#, unlabelled_y\n\n    @property\n    def metrics(self):\n        # We list our `Metric` objects here so that `reset_states()` can be\n        # called automatically at the start of each epoch\n        # or at the start of `evaluate()`.\n        # If you don't implement this property, you have to call\n        # `reset_states()` yourself at the time of your choosing.\n        #return [self.trackers[\"loss_tracker_predictor\"], self.trackers[\"acc_metric_predictor\"], self.trackers['f1_predictor'], self.trackers[\"loss_tracker_generator\"], self.trackers[\"loss_tracker_disc\"], self.trackers[\"acc_tracker_disc\"], self.trackers[\"f1_predictor\"], self.trackers[\"IOU_predictor\"]] + self.trackers[\"individual_f1_predictor\"] + self.trackers[\"individual_acc_predictor\"]\n        return [self.trackers[\"loss_tracker_predictor\"], self.trackers[\"acc_metric_predictor\"], self.trackers[\"loss_tracker_generator\"], self.trackers[\"loss_tracker_disc\"], self.trackers[\"acc_tracker_disc\"]] + self.trackers[\"individual_acc_predictor\"] + self.trackers['individual_IOU_predictor']","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:43.956555Z","iopub.execute_input":"2023-06-21T07:16:43.957294Z","iopub.status.idle":"2023-06-21T07:16:44.020546Z","shell.execute_reply.started":"2023-06-21T07:16:43.957260Z","shell.execute_reply":"2023-06-21T07:16:44.019373Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def weighted_cross_entropy(y_true, y_pred, sample_weights):\n    y_true = tf.cast(y_true,y_pred.dtype)\n    q1 = -1*(tf.math.multiply(y_true,tf.math.log(y_pred)) + tf.math.multiply((1.0-y_true),tf.math.log(1.0-y_pred)))\n    sample_weights = tf.cast(sample_weights,q1.dtype)\n    z = tf.math.multiply(q1,sample_weights)\n    \n    return tf.math.reduce_mean(z,axis=1)\n\ndef dice_coef(y_true, y_pred):\n    smooth = 1.0\n    y_true_f = tf.cast(K.flatten(y_true), tf.float64)\n    y_pred_f = tf.cast(K.flatten(y_pred), tf.float64)\n    intersection = K.sum(y_true_f * y_pred_f)\n    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\ndef dice_coef_loss(y_true, y_pred):\n    t = []\n    for i in range(7):\n        t.append(1 - dice_coef(y_true[i], y_pred[i]))\n        \n    return tf.convert_to_tensor(t)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:44.022080Z","iopub.execute_input":"2023-06-21T07:16:44.022808Z","iopub.status.idle":"2023-06-21T07:16:44.038310Z","shell.execute_reply.started":"2023-06-21T07:16:44.022765Z","shell.execute_reply":"2023-06-21T07:16:44.037210Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def uncertainity(probs, weights):\n    lis = []\n    lis_output = []\n    for i in range(hyperparameters['num_tasks']):\n        attr_output = probs[i]\n        w = weights[:,i]\n        k = -1* np.sum(attr_output*np.log(attr_output),axis=1)\n        lis_output.append(k)\n        lis.append(w*k)\n    \n    variance = np.var(np.array(lis),axis=0)\n    return np.array(lis).sum(axis=0), variance\n\ndef getIndices(output, hyperparameters ,pretrain=False):\n    if pretrain == True:\n        count =  hyperparameters['train_initial_batches']*hyperparameters['batch_size']\n        if ((output<=0.5).sum())>=count:\n            sort = np.argwhere(output<=0.5)[:,0]\n            return sort\n        else:\n            selection = (int((hyperparameters['train_initial_batches']*hyperparameters['batch_size'])/1000)+1)*1000\n            sort = np.argpartition((output)[:,0], selection)\n            return sort[:selection]\n    else:\n        count = hyperparameters['num_uncertain_elements']\n        if ((output<=0.5).sum())>=count:\n            sort = np.argwhere(output<=0.5)[:,0]\n            return sort\n        else:\n            selection = (int(hyperparameters['num_uncertain_elements']/1000)+1)*1000\n            sort = np.argpartition((output)[:,0], selection)\n            return sort[:selection]\n        \ndef divide_data(train, initial = False):\n    num_samples = train.values.shape[0]\n    \n    if initial:\n        idx = random.sample(list(np.arange(num_samples)), ((int(hyperparameters['initial_percent_val']*num_samples)//hyperparameters['batch_size'])*hyperparameters['batch_size']))\n    else:\n        idx = random.sample(list(np.arange(num_samples)), ((int(hyperparameters['initial_percent']*num_samples)//hyperparameters['batch_size'])*hyperparameters['batch_size']))\n\n    return pd.DataFrame(train.values[idx,:], columns=train.columns), idx","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:44.042833Z","iopub.execute_input":"2023-06-21T07:16:44.043458Z","iopub.status.idle":"2023-06-21T07:16:44.057005Z","shell.execute_reply.started":"2023-06-21T07:16:44.043425Z","shell.execute_reply":"2023-06-21T07:16:44.056212Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# def dice_coef_loss(y_true, y_pred):\n#     intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n#     return 1-((2. * intersection + 1) / (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + 1))\n\ndef soft_dice_loss(y_true, y_pred, epsilon=1e-6): \n    ''' \n    Soft dice loss calculation for arbitrary batch size, number of classes, and number of spatial dimensions.\n    Assumes the `channels_last` format.\n  \n    # Arguments\n        y_true: b x X x Y( x Z...) x c One hot encoding of ground truth\n        y_pred: b x X x Y( x Z...) x c Network output, must sum to 1 over c channel (such as after softmax) \n        epsilon: Used for numerical stability to avoid divide by zero errors\n    \n    # References\n        V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation \n        https://arxiv.org/abs/1606.04797\n        More details on Dice loss formulation \n        https://mediatum.ub.tum.de/doc/1395260/1395260.pdf (page 72)\n        \n        Adapted from https://github.com/Lasagne/Recipes/issues/99#issuecomment-347775022\n    '''\n    \n    # print(y_true.shape, y_pred.shape)\n    # skip the batch and class axis for calculating Dice score\n    axes = tuple(range(1, len(y_pred.shape)-1)) \n    # numerator = 2. * np.sum(y_pred * y_true, axes)\n    numerator = 2. * tf.math.reduce_sum(y_pred * y_true, axes)\n    # denominator = np.sum(np.square(y_pred) + np.square(y_true), axes)\n    denominator = tf.math.reduce_sum(tf.add(tf.square(y_pred),tf.square(y_true)), axes)\n\n    # return 1 - np.mean((numerator + epsilon) / (denominator + epsilon)) # average over classes and batch\n    return 1 - tf.math.reduce_mean(tf.add(numerator,epsilon) / tf.add(denominator,epsilon)) # average over classes and batch\n\n    # thanks @mfernezir for catching a bug in an earlier version of this implementation!","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:44.126122Z","iopub.execute_input":"2023-06-21T07:16:44.126553Z","iopub.status.idle":"2023-06-21T07:16:44.135691Z","shell.execute_reply.started":"2023-06-21T07:16:44.126520Z","shell.execute_reply":"2023-06-21T07:16:44.134702Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"class CalculatingPredictions(tf.keras.callbacks.Callback):\n    def __init__(self, filename, preds, test_gen, lr, is_validation=False):\n        \n        self.filename = filename\n        self.preds = preds\n        self.test_gen = test_gen\n        self.lr = lr\n        self.is_validation=is_validation\n\n    def on_epoch_end(self, epoch, logs=None):\n        \n#         predict=self.model.evaluate(self.test_gen)\n#         print(predict)\n#         self.preds.append(predict)\n#         k = np.array(self.preds)\n#         if (self.is_validation==True):\n#             np.save(\"./saved_history/\" + self.filename + \"_validation\"+ str(epoch)+ \".npy\", k)\n#         else:\n#             np.save(\"./saved_history/\"+ self.filename + '_' +str(epoch)+ \".npy\", k)\n        \n        date = datetime.datetime.now().strftime(\"%d - %b - %y - %H:%M:%S\")\n        \n        if (self.is_validation==False and epoch%2==0):\n            self.model.predictor.save_weights(\"./saved_history/models/pred_model_\" + self.filename + \"_epoch\"+str(epoch)+'.h5')\n            self.model.discriminator.save_weights(\"./saved_history/models/disc_model_\" + self.filename +\"_epoch\"+str(epoch)+'.h5')\n            self.model.generator.save_weights(\"./saved_history/models/vae_model_\" + self.filename + \"_epoch\"+str(epoch)+'.h5')\n        elif (self.is_validation==True):\n            self.model.predictor.save_weights(\"./saved_history/models/val_pred_model_\" + self.filename + \"_epoch\"+str(epoch)+'.h5')\n            self.model.discriminator.save_weights(\"./saved_history/models/val_disc_model_\" + self.filename +\"_epoch\"+str(epoch)+'.h5')\n            self.model.generator.save_weights(\"./saved_history/models/val_vae_model_\" + self.filename + \"_epoch\"+str(epoch)+'.h5')","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:44.609891Z","iopub.execute_input":"2023-06-21T07:16:44.611053Z","iopub.status.idle":"2023-06-21T07:16:44.622400Z","shell.execute_reply.started":"2023-06-21T07:16:44.611013Z","shell.execute_reply":"2023-06-21T07:16:44.621209Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/predictions\n!mkdir /kaggle/working/saved_history/\n!mkdir /kaggle/working/saved_history/models\n!mkdir logs","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:45.089296Z","iopub.execute_input":"2023-06-21T07:16:45.089715Z","iopub.status.idle":"2023-06-21T07:16:49.513744Z","shell.execute_reply.started":"2023-06-21T07:16:45.089681Z","shell.execute_reply":"2023-06-21T07:16:49.512040Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"break_point_ep = {'3': 1e-5,'6': 1e-5,'10': 1e-5}\nsplits = [0.1,0.15,0.2,0.25,0.3,0.35,0.4]\n\n# defining metrics\ntrackers = {\n#     'loss_tracker_predictor': tf.keras.metrics.Mean(name='loss_predictor_total'),\n    \"loss_tracker_predictor\": tf.keras.metrics.Mean(name=\"loss_predictor_total\"),# tf.keras.metrics.SparseCategoricalCrossentropy(name=\"loss_predictor_total\"),\n    \"acc_metric_predictor\": tf.keras.metrics.BinaryAccuracy (name=\"acc_predictor\"),\n    \n    #'f1_predictor': tf.keras.metrics.Mean(name='f1_predictor'),\n    #'IOU_predictor': tf.keras.metrics.MeanIoU(name='IOU_predictor', num_classes=hyperparameters['num_tasks']),\n\n    \"individual_acc_predictor\": [tf.keras.metrics.Mean(name=\"acc_predictor_\"+str(i)) for i in range(hyperparameters['num_tasks'])],\n    \"individual_IOU_predictor\": [tf.keras.metrics.MeanIoU(name='IOU_predictor'+str(i), num_classes=2) for i in range(hyperparameters['num_tasks'])],\n    \n    #\"individual_f1_predictor\": [tf.keras.metrics.Mean(name=\"f1_predictor_\"+str(i)) for i in range(hyperparameters['num_tasks'])],\n    #\"individual_f1_predictor\": [tf.keras.metrics.Mean(name=\"f1_predictor_\"+str(i)) for i in range(hyperparameters['num_tasks'])],\n    \"loss_tracker_generator\": tf.keras.metrics.MeanSquaredError(name='loss_VAE'),\n    \"loss_tracker_disc\":  tf.keras.metrics.BinaryCrossentropy(name='loss_disc'),\n    \"acc_tracker_disc\": tf.keras.metrics.BinaryAccuracy(\"acc_disc\")\n}","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:49.516563Z","iopub.execute_input":"2023-06-21T07:16:49.516962Z","iopub.status.idle":"2023-06-21T07:16:49.719593Z","shell.execute_reply.started":"2023-06-21T07:16:49.516923Z","shell.execute_reply":"2023-06-21T07:16:49.718435Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"## startTrain\ndef startTraining(filename, trackers, splits, break_point_ep,  validation_first = False, load_pred_model = False, load_vae = False, load_disc = False, further_training=False):\n    tf.config.run_functions_eagerly(True)\n    \n    preds=[]\n    validation_train_history=[] # new\n    logdir = \"./logs/\" + filename\n    filepath = \"./saved_history/model/\"+filename\n    \n    checkpoint = ModelCheckpoint(filepath, verbose=1, save_best_only=False, mode='max', period = 1)\n    csv_logger = CSVLogger('./saved_history/training_results_'+filename+'.csv', separator = ',', append=True)\n    pre_train_logger = CSVLogger('./saved_history/pre_training_results_'+filename+'.csv', separator = ',', append=True)\n    tensorboard_callback = TensorBoard(log_dir = logdir)\n    pre_tensorboard_callback = TensorBoard(log_dir =\" ./logs/pre_\"+filename)\n    \n    # predictor\n    pred_model = vgg_segnet(n_classes = hyperparameters['num_tasks'], input_height=hyperparameters['height'], input_width=hyperparameters['width'], encoder_level=2, channels=hyperparameters['channels'])\n    pred_model.compile(optimizer = keras.optimizers.SGD(learning_rate=hyperparameters['lr'],\n                                                    clipnorm=1.0 ))\n    \n    if load_pred_model:\n        print(\"Predictor weights loading ...\")\n        # pred_model.load_weights(\"./saved_history/models/pred_model_from_scratch_vanilla_4.h5\", by_name=True)\n        \n    # defining my discriminator\n    disc_in, disc_out = discriminator()\n    disc = Model(inputs = disc_in, outputs = disc_out)\n    disc.compile(optimizer = keras.optimizers.SGD(learning_rate=hyperparameters['lr'],\n                                                  clipnorm=1.0 ))\n\n    if load_disc:\n        print(\"Discriminator weights loading ...\")\n        # disc.load_weights()\n\n    # defining my generator\n    X, z, mu = variationalAutoEncoder()\n    vae = Model(inputs = X, outputs = [z,mu])\n    vae.compile(optimizer = keras.optimizers.RMSprop(learning_rate=hyperparameters['lr'],\n                                                     clipnorm=1.0 ))\n    if load_vae:\n        print(\"VAE weights loading ...\")\n        # vae.load_weights()\n        \n    # Instantiate AL model\n    AL_model = ActiveLearning(discriminator=disc, generator=vae, predictor=pred_model, hyperparameters=hyperparameters, trackers = trackers)\n    AL_model.compile(\n        d_optimizer=keras.optimizers.SGD(learning_rate=hyperparameters['lr'],clipnorm=1.0 ),\n        g_optimizer= keras.optimizers.RMSprop(learning_rate=hyperparameters['lr'],clipnorm=1.0),\n        p_optimizer=keras.optimizers.SGD(learning_rate=hyperparameters['lr'],clipnorm=1.0 ))\n    \n    print('model loaded')\n    \n    (train, val) = preprocess(hyperparameters)\n    \n    # Generators\n    train_gen_full = DataGenerator(partition['train'], \n                                       labels, \n                                       dim = (hyperparameters['height'],hyperparameters['width']),\n                                       batch_size= hyperparameters['batch_size'],\n                                       n_channels = hyperparameters['channels'],\n                                       shuffle = True)\n\n    val_gen = DataGenerator(partition['val'], \n                                       labels, \n                                       dim = (hyperparameters['height'],hyperparameters['width']),\n                                       batch_size= hyperparameters['batch_size'],\n                                       n_channels =hyperparameters['channels'],\n                                       shuffle = False, is_validation=True)\n    \n    if validation_first == True:\n        labelled_pretrain, idx_prelabelled = divide_data(val, initial=True)\n        idx_preunlabelled = list(np.setdiff1d(list(range(val.shape[0])), idx_prelabelled))\n        unlabelled_pretrain = pd.DataFrame(val.values[idx_preunlabelled,:], columns=val.columns)\n        \n        pretrain_gen = MultipleGenerator(labelled_pretrain.values, \n                                              unlabelled_pretrain.values, \n                                              labels,\n                                              batch_size= hyperparameters['batch_size'],\n                                              dim = (hyperparameters['height'],hyperparameters['width']),\n                                              n_channels = hyperparameters['channels'],\n                                              shuffle=True)\n        \n        labelled_pretrain_gen = DataGenerator(unlabelled_pretrain.values, \n                                       labels, \n                                       dim = (hyperparameters['height'],hyperparameters['width']),\n                                       batch_size= hyperparameters['batch_size'],\n                                       n_channels = hyperparameters['channels'],\n                                       shuffle = False)\n        val_history = AL_model.fit(pretrain_gen, epochs = hyperparameters['pretraining_epochs'], steps_per_epoch = 55, validation_data=val_gen, callbacks = [CalculatingPredictions(filename, preds, val_gen, 0.01, True) , pre_tensorboard_callback, pre_train_logger], verbose=1)\n        validation_train_history.append(val_history.history)\n        with open(\"./saved_history/pretraining_history_list_\"+filename+'.json', 'w') as f:\n            json.dump(validation_train_history, f, indent=2)\n            \n        labelled_train, idx_labelled = divide_data(train)\n        idx_unlabelled = list(np.setdiff1d(list(range(train.shape[0])), idx_labelled))\n        unlabelled_train = pd.DataFrame(train.values[idx_unlabelled,:], columns=train.columns)     \n        \n        train_gen = MultipleGenerator(labelled_train.values, \n                                              unlabelled_train.values, \n                                              labels,\n                                              batch_size= hyperparameters['batch_size'],\n                                              dim = (hyperparameters['height'],hyperparameters['width']),\n                                              n_channels = hyperparameters['channels'],\n                                              shuffle=True) \n\n        unlabelled_gen = DataGenerator(unlabelled_train.values, \n                                           labels, \n                                           dim = (hyperparameters['height'],hyperparameters['width']),\n                                           batch_size= hyperparameters['batch_size'],\n                                           n_channels = hyperparameters['channels'],\n                                           shuffle = False)\n        \n        labelled_gen = DataGenerator(labelled_train.values, \n                                           labels, \n                                           dim = (hyperparameters['height'],hyperparameters['width']),\n                                           batch_size= hyperparameters['batch_size'],\n                                           n_channels = hyperparameters['channels'],\n                                           shuffle = False)\n    elif further_training==True:\n    \n        idx_labelled = np.load(\"./saved_history/idx_labelled_4.npy\")\n        labelled_train = train.iloc[idx_labelled, :]\n        idx_unlabelled = list(np.setdiff1d(list(range(train.shape[0])), idx_labelled))\n        unlabelled_train = pd.DataFrame(train.values[idx_unlabelled,:], columns=train.columns)\n\n        train_gen = MultipleGenerator(labelled_train.values, \n                                              unlabelled_train.values, \n                                              labels,\n                                              batch_size= hyperparameters['batch_size'],\n                                              dim = (hyperparameters['height'],hyperparameters['width']),\n                                              n_channels = hyperparameters['channels'],\n                                              shuffle=True) \n\n        unlabelled_gen = DataGenerator(unlabelled_train.values, \n                                           labels, \n                                           dim = (hyperparameters['height'],hyperparameters['width']),\n                                           batch_size= hyperparameters['batch_size'],\n                                           n_channels = hyperparameters['channels'],\n                                           shuffle = False)\n        \n        labelled_gen = DataGenerator(labelled_train.values, \n                                           labels, \n                                           dim = (hyperparameters['height'],hyperparameters['width']),\n                                           batch_size= hyperparameters['batch_size'],\n                                           n_channels = hyperparameters['channels'],\n                                           shuffle = False) \n    else:\n        \n        labelled_train, idx_labelled = divide_data(train)\n        idx_unlabelled = list(np.setdiff1d(list(range(train.shape[0])), idx_labelled))\n        unlabelled_train = pd.DataFrame(train.values[idx_unlabelled,:], columns=train.columns)        \n\n        train_gen = MultipleGenerator(labelled_train.values, \n                                              unlabelled_train.values, \n                                              labels,\n                                              batch_size= hyperparameters['batch_size'],\n                                              dim = (hyperparameters['height'],hyperparameters['width']),\n                                              n_channels = hyperparameters['channels'],\n                                              shuffle=True) \n\n        unlabelled_gen = DataGenerator(unlabelled_train.values, \n                                           labels, \n                                           dim = (hyperparameters['height'],hyperparameters['width']),\n                                           batch_size= hyperparameters['batch_size'],\n                                           n_channels = hyperparameters['channels'],\n                                           shuffle = False)\n        \n        labelled_gen = DataGenerator(labelled_train.values, \n                                           labels, \n                                           dim = (hyperparameters['height'],hyperparameters['width']),\n                                           batch_size= hyperparameters['batch_size'],\n                                           n_channels = hyperparameters['channels'],\n                                           shuffle = False) \n    \n    history_list=[]\n    if further_training==True:\n        num_batches = 10 #idx_labelled.shape[0]//hyperparameters['batch_size']\n        tensorboard_callback = TensorBoard(log_dir = 'AL_model_from_scratch_non_cs_09 - Jan - 21 - 12:50:36')\n\n        iteration =5\n        epoch_num = 13\n        \n        history = AL_model.fit(train_gen,initial_epoch = epoch_num, epochs=epoch_num+7, steps_per_epoch = num_batches, validation_data=val_gen,callbacks = [CalculatingPredictions(filename, preds, val_gen, 0.01), csv_logger], verbose = 1)\n        history_list.append(history.history)\n        pred_model.save_weights(\"./saved_history/models/pred_model_from_scratch_vanilla_\"+str(iteration)+'.h5')\n        disc.save_weights(\"./saved_history/models/disc_model_from_scratch_vanilla_\"+str(iteration)+'.h5')\n        vae.save_weights(\"./saved_history/models/vae_model_from_scratch_vanilla_\"+str(iteration)+'.h5')\n        with open(\"./saved_history/history_list_only_\"+str(iteration)+\".json\", 'w') as f:\n            json.dump(history_list, f, indent=2)\n\n        with open(\"./saved_history/preds_only_\"+str(iteration)+\".json\", 'w') as f:\n            json.dump(preds, f, indent=2)\n    else:\n        test_predictions=[]\n        indices_list = []\n        epoch_num = 0        \n        num_batches = (train.shape[0]*splits[hyperparameters['split_index']])//hyperparameters['batch_size']\n                \n        for iteration in range(len(splits)): \n            print(iteration, splits[iteration], len(train_gen))\n            \n            if iteration==0:\n                # Initial training ---- change\n                history = AL_model.fit(train_gen, epochs=hyperparameters['initial_train_epoch'], steps_per_epoch = num_batches, validation_data=val_gen, callbacks = [CalculatingPredictions(filename, preds, val_gen, 0.01), csv_logger, tensorboard_callback], verbose = 1)\n                history_list.append(history.history)\n                epoch_num+=hyperparameters['initial_train_epoch']\n            else:\n                # predictor\n                pred_model = vgg_segnet(n_classes = hyperparameters['num_tasks'], input_height=hyperparameters['height'], input_width=hyperparameters['width'], encoder_level=2, channels=hyperparameters['channels'])\n                pred_model.compile(optimizer = keras.optimizers.SGD(learning_rate=hyperparameters['lr'],\n                                                                clipnorm=1.0 ))\n\n                # pred_model.load_weights(\"./saved_history/models/pred_model_from_scratch_vanilla_4.h5\", by_name=True)\n\n                # defining my discriminator\n                disc_in, disc_out = discriminator()\n                disc = Model(inputs = disc_in, outputs = disc_out)\n                disc.compile(optimizer = keras.optimizers.SGD(learning_rate=hyperparameters['lr'],\n                                                              clipnorm=1.0 ))\n\n                # defining my generator\n                X, z, mu = variationalAutoEncoder()\n                vae = Model(inputs = X, outputs = [z,mu])\n                vae.compile(optimizer = keras.optimizers.RMSprop(learning_rate=hyperparameters['lr'],\n                                                                 clipnorm=1.0 ))\n                \n                if validation_first==True:\n                    pred_model.load_weights(\"./saved_history/models/val_pred_model_\" + filename + \"_epoch2\"+'.h5', by_name=True)\n                    vae.load_weights(\"./saved_history/models/val_vae_model_\" + filename + \"_epoch2\"+'.h5', by_name=True)\n                    disc.load_weights(\"./saved_history/models/val_disc_model_\" + filename + \"_epoch2\"+'.h5', by_name=True)\n                \n                # Instantiate AL model\n                AL_model = ActiveLearning(discriminator=disc, generator=vae, predictor=pred_model, hyperparameters=hyperparameters, trackers = trackers)\n                AL_model.compile(\n                    d_optimizer=keras.optimizers.SGD(learning_rate=hyperparameters['lr'],clipnorm=1.0 ),\n                    g_optimizer= keras.optimizers.RMSprop(learning_rate=hyperparameters['lr'],clipnorm=1.0),\n                    p_optimizer=keras.optimizers.SGD(learning_rate=hyperparameters['lr'],clipnorm=1.0 ))\n\n                print('model loaded')\n\n                # Increment training --- change\n                history = AL_model.fit(train_gen, initial_epoch = epoch_num, epochs=epoch_num+hyperparameters['increment_train_epoch'] + iteration, steps_per_epoch = num_batches, validation_data=val_gen,callbacks = [CalculatingPredictions(filename, preds, val_gen, 0.01), csv_logger, tensorboard_callback], verbose = 1)\n                history_list.append(history.history)\n                epoch_num+=hyperparameters['increment_train_epoch']+iteration\n        \n            pred_model.save_weights(\"./saved_history/models/pred_model_\" + filename +\"_iteration_\"+str(iteration)+'.h5')\n            disc.save_weights(\"./saved_history/models/disc_model_\" + filename + \"_iteration_\"+str(iteration)+'.h5')\n            vae.save_weights(\"./saved_history/models/vae_model_\"+ filename + \"_iteration_\"+str(iteration)+'.h5')        \n\n            indices_list.append(idx_labelled)\n            inc = int(train.shape[0]*0.05 /  hyperparameters['batch_size'])\n            num_batches+= inc\n            print('Number of batches added:' , inc)       \n\n            with open(\"./saved_history/history_list_\"+filename+str(iteration)+\".json\", 'w') as f:\n                json.dump(history_list, f, indent=2)\n\n            with open(\"./saved_history/preds_\"+filename+str(iteration)+\".json\", 'w') as f:\n                json.dump(preds, f, indent=2)\n\n            np.save(\"./saved_history/idx_labelled_\"+filename+str(iteration)+\".npy\",np.array(idx_labelled))        \n\n            if (iteration!=(len(splits)-1)): # last iteration\n                k = random.sample(idx_unlabelled, hyperparameters['num_uncertain_elements'])\n                idx_labelled = idx_labelled+list(k)\n                labelled_train = pd.DataFrame(train.values[idx_labelled,:], columns=train.columns)\n                idx_unlabelled = list(np.setdiff1d(idx_unlabelled, k))\n                unlabelled_train = pd.DataFrame(train.values[idx_unlabelled,:], columns=train.columns)            \n\n\n                train_gen = MultipleGenerator(labelled_train.values, \n                                              unlabelled_train.values, \n                                              labels,\n                                              batch_size= hyperparameters['batch_size'],\n                                              dim = (hyperparameters['height'],hyperparameters['width']),\n                                              n_channels = hyperparameters['channels'],\n                                              shuffle=True)\n\n                unlabelled_gen = DataGenerator(unlabelled_train.values, \n                                       labels, \n                                       dim = (hyperparameters['height'],hyperparameters['width']),\n                                       batch_size= hyperparameters['batch_size'],\n                                       n_channels = hyperparameters['channels'],\n                                       shuffle = False)\n                labelled_gen = DataGenerator(labelled_train.values, \n                               labels, \n                               dim = (hyperparameters['height'],hyperparameters['width']),\n                               batch_size= hyperparameters['batch_size'],\n                               n_channels = hyperparameters['channels'],\n                               shuffle = False)\n                \n    return history_list, pred_model, vae, disc, AL_model, indices_list, preds","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:49.721682Z","iopub.execute_input":"2023-06-21T07:16:49.722157Z","iopub.status.idle":"2023-06-21T07:16:49.792199Z","shell.execute_reply.started":"2023-06-21T07:16:49.722098Z","shell.execute_reply":"2023-06-21T07:16:49.791029Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"attempt = '1'\ncls = 'all'\nfilename = \"Random_Sampling_CityScapes_\" +cls + '_' + attempt\n\nhistory_list, pred_model, vae, disc, Al_model, indices_list, preds = startTraining(filename, trackers, splits, break_point_ep, False, False, False, False, False)","metadata":{"execution":{"iopub.status.busy":"2023-06-21T07:16:52.020451Z","iopub.execute_input":"2023-06-21T07:16:52.020863Z","iopub.status.idle":"2023-06-21T07:19:23.008326Z","shell.execute_reply.started":"2023-06-21T07:16:52.020830Z","shell.execute_reply":"2023-06-21T07:19:23.006194Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"model loaded\n0 0.1 37\nEpoch 1/7\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/structured_function.py:254: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":" 1/37 [..............................] - ETA: 58:18 - loss_predictor_total: 1.4751 - acc_predictor: 0.4993 - loss_VAE: 0.1245 - loss_disc: 0.8275 - acc_disc: 0.3750 - acc_predictor_0: 0.5045 - IOU_predictor_0: 0.3373 - acc_predictor_1: 0.4980 - IOU_predictor_1: 0.3177 - acc_predictor_2: 0.4986 - IOU_predictor_2: 0.2539 - acc_predictor_3: 0.4989 - IOU_predictor_3: 0.2589 - acc_predictor_4: 0.4970 - IOU_predictor_4: 0.2799 - acc_predictor_5: 0.4988 - IOU_predictor_5: 0.2519 - acc_predictor_6: 0.4994 - IOU_predictor_6: 0.2737","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_32/3230237775.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'1'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Random_Sampling_CityScapes_\"\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mcls\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattempt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mhistory_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAl_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstartTraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrackers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbreak_point_ep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_32/1236392315.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(filename, trackers, splits, break_point_ep, validation_first, load_pred_model, load_vae, load_disc, further_training)\u001b[0m\n\u001b[1;32m    200\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0;31m# Initial training ---- change\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAL_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'initial_train_epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCalculatingPredictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m                 \u001b[0mhistory_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0mepoch_num\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'initial_train_epoch'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1681\u001b[0m                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m                             \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1688\u001b[0m                             \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m   1282\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m                 \u001b[0;34m\"\"\"Runs a training execution with a single step.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1284\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mstep_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1264\u001b[0m                 run_step = tf.function(\n\u001b[1;32m   1265\u001b[0m                     \u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjit_compile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_retracing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m                 )\n\u001b[1;32m   1267\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             outputs = reduce_per_replica(\n\u001b[1;32m   1270\u001b[0m                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1312\u001b[0m       \u001b[0;31m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m       \u001b[0;31m# applied when the caller is also in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1315\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1316\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2891\u001b[0m     \u001b[0m_require_cross_replica_or_default_context_extended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2892\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2893\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2894\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2895\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3694\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3695\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3696\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mControlStatusCtx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUNSPECIFIED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m                 \u001b[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1251\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_32/3755531282.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, real_images)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;31m# predictor_loss = l(re_1, re_2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;31m#w = tf.math.reduce_mean(predictor_loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mtrainable_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;31m# Update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1059\u001b[0m               output_gradients))\n\u001b[1;32m   1060\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[1;32m   1061\u001b[0m                           for x in output_gradients]\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1064\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     raise ValueError(\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gradient_tape/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    588\u001b[0m           \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m           data_format=data_format),\n\u001b[0;32m--> 592\u001b[0;31m       gen_nn_ops.conv2d_backprop_filter(\n\u001b[0m\u001b[1;32m    593\u001b[0m           \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m           \u001b[0mshape_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m           \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1259\u001b[0m         data_format, \"dilations\", dilations)\n\u001b[1;32m   1260\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1261\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1264\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m       return conv2d_backprop_filter_eager_fallback(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"# import os\n# os.chdir(r'/kaggle/working/')\n# from IPython.display import FileLink\n# FileLink(r'./saved_history/training_results_Random_Sampling_CityScapes_all_1.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # checking imbalance\n\n# train_gen_full = DataGenerator(partition['train'], \n#                                    labels, \n#                                    dim = (hyperparameters['height'],hyperparameters['width']),\n#                                    batch_size= hyperparameters['batch_size'],\n#                                    n_channels = hyperparameters['channels'],\n#                                    shuffle = True)\n\n# val_gen = DataGenerator(partition['val'], \n#                                    labels, \n#                                    dim = (hyperparameters['height'],hyperparameters['width']),\n#                                    batch_size= hyperparameters['batch_size'],\n#                                    n_channels =hyperparameters['channels'],\n#                                    shuffle = False)\n\n# l1 = []\n# t=0\n# for i in train_gen_full:\n#     l1.append(i[1])\n#     t+=1\n    \n#     if t%100==0:\n#         print(t)\n        \n# l2=[]\n# t=0\n# for i in val_gen:\n#     l2.append(i[1])\n#     t+=1\n    \n#     if t%100==0:\n#         print(t)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# p1 = [i[3] for i in l1]\n# p2 =  [i[3] for i in l2]\n# p1=np.concatenate(p1)\n# p2=np.concatenate(p2)\n# dic1={}\n# for i in range(p1.shape[0]):\n#     if i%500==0:\n#         print(i)\n#     for j in range(p1.shape[1]):\n#         for k in range(p1.shape[2]):\n#             if p1[i,j,k] in dic1:\n#                 dic1[p1[i,j,k]]+=1\n#             else:\n#                 dic1[p1[i,j,k]]=1\n\n# dic2={}\n# for i in range(p2.shape[0]):\n#     if i%500==0:\n#         print(i)\n#     for j in range(p2.shape[1]):\n#         for k in range(p2.shape[2]):\n#             if p2[i,j,k] in dic2:\n#                 dic2[p2[i,j,k]]+=1\n#             else:\n#                 dic2[p2[i,j,k]]=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# {1: 48034810, 0: 49220614} {1: 8467216, 0: 7785712} - \n# {0: 76243374, 1: 21012050} {0: 13708944, 1: 2543984}\n# {0: 95544125, 1: 1711299} {0: 16086768, 1: 166160}\n# {0: 93785234, 1: 3470190} {0: 16090240, 1: 162688}\n# {0: 82503746, 1: 14751678}, {0: 13721840, 1: 2531088} - \n# {0: 96086664, 1: 1168760} {0: 16184976, 1: 67952}   - \n# {0: 90148787, 1: 7106637} {0: 13939088, 1: 2313840} - ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# r = [{1: 48034810, 0: 49220614},\n#  {0: 76243374, 1: 21012050},\n#  {0: 95544125, 1: 1711299},\n#  {0: 93785234, 1: 3470190},\n#  {0: 82503746, 1: 14751678},\n#  {0: 96086664, 1: 1168760},\n#  {0: 90148787, 1: 7106637}]\n\n# wts = []\n# for i in r:\n#     wts.append([((i[0]+i[1])/(2*i[0])), ((i[0]+i[1])/(2*i[1]))])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tot1=0\n# for i in dic1:\n#     tot1+=dic1[i]\n    \n# for i in dic1:\n#     print(i, (dic1[0]+dic1[1])/(2*dic1[i]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tot1=0\n# for i in dic1:\n#     tot1+=dic1[i]\n\n# for i in dic1:\n#     print(i, (dic1[i])/tot1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# tot2=0\n# for i in dic2:\n#     tot2+=dic2[i]\n\n# for i in dic2:\n#     print(i, (dic2[i])/tot2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Individual testing","metadata":{}},{"cell_type":"code","source":"# tf.config.run_functions_eagerly(True)\n# attempt = '1'\n# cls = 'flat'\n# filename = \"Random_Sampling_CityScapes_\" +cls + '_' + attempt\n\n# preds=[]\n# validation_train_history=[] # new\n# logdir = \"./logs/\" + filename\n# filepath = \"./saved_history/model/\"+filename\n\n# checkpoint = ModelCheckpoint(filepath, verbose=1, save_best_only=False, mode='max', period = 1)\n# csv_logger = CSVLogger('./saved_history/training_results_'+filename+'.csv', separator = ',', append=True)\n# pre_train_logger = CSVLogger('./saved_history/pre_training_results_'+filename+'.csv', separator = ',', append=True)\n# tensorboard_callback = TensorBoard(log_dir = logdir)\n# pre_tensorboard_callback = TensorBoard(log_dir =\" ./logs/pre_\"+filename)\n\n# # predictor\n# pred_model = vgg_segnet(n_classes = hyperparameters['num_tasks'], input_height=hyperparameters['height'], input_width=hyperparameters['width'], encoder_level=2, channels=hyperparameters['channels'])\n# pred_model.compile(optimizer = keras.optimizers.SGD(learning_rate=hyperparameters['lr'],\n#                                                 clipnorm=1.0 ))\n\n# # defining my discriminator\n# disc_in, disc_out = discriminator()\n# disc = Model(inputs = disc_in, outputs = disc_out)\n# disc.compile(optimizer = keras.optimizers.SGD(learning_rate=hyperparameters['lr'],\n#                                               clipnorm=1.0 ))\n\n# # defining my generator\n# X, z, mu = variationalAutoEncoder()\n# vae = Model(inputs = X, outputs = [z,mu])\n# vae.compile(optimizer = keras.optimizers.RMSprop(learning_rate=hyperparameters['lr'],\n#                                                  clipnorm=1.0 ))\n\n# # Instantiate AL model\n# AL_model = ActiveLearning(discriminator=disc, generator=vae, predictor=pred_model, hyperparameters=hyperparameters, trackers = trackers)\n# AL_model.compile(\n#     d_optimizer=keras.optimizers.SGD(learning_rate=hyperparameters['lr'],clipnorm=1.0 ),\n#     g_optimizer= keras.optimizers.RMSprop(learning_rate=hyperparameters['lr'],clipnorm=1.0),\n#     p_optimizer=keras.optimizers.SGD(learning_rate=hyperparameters['lr'],clipnorm=1.0 ))\n\n# print('model loaded')\n\n# (train, val) = preprocess(hyperparameters)\n\n# # Generators\n# train_gen_full = DataGenerator(partition['train'], \n#                                    labels, \n#                                    dim = (hyperparameters['height'],hyperparameters['width']),\n#                                    batch_size= hyperparameters['batch_size'],\n#                                    n_channels = hyperparameters['channels'],\n#                                    shuffle = True)\n\n# val_gen = DataGenerator(partition['val'], \n#                                    labels, \n#                                    dim = (hyperparameters['height'],hyperparameters['width']),\n#                                    batch_size= hyperparameters['batch_size'],\n#                                    n_channels =hyperparameters['channels'],\n#                                    shuffle = False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checkpoint = ModelCheckpoint(filepath, verbose=1, save_best_only=False, mode='max', period = 1)\n# csv_logger = CSVLogger('./saved_history/training_results_'+filename+'.csv', separator = ',', append=True)\n# pre_train_logger = CSVLogger('./saved_history/pre_training_results_'+filename+'.csv', separator = ',', append=True)\n# tensorboard_callback = TensorBoard(log_dir = logdir)\n# pre_tensorboard_callback = TensorBoard(log_dir =\" ./logs/pre_\"+filename)\n\n# # predictor\n# pred_model = vgg_segnet(n_classes = hyperparameters['num_tasks'], input_height=hyperparameters['height'], input_width=hyperparameters['width'], encoder_level=2, channels=hyperparameters['channels'])\n# pred_model.compile(optimizer = keras.optimizers.SGD(learning_rate=hyperparameters['lr'],\n#                                                 clipnorm=1.0 ))\n\n# # defining my discriminator\n# disc_in, disc_out = discriminator()\n# disc = Model(inputs = disc_in, outputs = disc_out)\n# disc.compile(optimizer = keras.optimizers.SGD(learning_rate=hyperparameters['lr'],\n#                                               clipnorm=1.0 ))\n\n# # defining my generator\n# X, z, mu = variationalAutoEncoder()\n# vae = Model(inputs = X, outputs = [z,mu])\n# vae.compile(optimizer = keras.optimizers.RMSprop(learning_rate=hyperparameters['lr'],\n#                                                  clipnorm=1.0 ))\n\n# # Instantiate AL model\n# AL_model = ActiveLearning(discriminator=disc, generator=vae, predictor=pred_model, hyperparameters=hyperparameters, trackers = trackers)\n# AL_model.compile(\n#     d_optimizer=keras.optimizers.SGD(learning_rate=hyperparameters['lr'],clipnorm=1.0 ),\n#     g_optimizer= keras.optimizers.RMSprop(learning_rate=hyperparameters['lr'],clipnorm=1.0),\n#     p_optimizer=keras.optimizers.SGD(learning_rate=hyperparameters['lr'],clipnorm=1.0 ))\n\n# print('model loaded')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# p_optimizer=keras.optimizers.SGD(learning_rate=hyperparameters['lr'],clipnorm=1.0 )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i in train_gen_full:\n#     k=i\n#     break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# labelled_x=k[0]\n# labelled_y=k[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def dice_coef(y_true, y_pred):\n#     smooth = 1.0\n#     y_true_f = tf.cast(K.flatten(y_true), tf.float64)\n#     y_pred_f = tf.cast(K.flatten(y_pred), tf.float64)\n#     intersection = K.sum(y_true_f * y_pred_f)\n#     return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n# def dice_coef_loss(y_true, y_pred):\n#     t = []\n#     for i in range(7):\n#         t.append(1 - dice_coef(y_true[i], y_pred[i]))\n        \n#     return tf.convert_to_tensor(t)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with tf.GradientTape() as tape:\n#     labelled_prediction_y,_ = pred_model(labelled_x, training=True)\n    \n#     re_1=[]\n#     for i in range(7):\n#         re_1.append(tf.reshape(labelled_y[i], (hyperparameters['batch_size'], -1)))\n\n#     re_2 = []\n#     for i in range(7):\n#         re_2.append(tf.reshape(labelled_prediction_y[i], (hyperparameters['batch_size'], -1)))\n        \n#     weights = [{1: 1.0123, 0: 0.9879},\n#      {0: 0.6377, 1: 2.3142},\n#      {0: 0.5089, 1: 28.4156},\n#      {0: 0.5185, 1: 14.0129},\n#      {0: 0.5894, 1: 3.2964},\n#      {0: 0.5060, 1: 41.6062},\n#      {0: 0.5394, 1: 6.8425}]\n    \n#     w = []\n#     for i in range(7):\n#         wts = np.zeros((re_1[i].shape))\n#         for j in range(re_1[i].shape[0]):\n#             t = re_1[i][j].numpy()\n#             t = t.astype('float64')\n#             wts[j][t==0] = weights[i][0]\n#             wts[j][t==1] = weights[i][1]\n\n#         wts=tf.convert_to_tensor(wts)\n#         predictor_loss = weighted_cross_entropy(re_1[i], re_2[i], sample_weights=wts)\n#         w.append(predictor_loss)\n    \n#     w = tf.convert_to_tensor(w)\n#     w = tf.math.reduce_mean(w, axis=1) +  tf.cast(dice_coef_loss(re_1, re_2), tf.float32)\n\n# trainable_vars = pred_model.trainable_variables\n# gradients = tape.gradient(w, trainable_vars)\n\n# # Update weights\n# p_optimizer.apply_gradients(zip(gradients, trainable_vars))        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# with tf.GradientTape() as tape:\n#     labelled_prediction_y,_ = pred_model(labelled_x, training=True)\n    \n#     re_1=[]\n#     for i in range(7):\n#         re_1.append(tf.reshape(labelled_y[i], (hyperparameters['batch_size'], -1)))\n\n#     re_2 = []\n#     for i in range(7):\n#         re_2.append(tf.reshape(labelled_prediction_y[i], (hyperparameters['batch_size'], -1)))\n    \n# #     w = []\n# #     for i in range(7):\n# #         wts = np.zeros((re_1[i].shape))\n# #         for j in range(re_1[i].shape[0]):\n# #             t = re_1[i][j].numpy()\n# #             t = t.astype('float64')\n# #             wts[j][t==0] = weights[i][0]\n# #             wts[j][t==1] = weights[i][1]\n\n# #         wts=tf.convert_to_tensor(wts)\n# #         predictor_loss = weighted_cross_entropy(re_1[i], re_2[i], sample_weights=wts)\n# #         w.append(predictor_loss)\n# #     w = tf.convert_to_tensor(w)\n# #     w = tf.math.reduce_mean(w, axis=1)\n    \n# # trainable_vars = pred_model.trainable_variables\n# # gradients = tape.gradient(w, trainable_vars)\n\n# # # Update weights\n# # p_optimizer.apply_gradients(zip(gradients, trainable_vars))        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def dice_coef(y_true, y_pred):\n#     smooth = 1.0\n#     y_true_f = tf.cast(K.flatten(y_true), tf.float64)\n#     y_pred_f = tf.cast(K.flatten(y_pred), tf.float64)\n#     intersection = K.sum(y_true_f * y_pred_f)\n#     return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n\n# def dice_coef_loss(y_true, y_pred):\n#     t = []\n#     for i in range(7):\n#         t.append(1 - dice_coef(y_true[i], y_pred[i]))\n        \n#     return tf.convert_to_tensor(t)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# l = tfa.losses.SigmoidFocalCrossEntropy()\n# l(re_1, re_2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def weighted_cross_entropy(y_true, y_pred, sample_weights):\n#     y_true = tf.cast(y_true,y_pred.dtype)\n#     q1 = -1*(tf.math.multiply(y_true,tf.math.log(y_pred)) + tf.math.multiply((1.0-y_true),tf.math.log(1.0-y_pred)))\n#     sample_weights = tf.cast(sample_weights,q1.dtype)\n#     z = tf.math.multiply(q1,sample_weights)\n    \n#     return tf.math.reduce_mean(z,axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# weights = [{1: 1.0123, 0: 0.9879},\n#  {0: 0.6377, 1: 2.3142},\n#  {0: 0.5089, 1: 28.4156},\n#  {0: 0.5185, 1: 14.0129},\n#  {0: 0.5894, 1: 3.2964},\n#  {0: 0.5060, 1: 41.6062},\n#  {0: 0.5394, 1: 6.8425}]\n\n# w = []\n# for i in range(7):\n#     wts = np.zeros((re_1[i].shape))\n#     for j in range(re_1[i].shape[0]):\n#         t = re_1[i][j].numpy()\n#         t = t.astype('float64')\n#         wts[j][t==0] = weights[i][0]\n#         wts[j][t==1] = weights[i][1]\n        \n#     wts=tf.convert_to_tensor(wts)\n#     predictor_loss = weighted_cross_entropy(re_1[i], re_2[i], sample_weights=wts)\n#     w.append(predictor_loss)\n#     w = tf.convert_to_tensor(w)\n#     tf.math.reduce_mean(w, axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T06:25:06.040054Z","iopub.execute_input":"2022-06-13T06:25:06.040521Z","iopub.status.idle":"2022-06-13T06:25:06.045471Z","shell.execute_reply.started":"2022-06-13T06:25:06.040482Z","shell.execute_reply":"2022-06-13T06:25:06.044653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# a = tf.convert_to_tensor([[0.1,0.2,0.7], [0.4,0.3,0.5]])\n# y = tf.convert_to_tensor([[0.2,0.2,0.6], [0.1,0.1,0.8]])\n# sample_weights = tf.convert_to_tensor([[1,2,3], [2,4,5]])\n# a1 = weighted_cross_entropy(a,y,sample_weights)","metadata":{"execution":{"iopub.status.busy":"2022-06-13T06:25:06.856731Z","iopub.execute_input":"2022-06-13T06:25:06.85722Z","iopub.status.idle":"2022-06-13T06:25:06.864195Z","shell.execute_reply.started":"2022-06-13T06:25:06.857173Z","shell.execute_reply":"2022-06-13T06:25:06.862278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# iou=tf.keras.metrics.MeanIoU(num_classes=2)\n# iou(re_1[1], tf.cast(re_2[1]>=0.5, tf.int64))","metadata":{"execution":{"iopub.status.busy":"2022-06-13T06:25:07.225508Z","iopub.execute_input":"2022-06-13T06:25:07.225849Z","iopub.status.idle":"2022-06-13T06:25:07.23067Z","shell.execute_reply.started":"2022-06-13T06:25:07.22582Z","shell.execute_reply":"2022-06-13T06:25:07.229759Z"},"trusted":true},"execution_count":null,"outputs":[]}]}