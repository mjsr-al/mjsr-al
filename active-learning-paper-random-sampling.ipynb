{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# import tensorflow as tf\n# import random as random\n\n# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n# from tensorflow.keras.layers import Dense, Multiply, Softmax, Reshape, Input, Flatten, Conv2DTranspose, Conv2D, GlobalAveragePooling2D,BatchNormalization, Lambda, MaxPooling2D, ReLU, Dropout, Concatenate, Dot\n# from tensorflow.keras.optimizers import Adam, SGD\n# from tensorflow.keras.models import Model, model_from_json\n# from tensorflow.keras import regularizers\n# from tensorflow.keras.utils import plot_model, to_categorical\n# from tensorflow.keras.callbacks import ModelCheckpoint, LambdaCallback, ReduceLROnPlateau, CSVLogger, TensorBoard\n# from tensorflow.keras import backend as K\n\n# import datetime\n# from tensorflow.python.framework import ops\n# from tensorflow.python.ops import math_ops\n# from tensorflow.python.keras import backend as K\n# import tensorflow.keras as keras\n# import json","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-27T08:38:28.998328Z","iopub.execute_input":"2022-03-27T08:38:28.998588Z","iopub.status.idle":"2022-03-27T08:38:29.002396Z","shell.execute_reply.started":"2022-03-27T08:38:28.99856Z","shell.execute_reply":"2022-03-27T08:38:29.001683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# hyperparameters={\n#     'targets': ['image_id', 'Goatee','Heavy_Makeup', 'High_Cheekbones','No_Beard', 'Smiling'],  ### MODIFY \n#     'height': 224, \n#     'width': 192 ,\n#     'channels': 3, \n#     'batch_size': 16, \n#     'epochs': 1, \n#     'num_tasks': 5, ### MODIFY\n#     'initializer': 'he_uniform', \n#     'reg_lambda': 1e-3, \n#     'output': [2]*5, ### MODIFY === [2]*number of tasks\n#     'lr': 1e-5, ### MODIFY\n#     'is_trained': False, \n#     'dropout_prob': 0.3,\n#     'enable_cs': False, \n#     'enable_sluice': False,\n#     'initial_percent':0.6,\n#     'initial_train_epoch': 5,\n#     'increment_train_epoch':2,\n#     'uncertainity_repeat': 5,\n#     'num_uncertain_elements': 5008,\n#     'additional_epoch': 1,\n#     'pretraining_epochs': 1,\n#     'train_initial_batches':6103,\n#     'enable_additional': False,\n#     'additional_attr_count':2,\n#     'all_updates':False\n#     }","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.011692Z","iopub.execute_input":"2022-03-27T08:38:29.012216Z","iopub.status.idle":"2022-03-27T08:38:29.018078Z","shell.execute_reply.started":"2022-03-27T08:38:29.012186Z","shell.execute_reply":"2022-03-27T08:38:29.017404Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def preprocess(hyperparameters, attr, eval_partition):\n#     attr = attr[hyperparameters['targets']]\n#     attr = attr.replace(-1, 0)\n#     attr = attr.set_index('image_id')\n#     eval_partition = eval_partition.set_index('image_id')\n#     attr = attr.join(eval_partition)\n#     attr['image_id'] = attr.index\n    \n#     for column in attr.columns[:-2]:\n#         k = to_categorical(attr[column])\n#         attr = attr.drop(column, axis=1)\n#         attr[column] = k.tolist()\n\n#     train = attr.loc[attr['partition']==0]\n#     val = attr.loc[attr['partition']==1]\n#     test = attr.loc[attr['partition']==2]\n    \n#     train = train.drop('partition', axis=1)\n#     val = val.drop('partition', axis=1)\n#     test = test.drop('partition', axis=1)\n    \n#     train = train[:(len(train)//hyperparameters['batch_size'])*hyperparameters['batch_size']]\n#     val = val[:(len(val)//hyperparameters['batch_size'])*hyperparameters['batch_size']]\n#     test = test[:(len(test)//hyperparameters['batch_size'])*hyperparameters['batch_size']]\n    \n#     return (train, val, test)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.030559Z","iopub.execute_input":"2022-03-27T08:38:29.030869Z","iopub.status.idle":"2022-03-27T08:38:29.035261Z","shell.execute_reply.started":"2022-03-27T08:38:29.030839Z","shell.execute_reply":"2022-03-27T08:38:29.03431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def load_generator(df, shuffle=True):\n    \n#     # image_id = '000014.jpg'\n#     image_path = '../input/celeba-dataset/img_align_celeba/img_align_celeba/' \n#     #'./dataset/img_align_celeba/img_align_celeba/'\n#     data_gen = ImageDataGenerator(rescale=1/255.0)\n    \n#     generator = data_gen.flow_from_dataframe(dataframe = df, \n#                                      directory=image_path, \n#                                      x_col = 'image_id', \n#                                      y_col=hyperparameters['targets'][1:], \n#                                      class_mode = 'multi_output',\n#                                      target_size=(hyperparameters['height'], hyperparameters['width']), \n#                                      batch_size = hyperparameters['batch_size'],shuffle=shuffle)\n    \n#     return generator\n\n# def generate_generator_multiple(generator,dir1, dir2, df1,df2,batch_size, img_height,img_width,shuffle=True):\n#     # labelled \n#     genX1 = generator.flow_from_dataframe(df1, dir1,\n#                                           x_col = 'image_id', \n#                                           y_col=hyperparameters['targets'][1:], \n#                                           class_mode = 'multi_output',\n#                                           target_size=(img_height,img_width), \n#                                           batch_size = hyperparameters['batch_size'],\n#                                           shuffle=shuffle)\n#     # train --- make it train generator\n#     genX2 = generator.flow_from_dataframe(df2, dir2,\n#                                           x_col = 'image_id', \n#                                           y_col=hyperparameters['targets'][1:], \n#                                           class_mode = 'multi_output',\n#                                           target_size=(img_height, img_width), \n#                                           batch_size = batch_size,\n#                                           shuffle=shuffle)\n#     while True:\n#             X1,y1 = genX1.next()\n#             X2,_ = genX2.next()\n#             yield X1, X2, y1  #Yield both images and their mutual label     ","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.049472Z","iopub.execute_input":"2022-03-27T08:38:29.049849Z","iopub.status.idle":"2022-03-27T08:38:29.054482Z","shell.execute_reply.started":"2022-03-27T08:38:29.04982Z","shell.execute_reply":"2022-03-27T08:38:29.053555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # changed input to 360+360\n# def discriminator():\n    \n#     X = Input((720,), name='input_disc')\n    \n#     x = Dense(units = 512, activation = None, name='dense_1_disc')(X)\n#     x = BatchNormalization(name = 'bn_1_disc')(x)\n#     x = ReLU(name='relu_1_disc')(x)\n#     x = Dense(units = 512, activation = None, name='dense_2_disc')(x)\n#     x = BatchNormalization(name = 'bn_2_disc')(x)\n#     x = ReLU(name='relu_2_disc')(x)\n#     x = Dense(units = 1, activation = 'sigmoid', name='output_disc')(x)\n    \n#     return X,x","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.06576Z","iopub.execute_input":"2022-03-27T08:38:29.066242Z","iopub.status.idle":"2022-03-27T08:38:29.069666Z","shell.execute_reply.started":"2022-03-27T08:38:29.066213Z","shell.execute_reply":"2022-03-27T08:38:29.068761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def addConvBlock(num_filters, kernel_size, hyperparameters, pool_size, tops, stride, pad, pool_stride, isPool , i):\n    \n#     for task_id in range(hyperparameters['num_tasks']+hyperparameters['enable_additional']*hyperparameters['additional_attr_count']):\n        \n#         tops[task_id] = Conv2D(num_filters, kernel_size=kernel_size, name = 'conv'+str(i)+'_'+str(task_id),  strides=(stride, stride), padding = pad, kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l1(hyperparameters['reg_lambda']))(tops[task_id])\n#         tops[task_id] = ReLU(name='relu'+str(i)+'_'+str(task_id))(tops[task_id])\n\n#         if (isPool==True):\n#             name = 'pool'+str(i)+'_'+str(task_id)\n#             tops[task_id] = MaxPooling2D(pool_size=(pool_size, pool_size), name = name, strides = (pool_stride, pool_stride), padding='valid')(tops[task_id])\n            \n#     return tops","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.100256Z","iopub.execute_input":"2022-03-27T08:38:29.100822Z","iopub.status.idle":"2022-03-27T08:38:29.105763Z","shell.execute_reply.started":"2022-03-27T08:38:29.10079Z","shell.execute_reply":"2022-03-27T08:38:29.104956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def combined_model():\n    \n#     num_tasks = hyperparameters['num_tasks']+hyperparameters['enable_additional']*hyperparameters['additional_attr_count']\n    \n#     x,y,z = hyperparameters['height'], hyperparameters['width'], hyperparameters['channels']\n#     X = Input((x,y,z), name = 'input_predictor_'+\"_\".join(str(num) for num in range(num_tasks)))\n#     tops = [X]*num_tasks\n\n# # ------------------------------------------- Block 1 BEGINS ------------------------------------\n\n#     tops = addConvBlock(40, 5, hyperparameters, 3, tops, 1, 'same', 2, True, 1)\n#     if hyperparameters[\"enable_cs\"]:\n#         cs1 = CrossStitch(num_tasks,1, hyperparameters, True)(tops) \n#         tops = tf.unstack(cs1, axis=0)\n    \n#     tops = addConvBlock(60, 5, hyperparameters, 3, tops, 1, 'same', 2, True, 2)\n#     if hyperparameters[\"enable_cs\"]:\n#         cs2 = CrossStitch(num_tasks, 2, hyperparameters, True)(tops)\n#         tops = tf.unstack(cs2, axis=0)\n    \n#     tops = addConvBlock(80, 3, hyperparameters, 3, tops, 1, 'same', 2, True, 3)\n#     if hyperparameters[\"enable_cs\"]:\n#         cs3 = CrossStitch(num_tasks, 3, hyperparameters, True)(tops)\n#         tops = tf.unstack(cs3, axis=0)\n        \n    \n#     tops = addConvBlock(100, 3, hyperparameters, 3, tops, 1, 'same', 2, True,  4)  \n#     if hyperparameters[\"enable_cs\"]:\n#         cs4 = CrossStitch(num_tasks, 4, hyperparameters, True)(tops)\n#         tops = tf.unstack(cs4, axis=0)\n\n# # ------------------------------------------- Block 4 ENDS ------------------------------------\n    \n# # ------------------------------------------- Block 5 BEGINS ------------------------------------\n#     tops = addConvBlock(140, 2, hyperparameters, 3, tops, 1, 'same', 2, True, 5)\n#     if hyperparameters[\"enable_cs\"]:\n#         cs5 = CrossStitch(num_tasks, 5, hyperparameters, True)(tops)\n#         tops = tf.unstack(cs5, axis=0)\n\n# # ------------------------------------------- Block 5 ENDS ------------------------------------\n    \n#     latents=[]\n#     weights=[]\n#     for task_id in range(num_tasks): \n\n#         tops[task_id] = Flatten(name = 'flat'+'_'+str(task_id))(tops[task_id])\n\n#         tops[task_id] = Dense(units = 720, name = 'dense0'+'_'+str(task_id), kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(tops[task_id])\n#         if (task_id==0):\n#             tops[task_id] = ReLU(name='re_lu0'+'_'+str(0))(tops[task_id])\n#         else:\n#             tops[task_id] = ReLU(name='re_lu0'+'_'+str(task_id)+'_'+str(task_id))(tops[task_id])\n            \n            \n#         tops[task_id] = Dense(units = 360, name = 'dense1'+'_'+str(task_id), kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(tops[task_id])\n#         if (task_id==0):\n#             tops[task_id] = ReLU(name='re_lu'+'_'+str(0))(tops[task_id])\n#         else:\n#             tops[task_id] = ReLU(name='re_lu'+'_'+str(task_id)+'_'+str(task_id))(tops[task_id])\n                                                        \n#         latents.append(tops[task_id])\n#         weights.append(tops[task_id])\n        \n#         tops[task_id] = Dense(units = 180, name = 'dense2'+'_'+str(task_id), kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(tops[task_id])\n#         if (task_id==0):\n#             tops[task_id] = ReLU(name='re_lu2'+'_'+str(0))(tops[task_id])\n#         else:\n#             tops[task_id] = ReLU(name='re_lu2'+'_'+str(task_id)+'_'+str(task_id))(tops[task_id])\n        \n\n#     # added joined weights\n#     joint = Concatenate(name='joined_1')(weights)\n#     joint = Dense(units = 360, name=\"joined_2\",kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(joint)\n#     joint = Dense(units = num_tasks, name=\"joined_3\",activation = 'softmax', kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(joint)\n#     tops = taskEmbeddings(5)([tops, joint])\n       \n#     for task_id in range(num_tasks):\n#         tops[task_id] = Dense(units = 90, name = 'dense3'+'_'+str(task_id), kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(tops[task_id])\n#         if (task_id==0):\n#             tops[task_id] = ReLU(name='re_lu3'+'_'+str(0))(tops[task_id])\n#         else:\n#             tops[task_id] = ReLU(name='re_lu3'+'_'+str(task_id)+'_'+str(task_id))(tops[task_id])\n    \n\n#     for task_id in range(num_tasks): \n#         tops[task_id] = Dense(units = 2, name='output'+'_'+str(task_id),activation = 'softmax', kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(tops[task_id])    \n        \n#     return X, tops, latents, joint","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.130507Z","iopub.execute_input":"2022-03-27T08:38:29.130758Z","iopub.status.idle":"2022-03-27T08:38:29.139848Z","shell.execute_reply.started":"2022-03-27T08:38:29.130727Z","shell.execute_reply":"2022-03-27T08:38:29.137956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class CrossStitch(tf.keras.layers.Layer):\n\n#     def __init__(self, num_tasks,layer_num, hyperparameters, trainable, *args, **kwargs):\n#         self.num_tasks = num_tasks\n#         self.layer_num = layer_num\n#         self.hyperparameters = hyperparameters\n#         self.trainable = trainable\n#         super(CrossStitch, self).__init__(**kwargs)\n\n#     def build(self, input_shape):\n#         self.kernel = []\n#         for task_num in range(self.hyperparameters['num_tasks']):\n#             self.kernel.append(self.add_weight(name='cs_kernel_'+str(task_num)+'_'+str(self.layer_num), shape=(1, self.hyperparameters['num_tasks']),initializer=CrossStitchWeightInitializer(task_num, self.hyperparameters['num_tasks']),trainable=self.trainable))\n\n#         if self.hyperparameters['enable_additional'] == True:\n#             for task_num in range(self.hyperparameters['num_tasks'], self.num_tasks):\n#                 self.kernel.append(self.add_weight(name='cs_kernel_'+str(task_num)+'_'+str(self.layer_num), shape=(1, self.num_tasks),initializer=CrossStitchWeightInitializer(task_num, self.num_tasks),trainable=self.trainable))\n#         super(CrossStitch, self).build(input_shape)  \n\n#     def call(self, input_feature_maps):        \n#         if (len(input_feature_maps)!=self.num_tasks):\n#             print(\"ERROR IN CROSS-STITCH\")\n      \n#         output_feature_maps = []\n#         for current_task in range(self.hyperparameters['num_tasks']):\n#             output = tf.math.scalar_mul(self.kernel[current_task][0,current_task], input_feature_maps[current_task])\n#             for other_task in range(self.hyperparameters['num_tasks']):\n#                 if (current_task==other_task):\n#                     continue    \n#                 output+= tf.math.scalar_mul(self.kernel[current_task][0,other_task], input_feature_maps[other_task])\n#             output_feature_maps.append(output)\n        \n#         if self.hyperparameters['enable_additional'] == True:\n#             for current_task in range(self.hyperparameters['num_tasks'], self.num_tasks):\n#                 output = tf.math.scalar_mul(self.kernel[current_task][0,current_task], input_feature_maps[current_task])\n#                 for other_task in range(self.num_tasks):\n#                     if (current_task==other_task):\n#                         continue\n#                     output+= tf.math.scalar_mul(self.kernel[current_task][0,other_task], input_feature_maps[other_task])\n#                 output_feature_maps.append(output)\n#         return tf.stack(output_feature_maps, axis=0)\n  \n#     def compute_output_shape(self, input_shape):\n#         return [self.num_tasks] + input_shape\n\n#     def get_config(self):\n#         base_config = super(CrossStitch, self).get_config()\n#         base_config['num_tasks'] = self.num_tasks\n#         base_config['layer_num'] = self.task_num\n#         base_config['trainable'] = self.trainable\n#         base_config['hyperparameters'] = self.hyperparameters\n\n#         return base_config\n","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.14443Z","iopub.execute_input":"2022-03-27T08:38:29.145095Z","iopub.status.idle":"2022-03-27T08:38:29.15214Z","shell.execute_reply.started":"2022-03-27T08:38:29.145064Z","shell.execute_reply":"2022-03-27T08:38:29.151433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class CrossStitchWeightInitializer(tf.keras.initializers.Initializer):\n\n#     def __init__(self, index, num_tasks):\n#         self.index=index\n#         self.num_tasks = num_tasks\n\n#     def __call__(self, shape, dtype=None):\n#         lis = [[0]*self.num_tasks]\n#         lis[0][self.index]=1\n#         return tf.Variable(lis, dtype=tf.float32)\n\n#     def get_config(self):  # To support serialization\n#         return {'index':self.index, 'num_tasks':self.num_tasks}","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.157925Z","iopub.execute_input":"2022-03-27T08:38:29.15871Z","iopub.status.idle":"2022-03-27T08:38:29.164663Z","shell.execute_reply.started":"2022-03-27T08:38:29.158657Z","shell.execute_reply":"2022-03-27T08:38:29.163834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def variationalAutoEncoder(embed_size=360):\n#     # input\n#     X = Input((hyperparameters['height'],hyperparameters['width'],hyperparameters['channels']), name=\"input_vae\")\n    \n#     # encoder\n#     x = Conv2D(128, kernel_size=(4,4), strides = (2,2), padding='same', use_bias=False, name='conv_1_vae')(X)\n#     x = BatchNormalization(name='bn_1_vae')(x)\n#     x = ReLU(name='relu_1_vae')(x)\n#     x = Conv2D(256, kernel_size=(4,4), strides = (2,2), padding='same', use_bias=False, name='conv_2_vae')(x)\n#     x = BatchNormalization(name='bn_2_vae')(x)\n#     x = ReLU(name='relu_2_vae')(x)\n#     x = Conv2D(512, kernel_size=(4,4), strides = (2,2), padding='same', use_bias=False, name='conv_3_vae')(x)\n#     x = BatchNormalization(name='bn_3_vae')(x)\n#     x = ReLU(name='relu_3_vae')(x)\n#     x = Conv2D(1024, kernel_size=(4,4), strides = (2,2), padding='same', use_bias=False, name='conv_4_vae')(x)\n#     x = BatchNormalization(name='bn_4_vae')(x)\n#     x = ReLU(name='relu_4_vae')(x)\n    \n#     x = Flatten(name='flatten_1_vae')(x)\n    \n#     mu = Dense(units = embed_size, activation = None, name='dense_1_vae')(x)\n    \n#     #logvar = Dense(units = 32, activation = None)(x)\n#     # check what to be done here --->  ??\n#     #stds = Lambda(lambda x: x * 0.5)(logvar)\n#     #stds = tf.keras.backend.exp(stds)\n#     #epsilon = tf.keras.backend.random_normal((32,))\n#     #m = tf.keras.layers.Multiply()([stds,epsilon])\n#     #latents = tf.keras.layers.Add()([m,mu])\n        \n#     # decoder\n#     z = Dense(units = 14*12*1024, activation = None, name='dense_2_vae')(mu)\n#     z = Reshape((14,12,1024), name='reshape_vae')(z)\n#     z = Conv2DTranspose(512, kernel_size=(4,4), strides = (2,2), padding='same', use_bias=False, name='conv_5_vae')(z)\n#     z = BatchNormalization(name='bn_5_vae')(z)\n#     z = ReLU(name='relu_5_vae')(z)\n#     z = Conv2DTranspose(256, kernel_size=(4,4), strides = (2,2), padding='same', use_bias=False, name='conv_6_vae')(z)\n#     z = BatchNormalization(name='bn_6_vae')(z)\n#     z = ReLU(name='relu_6_vae')(z)\n#     z = Conv2DTranspose(128, kernel_size=(4,4), strides = (2,2), padding='same', use_bias=False, name='conv_7_vae')(z)\n#     z = BatchNormalization(name='bn_7_vae')(z)\n#     z = ReLU(name='relu_7_vae')(z)\n#     z = Conv2DTranspose(3, strides=(2,2), kernel_size=(1,1), name='conv_8_vae')(z)\n    \n#     return X, z, mu","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.182254Z","iopub.execute_input":"2022-03-27T08:38:29.182562Z","iopub.status.idle":"2022-03-27T08:38:29.187685Z","shell.execute_reply.started":"2022-03-27T08:38:29.182532Z","shell.execute_reply":"2022-03-27T08:38:29.186666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def single_model(X):\n\n# # ------------------------------------------- Block 1 BEGINS ------------------------------------\n\n#     tops = addConvBlock(40, 5, hyperparameters, 3, X, 1, 'same', 2, True, 1)\n\n\n# # ------------------------------------------- Block 1 ENDS ------------------------------------\n\n# # ------------------------------------------- Block 2 BEGINS ------------------------------------\n#     tops = addConvBlock(60, 5, hyperparameters, 3, tops, 1, 'same', 2, True, 2)\n\n# # ------------------------------------------- Block 2 ENDS ------------------------------------\n# # ------------------------------------------- Block 3 BEGINS ------------------------------------\n\n#     tops = addConvBlock(80, 3, hyperparameters, 3, tops, 1, 'same', 2, True, 3)\n\n# # ------------------------------------------- Block 3 ENDS ------------------------------------\n\n# # ------------------------------------------- Block 4 BEGINS ------------------------------------\n#     tops = addConvBlock(100, 3, hyperparameters, 3, tops, 1, 'same', 2, True,  4)\n\n# # ------------------------------------------- Block 4 ENDS ------------------------------------\n\n# # ------------------------------------------- Block 5 BEGINS ------------------------------------\n#     tops = addConvBlock(140, 2, hyperparameters, 3, tops, 1, 'same', 2, True, 5)\n\n# # ------------------------------------------- Block 5 ENDS ------------------------------------\n\n#     tops = Flatten(name = 'flat')(tops)\n#     latent = Dense(units = 360, name = 'dense1', kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(tops)\n#     latent = ReLU()(latent)\n\n#     output = Dense(units = 2, name='output',activation = 'softmax', kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(latent)\n\n#     # add 360 wala unit to o/p\n#     return output, latent","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.195227Z","iopub.execute_input":"2022-03-27T08:38:29.195785Z","iopub.status.idle":"2022-03-27T08:38:29.20075Z","shell.execute_reply.started":"2022-03-27T08:38:29.195757Z","shell.execute_reply":"2022-03-27T08:38:29.199769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def addConvBlock_v1(num_filters, kernel_size, hyperparameters, pool_size, tops, stride, pad, pool_stride, isPool , i):\n    \n#     input_tensor = tops\n#     conv = Conv2D(num_filters, kernel_size=kernel_size, name = 'conv'+str(i),  strides=(stride, stride), padding = pad, kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l1(hyperparameters['reg_lambda']))(input_tensor)\n#     conv = ReLU(name='relu'+str(i))(conv)\n\n#     if (isPool==True):\n#         name = 'pool'+str(i)\n#         conv = MaxPooling2D(pool_size=(pool_size, pool_size), name = name, strides = (pool_stride, pool_stride), padding='valid')(conv)\n#     tops= conv\n  \n#     return tops\n\n\n# def predictor_v1():\n#     x,y,z = hyperparameters['height'], hyperparameters['width'], hyperparameters['channels']\n#     X = Input((x,y,z), name='input_predictor')\n#     model_list=[]\n    \n#     output_list=[]\n#     latent_list=[]\n#     for task in range(hyperparameters['num_tasks']+hyperparameters['enable_additional']*hyperparameters['additional_attr_count']):\n#         output, latent = single_model(X) \n#         output_list.append(output)\n#         latent_list.append(latent)\n#         model = Model(inputs=X, outputs=[output, latent])\n#         for i in model.layers:\n#             if (i._name[0:5] == 're_lu'):\n#                 if (task==0):\n#                     i._name = 're_lu'+'_'+str(0)\n#                 else:\n#                     i._name='re_lu'+'_'+str(task)+'_'+str(task)\n#             else:   \n#                 i._name+='_'+str(task)\n\n#         model_list.append(model)\n#     model = Model(inputs = X, outputs = [output_list, latent_list])\n#     # model = Model(inputs = X, outputs = [model.output for model in model_list])\n    \n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.212112Z","iopub.execute_input":"2022-03-27T08:38:29.212545Z","iopub.status.idle":"2022-03-27T08:38:29.217014Z","shell.execute_reply.started":"2022-03-27T08:38:29.212514Z","shell.execute_reply":"2022-03-27T08:38:29.216127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def predictor():\n#     X, tops, latents,joint = combined_model()\n#     model = Model(inputs=X, outputs=[tops,latents,joint])\n#     return model","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.228228Z","iopub.execute_input":"2022-03-27T08:38:29.228738Z","iopub.status.idle":"2022-03-27T08:38:29.232278Z","shell.execute_reply.started":"2022-03-27T08:38:29.228685Z","shell.execute_reply":"2022-03-27T08:38:29.231417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class taskEmbeddings(tf.keras.layers.Layer):\n#     def __init__(self, num_layers, **kwargs):\n#         self.num_layers = num_layers\n#         super(taskEmbeddings, self).__init__(**kwargs)\n\n#     def build(self, input_shape):\n#         super(taskEmbeddings, self).build(input_shape)\n\n#     def call(self, x):\n\n#         dynamic_weights = x[1] # 16x5\n#         output_feature_maps = x[0] # 16x5x90\n\n#         data = np.identity(len(output_feature_maps), dtype = np.float32)\n#         #onehotencoder = OneHotEncoder() # categorical_features = [0]\n#         #data = onehotencoder.fit_transform(np.arange(len(output_feature_maps)).reshape((-1,1))).toarray()\n#         weighted_output=[]\n#         for output in range(len(output_feature_maps)):\n#             one_hot = tf.constant(data[output])\n#             one_hot = tf.keras.backend.reshape(one_hot, shape=(-1, len(output_feature_maps)))\n#             #one_hot = tf.repeat(one_hot, repeats=[hyperparameters['batch_size']], axis = 0)\n#             product1 = Dot(axes=1)([dynamic_weights, one_hot]) # 16x1\n#             product1 = tf.repeat(product1, repeats=180, axis = 1)\n#             product2 = Multiply()([product1, output_feature_maps[output]])\n#             weighted_output.append(product2)\n\n#         return weighted_output\n\n#     #def compute_output_shape(self, input_shape):\n#     #    return input_shape\n\n#     def get_config(self):\n#         base_config = super(taskEmbeddings, self).get_config()\n#         base_config['num_layers'] = self.num_layers\n#         return base_config","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.248104Z","iopub.execute_input":"2022-03-27T08:38:29.248418Z","iopub.status.idle":"2022-03-27T08:38:29.253082Z","shell.execute_reply.started":"2022-03-27T08:38:29.248387Z","shell.execute_reply":"2022-03-27T08:38:29.252248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class ActiveLearning(keras.Model):\n#     def __init__(self, discriminator, generator, predictor, trackers, alpha):\n#         super(ActiveLearning, self).__init__()\n#         self.discriminator = discriminator\n#         self.generator = generator\n#         self.predictor = predictor\n#         self.trackers = trackers\n#         self.alpha=alpha\n#         self.prev_loss = 10000\n\n#     def compile(self, d_optimizer, g_optimizer, p_optimizer):\n#         super(ActiveLearning, self).compile()\n#         self.d_optimizer = d_optimizer\n#         self.g_optimizer = g_optimizer\n#         self.p_optimizer = p_optimizer\n                \n#     def train_step(self, real_images):\n        \n#         # get labelled_x, unlabelled_x and labelled_y\n        \n#         x = real_images\n#         labelled_x = x[0]\n#         unlabelled_x = x[1]\n#         labelled_y = x[2]\n        \n#         ##### TRAIN THE PREDICTOR #####\n        \n        \n#         # Compute output and latents\n#         with tf.GradientTape() as tape:\n#             labelled_prediction_y, _, _ = self.predictor(labelled_x, training=True)\n#             predictor_loss = keras.losses.categorical_crossentropy(labelled_y, labelled_prediction_y) # ----> 1\n        \n#         # Compute gradients\n#         trainable_vars = self.predictor.trainable_variables\n#         gradients = tape.gradient(predictor_loss, trainable_vars)\n\n#         # Update weights\n#         self.p_optimizer.apply_gradients(zip(gradients, trainable_vars)) \n        \n#         # ------------------------------------------------------------------------------------------------\n        \n#         ##### TRAIN THE GENERATOR #####\n        \n#         # Create labels for VAE\n#         labelled_disc_true = np.ones((hyperparameters['batch_size'],1))\n#         unlabelled_disc_fake = np.ones((hyperparameters['batch_size'],1))\n        \n#         # Compute VAE outputs\n#         with tf.GradientTape() as tape:\n#             # Compute generator o/p\n#             labelled_vae_y, labelled_vae_latent = self.generator(labelled_x)\n#             unlabelled_vae_y, unlabelled_vae_latent = self.generator(unlabelled_x)\n            \n#             # Calculate loss for VAE\n#             labelled_vae_loss = keras.losses.mean_squared_error(labelled_x, labelled_vae_y) # ----> 2\n#             unlabelled_vae_loss = keras.losses.mean_squared_error(unlabelled_x, unlabelled_vae_y) # ----> 2\n            \n#             # Compute predictor o/p\n#            # _, labelled_predictor_latent = self.predictor(labelled_x)\n#             #_, unlabelled_predictor_latent = self.predictor(unlabelled_x)\n            \n#             # Average out the latents for 5 tasks --- SHOULD I?\n#             # labelled_vae_latent = math_ops.mean(ops.convert_to_tensor(labelled_vae_latent), axis=0)\n#             # unlabelled_vae_latent = math_ops.mean(ops.convert_to_tensor(unlabelled_vae_latent), axis=0)\n#             #labelled_predictor_latent = math_ops.mean(ops.convert_to_tensor(labelled_predictor_latent), axis=0)\n#             #unlabelled_predictor_latent = math_ops.mean(ops.convert_to_tensor(unlabelled_predictor_latent), axis=0)\n            \n#             # Join vae and predictor latents\n#             #labelled_disc_in = tf.concat([labelled_vae_latent,labelled_predictor_latent],axis=1)\n#             #unlabelled_disc_in = tf.concat([unlabelled_vae_latent,unlabelled_predictor_latent],axis=1)            \n            \n#             # Compute disc o/p\n#             #labelled_disc_y = self.discriminator(labelled_disc_in)\n#             #unlabelled_disc_y = self.discriminator(unlabelled_disc_in)\n            \n#             # Calculate loss for disc\n#             #labelled_disc_loss = keras.losses.binary_crossentropy(labelled_disc_true, labelled_disc_y) # ----> 3\n#             #unlabelled_dic_loss = keras.losses.binary_crossentropy(unlabelled_disc_fake, unlabelled_disc_y) # ----> 3\n            \n#             # Compute total VAE loss\n#             #disc_loss = labelled_disc_loss + unlabelled_dic_loss\n#             vae_loss = labelled_vae_loss + unlabelled_vae_loss #+ (self.advisory_param*disc_loss)\n        \n#         # Compute gradients\n#         trainable_vars = self.generator.trainable_variables\n#         gradients = tape.gradient(vae_loss, trainable_vars)\n        \n#         # Update weights\n#         self.g_optimizer.apply_gradients(zip(gradients, trainable_vars))         \n        \n#         # ------------------------------------------------------------------------------------------------\n        \n#         ##### TRAIN THE DISCRIMINATOR #####\n        \n#         # Create disc labels\n#         labelled_disc_true = np.ones((hyperparameters['batch_size'],1))\n#         unlabelled_disc_true = np.zeros((hyperparameters['batch_size'],1))\n        \n#         # Compute VAE latents\n#         _, labelled_vae_latent = self.generator(labelled_x, training = False)\n#         _, unlabelled_vae_latent = self.generator(unlabelled_x, training = False)\n        \n#         # Compute predictor latents\n#         _, labelled_predictor_latent, _ = self.predictor(labelled_x, training=False)\n#         _, unlabelled_predictor_latent, _ = self.predictor(unlabelled_x, training=False)\n        \n#         # Average out the latents for 5 tasks --- SHOULD I?\n#         # labelled_vae_latent = math_ops.mean(ops.convert_to_tensor(labelled_vae_latent), axis=0)\n#         # unlabelled_vae_latent = math_ops.mean(ops.convert_to_tensor(unlabelled_vae_latent), axis=0)\n#         labelled_predictor_latent = math_ops.mean(ops.convert_to_tensor(labelled_predictor_latent), axis=0)\n#         unlabelled_predictor_latent = math_ops.mean(ops.convert_to_tensor(unlabelled_predictor_latent), axis=0)\n        \n#         # Join vae and predictor latents\n#         labelled_disc_in = tf.concat([labelled_vae_latent,labelled_predictor_latent],axis=1)\n#         unlabelled_disc_in = tf.concat([unlabelled_vae_latent,unlabelled_predictor_latent],axis=1)\n        \n#         # Compute disc output\n#         with tf.GradientTape() as tape:\n#             labelled_disc_y = self.discriminator(labelled_disc_in,training=True)\n#             unlabelled_disc_y = self.discriminator(unlabelled_disc_in,training=True)\n            \n#             labelled_disc_loss = keras.losses.binary_crossentropy(labelled_disc_true, labelled_disc_y) # ----> 3\n#             unlabelled_dic_loss = keras.losses.binary_crossentropy(unlabelled_disc_true, unlabelled_disc_y) # ----> 3\n            \n#             disc_loss = labelled_disc_loss + unlabelled_dic_loss\n        \n#         # Compute gradients\n#         trainable_vars = self.discriminator.trainable_variables\n#         gradients = tape.gradient(disc_loss, trainable_vars)\n        \n#         # Update weights\n#         self.d_optimizer.apply_gradients(zip(gradients, trainable_vars)) \n    \n#         # ------------------------------------------------------------------------------------------------\n        \n#         # Computing Metrics\n        \n#         # For predictor\n        \n#         self.trackers['loss_tracker_predictor'].update_state(labelled_y, labelled_prediction_y)\n#         self.trackers['acc_metric_predictor'].update_state(labelled_y, labelled_prediction_y)\n        \n#         #loss_tracker_predictor.update_state(labelled_y, labelled_prediction_y)\n#         #acc_metric_predictor.update_state(labelled_y, labelled_prediction_y)\n        \n#         for i in range(hyperparameters['num_tasks']):\n#             self.trackers['individual_loss_tracker_predictor'][i].update_state(labelled_y[i], labelled_prediction_y[i])\n#             self.trackers['individual_acc_metric_predictor'][i].update_state(labelled_y[i], labelled_prediction_y[i])\n                \n#         # For VAE\n#         self.trackers['loss_tracker_generator'].update_state(labelled_x, labelled_vae_y)\n#         self.trackers['loss_tracker_generator'].update_state(unlabelled_x, unlabelled_vae_y)\n#         # For Discriminator\n#         self.trackers['loss_tracker_disc'].update_state(labelled_disc_true,labelled_disc_y)\n#         self.trackers['loss_tracker_disc'].update_state(unlabelled_disc_true,unlabelled_disc_y)\n#         self.trackers['acc_tracker_disc'].update_state(labelled_disc_true,labelled_disc_y)\n#         self.trackers['acc_tracker_disc'].update_state(unlabelled_disc_true,unlabelled_disc_y)\n\n\n            \n#         ret_dic = {\"loss_predictor_total\": self.trackers['loss_tracker_predictor'].result(), # loss_tracker_predictor.result(), \n#                    \"acc_predictor\":self.trackers['acc_metric_predictor'].result(), # acc_metric_predictor.result(), \n#                    \"loss_VAE\":  self.trackers['loss_tracker_generator'].result(), # loss_tracker_generator.result(),\n#                    \"loss_disc\": self.trackers['loss_tracker_disc'].result(), # loss_tracker_disc.result(),\n#                    \"acc_disc\": self.trackers['acc_tracker_disc'].result()} # acc_tracker_disc.result()}\n        \n#         for i in range(hyperparameters['num_tasks']):\n#             ret_dic[\"loss_predictor_\"+str(i)] = self.trackers['individual_loss_tracker_predictor'][i].result() # individual_loss_tracker_predictor[i].result()\n#         for i in range(hyperparameters['num_tasks']):\n#             ret_dic[\"acc_predictor_\"+str(i)] = self.trackers['individual_acc_metric_predictor'][i].result() # individual_acc_metric_predictor[i].result()\n            \n#         return ret_dic\n    \n#     def call(self, x):\n#         return\n    \n#     def test_step(self, real_images):\n        \n#         x = real_images\n#         labelled_x = x[0]\n#         labelled_y = x[1]\n        \n#         # Predictor step\n#         labelled_prediction_y, labelled_predictor_latent, _ = self.predictor(labelled_x, training=False)\n        \n#         # Generator step\n#         labelled_vae_y, labelled_vae_latent = self.generator(labelled_x, training=False)\n        \n#         # Discriminator step\n#         labelled_predictor_latent = math_ops.mean(ops.convert_to_tensor(labelled_predictor_latent), axis=0)\n#         labelled_disc_in = tf.concat([labelled_vae_latent,labelled_predictor_latent],axis=1)\n        \n#         labelled_disc_y = self.discriminator(labelled_disc_in,training=False)\n        \n#         # Updating metrics\n#         # For Predictor\n#         self.trackers['loss_tracker_predictor'].update_state(labelled_y, labelled_prediction_y)\n#         self.trackers['acc_metric_predictor'].update_state(labelled_y, labelled_prediction_y)\n        \n#         for i in range(hyperparameters['num_tasks']):\n#             self.trackers['individual_loss_tracker_predictor'][i].update_state(labelled_y[i], labelled_prediction_y[i])\n#             self.trackers['individual_acc_metric_predictor'][i].update_state(labelled_y[i], labelled_prediction_y[i])\n            \n#         self.trackers['loss_tracker_generator'].update_state(labelled_x, labelled_vae_y)\n        \n        \n#         # For Discriminator\n#         labelled_disc_true = np.ones((hyperparameters['batch_size'],1))\n#         self.trackers['loss_tracker_disc'].update_state(labelled_disc_true,labelled_disc_y)\n#         self.trackers['acc_tracker_disc'].update_state(labelled_disc_true,labelled_disc_y)\n        \n        \n#         ret_dic = {\"loss_predictor_total\": self.trackers['loss_tracker_predictor'].result(), # loss_tracker_predictor.result(), \n#                    \"acc_predictor\": self.trackers['acc_metric_predictor'].result(), # acc_metric_predictor.result(), \n#                    \"loss_VAE\":  self.trackers['loss_tracker_generator'].result(), # loss_tracker_generator.result(),\n#                    \"loss_disc\": self.trackers['loss_tracker_disc'].result(), # loss_tracker_disc.result(),\n#                    \"acc_disc\": self.trackers['acc_tracker_disc'].result()} # acc_tracker_disc.result()}\n        \n#         for i in range(hyperparameters['num_tasks']):\n#             ret_dic[\"loss_predictor_\"+str(i)] = self.trackers['individual_loss_tracker_predictor'][i].result() # individual_loss_tracker_predictor[i].result()\n#         for i in range(hyperparameters['num_tasks']):\n#             ret_dic[\"acc_predictor_\"+str(i)] = self.trackers['individual_acc_metric_predictor'][i].result() # individual_acc_metric_predictor[i].result()\n                    \n# #         ret_dic = {\"loss_predictor_total\": loss_tracker_predictor.result(), \n# #                    \"acc_predictor\": acc_metric_predictor.result(), \n# #                    \"loss_VAE\": loss_tracker_generator.result(),\n# #                    \"loss_disc\": loss_tracker_disc.result(),\n# #                    \"acc_disc\": acc_tracker_disc.result()}\n        \n# #         for i in range(hyperparameters['num_tasks']):\n# #             ret_dic[\"loss_predictor_\"+str(i)] = individual_loss_tracker_predictor[i].result()\n# #         for i in range(hyperparameters['num_tasks']):\n# #             ret_dic[\"acc_predictor_\"+str(i)] = individual_acc_metric_predictor[i].result()\n            \n#         return ret_dic        \n    \n#     def predict_step(self, real_images):\n#         unlabelled_x, unlabelled_y = real_images\n        \n#         # Predictor step\n#         unlabelled_prediction_y, unlabelled_predictor_latent, joint_weights = self.predictor(unlabelled_x, training=False)\n        \n#         # Generator step\n#         unlabelled_vae_y, unlabelled_vae_latent = self.generator(unlabelled_x, training=False)\n        \n#         # Discriminator step\n#         unlabelled_predictor_latent = math_ops.mean(ops.convert_to_tensor(unlabelled_predictor_latent), axis=0)\n#         unlabelled_disc_in = tf.concat([unlabelled_vae_latent,unlabelled_predictor_latent],axis=1)\n        \n#         unlabelled_disc_y = self.discriminator(unlabelled_disc_in,training=False)\n        \n#         return unlabelled_prediction_y, unlabelled_disc_y, unlabelled_y, joint_weights\n\n#     @property\n#     def metrics(self):\n#         # We list our `Metric` objects here so that `reset_states()` can be\n#         # called automatically at the start of each epoch\n#         # or at the start of `evaluate()`.\n#         # If you don't implement this property, you have to call\n#         # `reset_states()` yourself at the time of your choosing.\n#         return [self.trackers[\"loss_tracker_predictor\"], self.trackers[\"acc_metric_predictor\"], self.trackers[\"loss_tracker_generator\"], self.trackers[\"loss_tracker_disc\"], self.trackers[\"acc_tracker_disc\"]] + self.trackers[\"individual_loss_tracker_predictor\"] + self.trackers[\"individual_acc_metric_predictor\"]\n","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.296196Z","iopub.execute_input":"2022-03-27T08:38:29.296424Z","iopub.status.idle":"2022-03-27T08:38:29.31443Z","shell.execute_reply.started":"2022-03-27T08:38:29.296398Z","shell.execute_reply":"2022-03-27T08:38:29.313711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def uncertainity(probs, weights):\n#     lis = []\n#     lis_output = []\n#     for i in range(hyperparameters['num_tasks']):\n#         attr_output = probs[i]\n#         w = weights[:,i]\n#         k = -1* np.sum(attr_output*np.log(attr_output),axis=1)\n#         lis_output.append(k)\n#         lis.append(w*k)\n    \n#     variance = np.var(np.array(lis),axis=0)\n#     return np.array(lis).sum(axis=0), variance","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.316396Z","iopub.execute_input":"2022-03-27T08:38:29.31729Z","iopub.status.idle":"2022-03-27T08:38:29.321591Z","shell.execute_reply.started":"2022-03-27T08:38:29.31725Z","shell.execute_reply":"2022-03-27T08:38:29.320801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def getIndices(output, hyperparameters ,pretrain=False):\n#     if pretrain == True:\n#         count =  hyperparameters['train_initial_batches']*hyperparameters['batch_size']\n#         if ((output<=0.5).sum())>=count:\n#             sort = np.argwhere(output<=0.5)[:,0]\n#             return sort\n#         else:\n#             selection = (int((hyperparameters['train_initial_batches']*hyperparameters['batch_size'])/1000)+1)*1000\n#             sort = np.argpartition((output)[:,0], selection)\n#             return sort[:selection]\n#     else:\n#         count = hyperparameters['num_uncertain_elements']\n#         if ((output<=0.5).sum())>=count:\n#             sort = np.argwhere(output<=0.5)[:,0]\n#             return sort\n#         else:\n#             selection = (int(hyperparameters['num_uncertain_elements']/1000)+1)*1000\n#             sort = np.argpartition((output)[:,0], selection)\n#             return sort[:selection]","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.323235Z","iopub.execute_input":"2022-03-27T08:38:29.324009Z","iopub.status.idle":"2022-03-27T08:38:29.331917Z","shell.execute_reply.started":"2022-03-27T08:38:29.323971Z","shell.execute_reply":"2022-03-27T08:38:29.33101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def divide_data(train):\n    \n#     num_samples = train.values.shape[0]\n#     idx = random.sample(list(np.arange(num_samples)), ((int(hyperparameters['initial_percent']*num_samples)//hyperparameters['batch_size'])*hyperparameters['batch_size']))\n    \n#     return pd.DataFrame(train.values[idx,:], columns=train.columns), idx\n","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.333239Z","iopub.execute_input":"2022-03-27T08:38:29.334834Z","iopub.status.idle":"2022-03-27T08:38:29.34097Z","shell.execute_reply.started":"2022-03-27T08:38:29.334796Z","shell.execute_reply":"2022-03-27T08:38:29.340107Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def uncertainity_v2(probs, weights):\n#     lis = []\n#     for i in range(hyperparameters['num_tasks']):\n#         attr_output = probs[i]\n#         w = weights[:,i]\n#         k = -1* np.sum(attr_output*np.log(attr_output),axis=1)\n#         lis.append(w*k)\n                \n#     return np.array(lis).sum(axis=0) # np.mean(np.array(lis), axis=0)\n\n# def uncertainity_v1(probs):\n#     lis = []\n#     for i in range(hyperparameters['num_tasks']):\n#         attr_output = probs[i]\n#         lis.append(-1* np.sum(attr_output*np.log(attr_output),axis=1))\n#     returnnp.mean(np.array(lis), axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.343297Z","iopub.execute_input":"2022-03-27T08:38:29.344Z","iopub.status.idle":"2022-03-27T08:38:29.350448Z","shell.execute_reply.started":"2022-03-27T08:38:29.343964Z","shell.execute_reply":"2022-03-27T08:38:29.349435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# attr = pd.read_csv('../input/celeba-dataset/list_attr_celeba.csv')\n# eval_partition = pd.read_csv('../input/celeba-dataset/list_eval_partition.csv')","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.354288Z","iopub.execute_input":"2022-03-27T08:38:29.354965Z","iopub.status.idle":"2022-03-27T08:38:29.361403Z","shell.execute_reply.started":"2022-03-27T08:38:29.35493Z","shell.execute_reply":"2022-03-27T08:38:29.360629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# break_point_ep = {'3': 5e-4,'6': 5e-4,'10': 1e-5}\n# splits = [0.6,0.65,0.7,0.75,0.8]\n\n# # defining metrics\n\n# trackers = {\n#     \"loss_tracker_predictor\": tf.keras.metrics.CategoricalCrossentropy(name=\"loss_predictor_total\"),\n#     \"acc_metric_predictor\": tf.keras.metrics.CategoricalAccuracy(name=\"acc_predictor\"),\n#     \"individual_loss_tracker_predictor\": [tf.keras.metrics.CategoricalCrossentropy(name=\"loss_predictor_\"+str(i)) for i in range(hyperparameters['num_tasks'])],\n#     \"individual_acc_metric_predictor\": [tf.keras.metrics.CategoricalAccuracy(name=\"acc_predictor_\"+str(i)) for i in range(hyperparameters['num_tasks'])],\n#     \"loss_tracker_generator\": tf.keras.metrics.MeanSquaredError(name='loss_VAE'),\n#     \"loss_tracker_disc\":  tf.keras.metrics.BinaryCrossentropy(name='loss_disc'),\n#     \"acc_tracker_disc\": tf.keras.metrics.BinaryAccuracy(\"acc_disc\")\n# }","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.370946Z","iopub.execute_input":"2022-03-27T08:38:29.371146Z","iopub.status.idle":"2022-03-27T08:38:29.374913Z","shell.execute_reply.started":"2022-03-27T08:38:29.371123Z","shell.execute_reply":"2022-03-27T08:38:29.374157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class CalculatingPredictions(tf.keras.callbacks.Callback):\n#     def __init__(self, preds, test_gen, train_gen, lr, is_validation=False):\n#         self.preds = preds\n#         self.train_gen = train_gen\n#         self.test_gen = test_gen\n#         self.lr = lr\n#         self.is_validation=is_validation\n\n#     def on_epoch_end(self, epoch, logs=None):\n#         model_name = \"AL_model\"\n#         method_name = 'random_sampling'\n#         dataset_name = 'celeba'\n#         attr_grp = 'mouth'\n#         attempt = '1'\n        \n#         file_name = \"_\".join([model_name,method_name,dataset_name,attr_grp,attempt])\n\n#         predict=self.model.evaluate(self.test_gen)\n#         print(predict)\n#         self.preds.append(predict)\n#         k = np.array(self.preds)\n#         if (self.is_validation==True):\n#             np.save(\"./saved_history/\" + file_name + \"_validation_epoch_\"+ str(epoch)+ \".npy\", k)\n#         else:\n#             np.save(\"./saved_history/\" + file_name + \"_training_\" + str(epoch)+ \".npy\", k)\n        \n#         if (self.is_validation==False and epoch%2==0):\n#             self.model.predictor.save_weights(\"./saved_history/models/pred_model_\" + \"_\".join([method_name,dataset_name,attr_grp,attempt]) + \"_epoch_\" + str(epoch) + \".h5\")\n#             self.model.discriminator.save_weights(\"./saved_history/models/disc_model_\" + \"_\".join([method_name,dataset_name,attr_grp,attempt]) + \"_epoch_\" + str(epoch) + \".h5\")\n#             self.model.generator.save_weights(\"./saved_history/models/vae_model_\" + \"_\".join([method_name,dataset_name,attr_grp,attempt]) + \"_epoch_\" + str(epoch) + \".h5\")\n","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.390692Z","iopub.execute_input":"2022-03-27T08:38:29.391173Z","iopub.status.idle":"2022-03-27T08:38:29.397514Z","shell.execute_reply.started":"2022-03-27T08:38:29.391142Z","shell.execute_reply":"2022-03-27T08:38:29.396619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !mkdir logs\n# !mkdir saved_history\n# !mkdir saved_history/models","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.407253Z","iopub.execute_input":"2022-03-27T08:38:29.407736Z","iopub.status.idle":"2022-03-27T08:38:29.411753Z","shell.execute_reply.started":"2022-03-27T08:38:29.40769Z","shell.execute_reply":"2022-03-27T08:38:29.410761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# preds=[] ####\n# validation_train_history=[] # new\n# date = datetime.datetime.now().strftime(\"%d - %b - %y - %H:%M:%S\")\n\n# filename = \"AL_model_from_scratch_non_cs_\"+date ###\n# logdir = \"./logs/\" + filename\n\n# filepath = \"./saved_history/model/AL_model_from_scratch_non_cs\"\n# checkpoint = ModelCheckpoint(filepath, verbose=1, save_best_only=False, mode='max', period = 1)\n# csv_logger = CSVLogger('./saved_history/training_results_AL_model_from_scratch_non_cs.csv', separator = ',', append=True)\n# pre_train_logger = CSVLogger('./saved_history/pre_training_results_AL_model_from_scratch_non_cs.csv', separator = ',', append=True)\n# tensorboard_callback = TensorBoard(log_dir = logdir)\n\n# pre_tensorboard_callback = TensorBoard(log_dir =\" ./logs/pre_\"+filename)\n\n# # Instantiate components\n# # defining my predictor\n# pred_model = predictor()\n# pred_model.compile(optimizer = keras.optimizers.SGD(learning_rate=hyperparameters['lr'],\n#                                                     clipnorm=1.0 ))\n# # if load_pred_model: #####\n# #     print(\"Predictor weights loading ...\")\n# #     pred_model.load_weights(\"./saved_history/models/pred_model_from_scratch_vanilla_4.h5\", by_name=True)\n\n# # defining my discriminator\n# disc_in, disc_out = discriminator()\n# disc = Model(inputs = disc_in, outputs = disc_out)\n# disc.compile(optimizer = keras.optimizers.SGD(learning_rate=hyperparameters['lr'],\n#                                               clipnorm=1.0 ))\n# # if load_disc: ###\n# #     print(\"Discriminator weights loading ...\")\n# #     disc.load_weights(\"./saved_history/models/disc_model_from_scratch_vanilla_4.h5\", by_name=True)\n\n# # defining my generator\n# X, z, mu = variationalAutoEncoder()\n# vae = Model(inputs = X, outputs = [z,mu])\n# vae.compile(optimizer = keras.optimizers.RMSprop(learning_rate=hyperparameters['lr'],\n#                                                  clipnorm=1.0 ))\n# # if load_vae: ####\n# #     print(\"VAE weights loading ...\")\n# #     vae.load_weights(\"./saved_history/models/vae_model_from_scratch_vanilla_4.h5\" ,by_name = True)\n\n# # Instantiate AL model\n# AL_model = ActiveLearning(discriminator=disc, generator=vae, predictor=pred_model, trackers = trackers, alpha=1)\n# AL_model.compile(\n#     d_optimizer=keras.optimizers.SGD(learning_rate=hyperparameters['lr'],clipnorm=1.0 ),\n#     g_optimizer= keras.optimizers.RMSprop(learning_rate=hyperparameters['lr'],clipnorm=1.0),\n#     p_optimizer=keras.optimizers.SGD(learning_rate=hyperparameters['lr'],clipnorm=1.0 ))","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.430586Z","iopub.execute_input":"2022-03-27T08:38:29.430902Z","iopub.status.idle":"2022-03-27T08:38:29.435909Z","shell.execute_reply.started":"2022-03-27T08:38:29.430873Z","shell.execute_reply":"2022-03-27T08:38:29.434934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def startTraining(trackers, splits, break_point_ep,  validation_first = False, load_pred_model = False, load_vae = False, load_disc = False, further_training=False):\n#     preds=[] ####\n#     validation_train_history=[] # new\n#     date = datetime.datetime.now().strftime(\"%d - %b - %y - %H:%M:%S\")\n    \n#     model_name = \"AL_model\"\n#     method_name = 'random_sampling'\n#     dataset_name = 'celeba'\n#     attr_grp = 'mouth'\n#     attempt = '1'\n#     file_name = \"_\".join([model_name,method_name,dataset_name,attr_grp,attempt])\n    \n#     csv_logger = CSVLogger('./saved_history/'+'training_results_'+file_name+'.csv', separator = ',', append=True)\n#     pre_train_logger = CSVLogger('./saved_history/'+'pretraining_results_'+file_name+'.csv', separator = ',', append=True)\n#     tensorboard_callback = TensorBoard(log_dir = (\"./logs/\" + file_name))\n#     pre_tensorboard_callback = TensorBoard(log_dir = (\"./logs/\" + \"pre_\" + file_name))\n\n#     # Instantiate components\n#     # defining my predictor\n#     pred_model = predictor()\n#     pred_model.compile(optimizer = keras.optimizers.SGD(learning_rate=hyperparameters['lr'],\n#                                                         clipnorm=1.0 ))\n#     if load_pred_model: #####\n#         print(\"Predictor weights loading ...\")\n#         pred_model.load_weights(\"./saved_history/models/pred_model_from_scratch_vanilla_4.h5\", by_name=True)\n    \n#     # defining my discriminator\n#     disc_in, disc_out = discriminator()\n#     disc = Model(inputs = disc_in, outputs = disc_out)\n#     disc.compile(optimizer = keras.optimizers.SGD(learning_rate=hyperparameters['lr'],\n#                                                   clipnorm=1.0 ))\n#     if load_disc: ###\n#         print(\"Discriminator weights loading ...\")\n#         disc.load_weights(\"./saved_history/models/disc_model_from_scratch_vanilla_4.h5\", by_name=True)\n        \n#     # defining my generator\n#     X, z, mu = variationalAutoEncoder()\n#     vae = Model(inputs = X, outputs = [z,mu])\n#     vae.compile(optimizer = keras.optimizers.RMSprop(learning_rate=hyperparameters['lr'],\n#                                                      clipnorm=1.0 ))\n#     if load_vae: ####\n#         print(\"VAE weights loading ...\")\n#         vae.load_weights(\"./saved_history/models/vae_model_from_scratch_vanilla_4.h5\" ,by_name = True)\n    \n#     # Instantiate AL model\n#     AL_model = ActiveLearning(discriminator=disc, generator=vae, predictor=pred_model, trackers = trackers, alpha=1)\n#     AL_model.compile(\n#         d_optimizer=keras.optimizers.SGD(learning_rate=hyperparameters['lr'],clipnorm=1.0 ),\n#         g_optimizer= keras.optimizers.RMSprop(learning_rate=hyperparameters['lr'],clipnorm=1.0),\n#         p_optimizer=keras.optimizers.SGD(learning_rate=hyperparameters['lr'],clipnorm=1.0 ))\n\n#     train_imggen = ImageDataGenerator(rescale = 1./255)\n#     image_path = './dataset/img_align_celeba/img_align_celeba/' # -----> CHANGE IT\n#     train, val, test = preprocess(hyperparameters, attr, eval_partition)\n#     train_gen_full = load_generator(train, False)\n#     val_gen = load_generator(val)\n#     test_gen = load_generator(test, False)\n\n#     if validation_first == True:\n\n#         labelled_pretrain, idx_prelabelled = divide_data(val)\n#         idx_preunlabelled = list(np.setdiff1d(list(range(val.shape[0])), idx_prelabelled))\n#         unlabelled_pretrain = pd.DataFrame(val.values[idx_preunlabelled,:], columns=val.columns)\n#         pretrain_gen = generate_generator_multiple(generator=train_imggen,\n#                                                dir1=image_path,\n#                                                dir2=image_path,\n#                                                df1 = labelled_pretrain,\n#                                                df2 = unlabelled_pretrain,\n#                                                batch_size=hyperparameters['batch_size'],\n#                                                img_height=hyperparameters['height'],\n#                                                img_width=hyperparameters['width'])\n#         labelled_pretrain_gen  = load_generator(labelled_pretrain, False)\n#         val_history = AL_model.fit(pretrain_gen, epochs = hyperparameters['pretraining_epochs'], steps_per_epoch = 744, callbacks = [CalculatingPredictions(preds, test_gen, labelled_pretrain_gen, 0.01, True) , pre_tensorboard_callback, pre_train_logger], verbose=1)\n#         validation_train_history.append(val_history.history)\n        \n#         with open(\"./saved_history/pretraining_history_list_\"+attempt\"\n                  \n                  \n#                   pretraining_history_list.json\", 'w') as f:\n#             json.dump(validation_train_history, f, indent=2)\n\n#         _, disc_output,_, _= AL_model.predict(train_gen_full, verbose=1)\n#         unlabelled_indices = getIndices(disc_output, hyperparameters, True) # 0-49999\n#         outputs = []\n#         weights = [] ## --\n#         for i in range(hyperparameters['uncertainity_repeat']):\n#             print(\"Done \"+str(i))\n#             p = AL_model.predict(train_gen_full, verbose=1)\n#             predictor_output= p[0]\n#             joint_weights = p[3] ## --\n#             weights.append(np.array(joint_weights)[unlabelled_indices]) ## --\n#             unlabelled_predictions = np.array(predictor_output)[:,unlabelled_indices,:]\n#             outputs.append(unlabelled_predictions)\n        \n#         avg_output = np.mean(np.array(outputs), axis=0)   \n#         avg_weights = np.mean(np.array(weights), axis=0)##--\n\n\n#         t = uncertainity(avg_output, avg_weights)\n#         t = t[0] + (t[1]*AL_model.alpha)\n#         uncertain_indices = np.argpartition(t, -1*(hyperparameters['train_initial_batches']*hyperparameters['batch_size']))[-1*(hyperparameters['train_initial_batches']*hyperparameters['batch_size']):]\n#         img_indice = np.array(unlabelled_indices)[uncertain_indices]        \n        \n#         k =  np.array(list(range(train.shape[0])))[img_indice]\n#         np.save(\"./saved_history/pretrained_indices.npy\", k)\n#         idx_labelled = list(k)\n        \n#         labelled_train = pd.DataFrame(train.values[idx_labelled,:], columns=train.columns)\n#         idx_unlabelled = list(np.setdiff1d(list(range(train.shape[0])), idx_labelled))\n        \n#         unlabelled_train = pd.DataFrame(train.values[idx_unlabelled,:], columns=train.columns)\n#         train_gen = generate_generator_multiple(generator=train_imggen,\n#                                                        dir1=image_path,\n#                                                        dir2=image_path,\n#                                                        df1 = labelled_train,\n#                                                        df2 = unlabelled_train, \n#                                                        batch_size=hyperparameters['batch_size'],\n#                                                        img_height=hyperparameters['height'],\n#                                                        img_width=hyperparameters['width'])\n#         unlabelled_gen = load_generator(unlabelled_train, False)\n#         labelled_train_gen = load_generator(labelled_train, False)\n#     elif further_training==True:\n\n#         idx_labelled = np.load(\"./saved_history/idx_labelled_4.npy\")\n#         labelled_train = train.iloc[idx_labelled, :]\n#         idx_unlabelled = list(np.setdiff1d(list(range(train.shape[0])), idx_labelled))\n#         unlabelled_train = pd.DataFrame(train.values[idx_unlabelled,:], columns=train.columns)\n#         train_gen = generate_generator_multiple(generator=train_imggen,\n#                                                dir1=image_path,\n#                                                dir2=image_path,\n#                                                df1 = labelled_train,\n#                                                df2 = unlabelled_train,\n#                                                batch_size=hyperparameters['batch_size'],\n#                                                img_height=hyperparameters['height'],\n#                                                img_width=hyperparameters['width'])\n#         unlabelled_gen = load_generator(unlabelled_train, False)\n#         labelled_train_gen = load_generator(labelled_train, False)\n#     else:\n#         labelled_train, idx_labelled = divide_data(train)\n#         idx_unlabelled = list(np.setdiff1d(list(range(train.shape[0])), idx_labelled))\n#         unlabelled_train = pd.DataFrame(train.values[idx_unlabelled,:], columns=train.columns)        \n#         train_gen = generate_generator_multiple(generator=train_imggen,\n#                                                dir1=image_path,\n#                                                dir2=image_path,\n#                                                df1 = labelled_train,\n#                                                df2 = unlabelled_train, \n#                                                batch_size=hyperparameters['batch_size'],\n#                                                img_height=hyperparameters['height'],\n#                                                img_width=hyperparameters['width'])\n#         unlabelled_gen = load_generator(unlabelled_train, False)    \n#         labelled_train_gen = load_generator(labelled_train, False)\n\n\n\n#     history_list=[]\n\n#     if further_training==True:\n#         num_batches = idx_labelled.shape[0]//hyperparameters['batch_size']\n#         tensorboard_callback = TensorBoard(log_dir = 'AL_model_from_scratch_non_cs_09 - Jan - 21 - 12:50:36')\n\n#         iteration =5\n#         epoch_num = 13\n\n#         history = AL_model.fit(train_gen,initial_epoch = epoch_num, epochs=epoch_num+7, steps_per_epoch = num_batches, validation_data=val_gen,callbacks = [CalculatingPredictions(preds, test_gen,labelled_train_gen, 0.01), csv_logger], verbose = 1)\n#         history_list.append(history.history)\n#         pred_model.save_weights(\"./saved_history/models/pred_model_from_scratch_vanilla_\"+str(iteration)+'.h5')\n#         disc.save_weights(\"./saved_history/models/disc_model_from_scratch_vanilla_\"+str(iteration)+'.h5')\n#         vae.save_weights(\"./saved_history/models/vae_model_from_scratch_vanilla_\"+str(iteration)+'.h5')\n#         with open(\"./saved_history/history_list_only_\"+str(iteration)+\".json\", 'w') as f:\n#             json.dump(history_list, f, indent=2)\n\n#         with open(\"./saved_history/preds_only_\"+str(iteration)+\".json\", 'w') as f:\n#             json.dump(preds, f, indent=2)\n#     else:\n#         test_predictions=[]\n#         indices_list = []\n#         epoch_num = 0\n#         num_batches=6103\n\n#         for iteration in range(len(splits)):\n#             print(iteration)\n\n#             if iteration==0:\n#                 # Initial training ---- change\n#                 history = AL_model.fit(train_gen, epochs=hyperparameters['initial_train_epoch'], steps_per_epoch = num_batches, validation_data=val_gen,callbacks = [CalculatingPredictions(preds, test_gen, labelled_train_gen, 0.01), csv_logger, tensorboard_callback], verbose = 1)\n#                 history_list.append(history.history)\n#                 epoch_num+=hyperparameters['initial_train_epoch']\n#             else:\n#                 # Increment training --- change\n#                 history = AL_model.fit(train_gen, initial_epoch = epoch_num, epochs=epoch_num+hyperparameters['increment_train_epoch'], steps_per_epoch = num_batches, validation_data=val_gen,callbacks = [CalculatingPredictions(preds, test_gen, labelled_train_gen, 0.01), csv_logger, tensorboard_callback], verbose = 1)\n#                 history_list.append(history.history)\n#                 epoch_num+=hyperparameters['increment_train_epoch']\n\n#             pred_model.save_weights(\"./saved_history/models/pred_model_from_scratch_vanilla_\"+str(iteration)+'.h5')\n#             disc.save_weights(\"./saved_history/models/disc_model_from_scratch_vanilla_\"+str(iteration)+'.h5')\n#             vae.save_weights(\"./saved_history/models/vae_model_from_scratch_vanilla_\"+str(iteration)+'.h5')\n\n#             # append indices\n#             indices_list.append(idx_labelled)\n#             num_batches+=313\n\n#             with open(\"./saved_history/history_list_\"+str(iteration)+\".json\", 'w') as f:\n#                 json.dump(history_list, f, indent=2)\n\n#             with open(\"./saved_history/preds_\"+str(iteration)+\".json\", 'w') as f:\n#                 json.dump(preds, f, indent=2)\n\n#             np.save(\"./saved_history/idx_labelled_\"+str(iteration)+\".npy\",np.array(idx_labelled))\n\n#             if (iteration!=(len(splits)-1)): # last iteration\n#                 print(\"checking for uncertainities\")\n#                 # Get uncertainities\n\n#                 _, disc_output,_ ,_= AL_model.predict(unlabelled_gen, verbose=1)\n\n#                 unlabelled_indices = getIndices(disc_output, hyperparameters) # 0-49999\n#                 outputs = []\n#                 weights = []\n#                 for i in range(hyperparameters['uncertainity_repeat']):\n#                     print(\"Done \"+str(i))\n#                     p = AL_model.predict(unlabelled_gen, verbose=1)\n#                     predictor_output= p[0]\n#                     joint_weights = p[3]\n#                     weights.append(np.array(joint_weights)[unlabelled_indices])\n#                     unlabelled_predictions = np.array(predictor_output)[:,unlabelled_indices,:]\n#                     outputs.append(unlabelled_predictions)\n\n#                 avg_output = np.mean(np.array(outputs), axis=0)\n#                 avg_weights = np.mean(np.array(weights), axis=0)    \n\n\n\n#                 t = uncertainity(avg_output, avg_weights)\n#                 t=t[0] + (t[1] * AL_model.alpha)\n#                 uncertain_indices = np.argpartition(t, -1*hyperparameters['num_uncertain_elements'])[-1*hyperparameters['num_uncertain_elements']:]\n#                 img_indice = np.array(unlabelled_indices)[uncertain_indices]\n\n#                 k = np.array(idx_unlabelled)[img_indice]\n#                 idx_labelled = idx_labelled+list(k)\n#                 labelled_train = pd.DataFrame(train.values[idx_labelled,:], columns=train.columns)\n#                 idx_unlabelled = list(np.setdiff1d(idx_unlabelled, k))\n#                 unlabelled_train = pd.DataFrame(train.values[idx_unlabelled,:], columns=train.columns)\n#                 train_gen = generate_generator_multiple(generator=train_imggen,\n#                                                                dir1=image_path,\n#                                                                dir2=image_path,\n#                                                                df1 = labelled_train,\n#                                                                df2 = unlabelled_train,\n#                                                                batch_size=hyperparameters['batch_size'],\n#                                                                img_height=hyperparameters['height'],\n#                                                                img_width=hyperparameters['width'])\n#                 unlabelled_gen = load_generator(unlabelled_train, False)\n#                 labelled_train_gen = load_generator(labelled_train, False)\n    \n    \n#     return history_list, pred_model, vae, disc, AL_model, indices_list, preds","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.480137Z","iopub.execute_input":"2022-03-27T08:38:29.480351Z","iopub.status.idle":"2022-03-27T08:38:29.535174Z","shell.execute_reply.started":"2022-03-27T08:38:29.480327Z","shell.execute_reply":"2022-03-27T08:38:29.534394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# history_list, pred_model, vae, disc, Al_model, indices_list, preds = startTraining(trackers, splits, break_point_ep, True, False, False, False, False)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.53825Z","iopub.execute_input":"2022-03-27T08:38:29.538951Z","iopub.status.idle":"2022-03-27T08:38:29.543601Z","shell.execute_reply.started":"2022-03-27T08:38:29.538903Z","shell.execute_reply":"2022-03-27T08:38:29.541894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Active Learning Ours Random Sampling CelebA","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport random as random\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Dense, Dot, Multiply, Softmax, Reshape, Input, Flatten, Conv2DTranspose, Conv2D, GlobalAveragePooling2D,BatchNormalization, Lambda, MaxPooling2D, ReLU, Dropout, Concatenate\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.models import Model, model_from_json\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.utils import plot_model, to_categorical\n\nfrom tensorflow.keras.callbacks import ModelCheckpoint, LambdaCallback, ReduceLROnPlateau, CSVLogger, TensorBoard\nfrom tensorflow.keras import backend as K\n\nimport datetime\nfrom tensorflow.python.framework import ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.keras import backend as K\nimport tensorflow.keras as keras\nimport json","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.544887Z","iopub.execute_input":"2022-03-27T08:38:29.54544Z","iopub.status.idle":"2022-03-27T08:38:29.555251Z","shell.execute_reply.started":"2022-03-27T08:38:29.545385Z","shell.execute_reply":"2022-03-27T08:38:29.554501Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Mouth       4 'Big_Lips', 'Mouth_Slightly_Open', 'Smiling', 'Wearing_Lipstick'\n#Eyes        5 'Arched_Eyebrows', 'Bag_Under_Eyes', 'Bushy_Eyebrows', 'Eyeglasses', 'Narrow_Eyes'\n#Face        6 'Attractive', 'Blurry', 'Heavy_Makeup', 'Oval_Face', 'Pale_Skin', 'Young'\n#Facial Hair 5 '5_o_clock shadow', 'Goatee', 'Moustache', 'No_Beard', 'Sideburns'\n#Head       11 'Bald', 'Bangs', 'Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair', 'Receding_Hairline', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat'","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.556366Z","iopub.execute_input":"2022-03-27T08:38:29.557798Z","iopub.status.idle":"2022-03-27T08:38:29.56646Z","shell.execute_reply.started":"2022-03-27T08:38:29.557755Z","shell.execute_reply":"2022-03-27T08:38:29.565705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hyperparameters={\n    'targets': ['image_id', 'Big_Lips', 'Mouth_Slightly_Open', 'Smiling', 'Wearing_Lipstick'],  ### MODIFY \n    'height': 224,\n    'width': 192 ,\n    'channels': 3,\n    'batch_size': 8,\n    'epochs': 1,\n    'num_tasks': 4, ### MODIFY\n    'initializer': 'he_uniform',\n    'reg_lambda': 1e-3,\n    'output': [2]*4, ### MODIFY === [2]*number of tasks\n    'lr': 5e-5, ### MODIFY\n    'is_trained': False,\n    'dropout_prob': 0.3,\n    'enable_cs': False,\n    'enable_sluice': False,\n    'initial_percent':0.1,\n    'initial_train_epoch': 2, ### NEED TO SEE----- Done\n    'increment_train_epoch':2, ### NEED TO SEE --- Done\n    'uncertainity_repeat': 5,\n    'num_uncertain_elements': 5008,\n    'additional_epoch': 5, ### NEED TO SEE\n    'pretraining_epochs': 3, ### -- make it 2 or originally 1 - Done\n    'train_initial_batches': 8138, #6103, ### NEED TO SEE ------------- Done\n    'enable_additional': False,\n    'additional_attr_count':2,\n    'all_updates':False,\n    'initial_percent_val': 0.6 ### Added Now\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.568023Z","iopub.execute_input":"2022-03-27T08:38:29.568661Z","iopub.status.idle":"2022-03-27T08:38:29.577071Z","shell.execute_reply.started":"2022-03-27T08:38:29.568625Z","shell.execute_reply":"2022-03-27T08:38:29.576046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocess(hyperparameters, attr, eval_partition):\n    attr = attr[hyperparameters['targets']]\n    attr = attr.replace(-1, 0)\n    attr = attr.set_index('image_id')\n    eval_partition = eval_partition.set_index('image_id')\n    attr = attr.join(eval_partition)\n    attr['image_id'] = attr.index\n\n    for column in attr.columns[:-2]:\n        k = to_categorical(attr[column])\n        attr = attr.drop(column, axis=1)\n        attr[column] = k.tolist()\n\n    train = attr.loc[attr['partition']==0]\n    val = attr.loc[attr['partition']==1]\n    test = attr.loc[attr['partition']==2]\n\n    train = train.drop('partition', axis=1)\n    val = val.drop('partition', axis=1)\n    test = test.drop('partition', axis=1)\n\n    train = train[:(len(train)//hyperparameters['batch_size'])*hyperparameters['batch_size']]\n    val = val[:(len(val)//hyperparameters['batch_size'])*hyperparameters['batch_size']]\n    test = test[:(len(test)//hyperparameters['batch_size'])*hyperparameters['batch_size']]\n\n    return (train, val, test)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.5785Z","iopub.execute_input":"2022-03-27T08:38:29.578903Z","iopub.status.idle":"2022-03-27T08:38:29.589749Z","shell.execute_reply.started":"2022-03-27T08:38:29.578867Z","shell.execute_reply":"2022-03-27T08:38:29.588866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_generator(df, shuffle=True):\n\n    # image_id = '000014.jpg'\n    image_path = '../input/celeba-dataset/img_align_celeba/img_align_celeba'\n    data_gen = ImageDataGenerator(rescale=1/255.0)\n\n    generator = data_gen.flow_from_dataframe(dataframe = df,\n                                     directory=image_path,\n                                     x_col = 'image_id',\n                                     y_col=hyperparameters['targets'][1:],\n                                     class_mode = 'multi_output',\n                                     target_size=(hyperparameters['height'], hyperparameters['width']),\n                                     batch_size = hyperparameters['batch_size'],shuffle=shuffle)\n\n    return generator","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.591903Z","iopub.execute_input":"2022-03-27T08:38:29.592971Z","iopub.status.idle":"2022-03-27T08:38:29.601075Z","shell.execute_reply.started":"2022-03-27T08:38:29.592922Z","shell.execute_reply":"2022-03-27T08:38:29.600213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_generator_multiple(generator,dir1, dir2, df1, df2, batch_size, img_height,img_width,shuffle=True):\n    # labelled \n    genX1 = generator.flow_from_dataframe(df1, dir1,\n                                          x_col = 'image_id',\n                                          y_col=hyperparameters['targets'][1:],\n                                          class_mode = 'multi_output',\n                                          target_size=(img_height,img_width),\n                                          batch_size = hyperparameters['batch_size'],\n                                          shuffle=shuffle)\n    # train --- make it train generator\n    genX2 = generator.flow_from_dataframe(df2, dir2,\n                                          x_col = 'image_id',\n                                          y_col=hyperparameters['targets'][1:],\n                                          class_mode = 'multi_output',\n                                          target_size=(img_height, img_width),\n                                          batch_size = batch_size,\n                                          shuffle=shuffle)\n    while True:\n            X1,y1 = genX1.next()\n            X2,_ = genX2.next()\n            yield X1, X2, y1  #Yield both images and their mutual label","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.604209Z","iopub.execute_input":"2022-03-27T08:38:29.604953Z","iopub.status.idle":"2022-03-27T08:38:29.614486Z","shell.execute_reply.started":"2022-03-27T08:38:29.604908Z","shell.execute_reply":"2022-03-27T08:38:29.613581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# changed input to 360+360\ndef discriminator():\n\n    X = Input((720,), name='input_disc')\n\n    x = Dense(units = 512, activation = None, name='dense_1_disc')(X)\n    x = BatchNormalization(name = 'bn_1_disc')(x)\n    x = ReLU(name='relu_1_disc')(x)\n    x = Dense(units = 512, activation = None, name='dense_2_disc')(x)\n    x = BatchNormalization(name = 'bn_2_disc')(x)\n    x = ReLU(name='relu_2_disc')(x)\n    x = Dense(units = 1, activation = 'sigmoid', name='output_disc')(x)\n\n    return X,x","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.615837Z","iopub.execute_input":"2022-03-27T08:38:29.616549Z","iopub.status.idle":"2022-03-27T08:38:29.625399Z","shell.execute_reply.started":"2022-03-27T08:38:29.616511Z","shell.execute_reply":"2022-03-27T08:38:29.62442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def addConvBlock(num_filters, kernel_size, hyperparameters, pool_size, tops, stride, pad, pool_stride, isPool , i):\n\n    for task_id in range(hyperparameters['num_tasks']+hyperparameters['enable_additional']*hyperparameters['additional_attr_count']):\n\n        tops[task_id] = Conv2D(num_filters, kernel_size=kernel_size, name = 'conv'+str(i)+'_'+str(task_id),  strides=(stride, stride), padding = pad, kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l1(hyperparameters['reg_lambda']))(tops[task_id])\n        tops[task_id] = ReLU(name='relu'+str(i)+'_'+str(task_id))(tops[task_id])\n\n        if (isPool==True):\n            name = 'pool'+str(i)+'_'+str(task_id)\n            tops[task_id] = MaxPooling2D(pool_size=(pool_size, pool_size), name = name, strides = (pool_stride, pool_stride), padding='valid')(tops[task_id])\n\n    return tops","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.627347Z","iopub.execute_input":"2022-03-27T08:38:29.628136Z","iopub.status.idle":"2022-03-27T08:38:29.639307Z","shell.execute_reply.started":"2022-03-27T08:38:29.628096Z","shell.execute_reply":"2022-03-27T08:38:29.638317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def combined_model():\n\n    num_tasks = hyperparameters['num_tasks']+hyperparameters['enable_additional']*hyperparameters['additional_attr_count']\n\n    x,y,z = hyperparameters['height'], hyperparameters['width'], hyperparameters['channels']\n    X = Input((x,y,z), name = 'input_predictor_'+\"_\".join(str(num) for num in range(num_tasks)))\n    tops = [X]*num_tasks\n\n# ------------------------------------------- Block 1 BEGINS ------------------------------------\n\n    tops = addConvBlock(40, 5, hyperparameters, 3, tops, 1, 'same', 2, True, 1)\n    if hyperparameters[\"enable_cs\"]:\n        cs1 = CrossStitch(num_tasks,1, hyperparameters, True)(tops)\n        tops = tf.unstack(cs1, axis=0)\n\n    tops = addConvBlock(60, 5, hyperparameters, 3, tops, 1, 'same', 2, True, 2)\n    if hyperparameters[\"enable_cs\"]:\n        cs2 = CrossStitch(num_tasks, 2, hyperparameters, True)(tops)\n        tops = tf.unstack(cs2, axis=0)\n\n    tops = addConvBlock(80, 3, hyperparameters, 3, tops, 1, 'same', 2, True, 3)\n    if hyperparameters[\"enable_cs\"]:\n        cs3 = CrossStitch(num_tasks, 3, hyperparameters, True)(tops)\n        tops = tf.unstack(cs3, axis=0)\n\n\n    tops = addConvBlock(100, 3, hyperparameters, 3, tops, 1, 'same', 2, True,  4)\n    if hyperparameters[\"enable_cs\"]:\n        cs4 = CrossStitch(num_tasks, 4, hyperparameters, True)(tops)\n        tops = tf.unstack(cs4, axis=0)\n# ------------------------------------------- Block 4 ENDS ------------------------------------\n\n# ------------------------------------------- Block 5 BEGINS ------------------------------------\n    tops = addConvBlock(140, 2, hyperparameters, 3, tops, 1, 'same', 2, True, 5)\n    if hyperparameters[\"enable_cs\"]:\n        cs5 = CrossStitch(num_tasks, 5, hyperparameters, True)(tops)\n        tops = tf.unstack(cs5, axis=0)\n\n# ------------------------------------------- Block 5 ENDS ------------------------------------\n\n    latents=[]\n    for task_id in range(num_tasks):\n\n        tops[task_id] = Flatten(name = 'flat'+'_'+str(task_id))(tops[task_id])\n\n        tops[task_id] = Dense(units = 720, name = 'dense0'+'_'+str(task_id), kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(tops[task_id])\n        if (task_id==0):\n            tops[task_id] = ReLU(name='re_lu0'+'_'+str(0))(tops[task_id])\n        else:\n            tops[task_id] = ReLU(name='re_lu0'+'_'+str(task_id)+'_'+str(task_id))(tops[task_id])\n\n\n        tops[task_id] = Dense(units = 360, name = 'dense1'+'_'+str(task_id), kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(tops[task_id])\n        if (task_id==0):\n            tops[task_id] = ReLU(name='re_lu'+'_'+str(0))(tops[task_id])\n        else:\n            tops[task_id] = ReLU(name='re_lu'+'_'+str(task_id)+'_'+str(task_id))(tops[task_id])\n\n        latents.append(tops[task_id])\n        tops[task_id] = Dense(units = 180, name = 'dense2'+'_'+str(task_id), kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(tops[task_id])\n        if (task_id==0):\n            tops[task_id] = ReLU(name='re_lu2'+'_'+str(0))(tops[task_id])\n        else:\n            tops[task_id] = ReLU(name='re_lu2'+'_'+str(task_id)+'_'+str(task_id))(tops[task_id])\n\n    # added joined weights\n    # joint = Concatenate(name='joined_1')(weights)\n    # joint = Dense(units = 360, name=\"joined_2\",kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(joint)\n    # joint = Dense(units = num_tasks, name=\"joined_3\",activation = 'softmax', kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(joint)\n    # tops = taskEmbeddings(5)([tops, joint])\n\n    for task_id in range(num_tasks):\n        tops[task_id] = Dense(units = 90, name = 'dense3'+'_'+str(task_id), kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(tops[task_id])\n        if (task_id==0):\n            tops[task_id] = ReLU(name='re_lu3'+'_'+str(0))(tops[task_id])\n        else:\n            tops[task_id] = ReLU(name='re_lu3'+'_'+str(task_id)+'_'+str(task_id))(tops[task_id])\n    \n    \n    for task_id in range(num_tasks):\n        tops[task_id] = Dense(units = 2, name='output'+'_'+str(task_id),activation = 'softmax', kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(tops[task_id])\n\n    return X, tops, latents # joints # 1 is dummy for joints","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.641433Z","iopub.execute_input":"2022-03-27T08:38:29.641792Z","iopub.status.idle":"2022-03-27T08:38:29.669834Z","shell.execute_reply.started":"2022-03-27T08:38:29.641751Z","shell.execute_reply":"2022-03-27T08:38:29.668834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def predictor():\n    X, tops, latents = combined_model()\n    model = Model(inputs=X, outputs=[tops,latents])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.672179Z","iopub.execute_input":"2022-03-27T08:38:29.672914Z","iopub.status.idle":"2022-03-27T08:38:29.681036Z","shell.execute_reply.started":"2022-03-27T08:38:29.672873Z","shell.execute_reply":"2022-03-27T08:38:29.680262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def variationalAutoEncoder(embed_size=360):\n    # input\n    X = Input((hyperparameters['height'],hyperparameters['width'],hyperparameters['channels']), name=\"input_vae\")\n\n    # encoder\n    x = Conv2D(128, kernel_size=(4,4), strides = (2,2), padding='same', use_bias=False, name='conv_1_vae')(X)\n    x = BatchNormalization(name='bn_1_vae')(x)\n    x = ReLU(name='relu_1_vae')(x)\n    x = Conv2D(256, kernel_size=(4,4), strides = (2,2), padding='same', use_bias=False, name='conv_2_vae')(x)\n    x = BatchNormalization(name='bn_2_vae')(x)\n    x = ReLU(name='relu_2_vae')(x)\n    x = Conv2D(512, kernel_size=(4,4), strides = (2,2), padding='same', use_bias=False, name='conv_3_vae')(x)\n    x = BatchNormalization(name='bn_3_vae')(x)\n    x = ReLU(name='relu_3_vae')(x)\n    x = Conv2D(1024, kernel_size=(4,4), strides = (2,2), padding='same', use_bias=False, name='conv_4_vae')(x)\n    x = BatchNormalization(name='bn_4_vae')(x)\n    x = ReLU(name='relu_4_vae')(x)\n\n    x = Flatten(name='flatten_1_vae')(x)\n\n    mu = Dense(units = embed_size, activation = None, name='dense_1_vae')(x)\n\n    #logvar = Dense(units = 32, activation = None)(x)\n    # check what to be done here --->  ??\n    #stds = Lambda(lambda x: x * 0.5)(logvar)\n    #stds = tf.keras.backend.exp(stds)\n    #epsilon = tf.keras.backend.random_normal((32,))\n    #m = tf.keras.layers.Multiply()([stds,epsilon])\n    #latents = tf.keras.layers.Add()([m,mu])\n\n    # decoder\n    z = Dense(units = 14*12*1024, activation = None, name='dense_2_vae')(mu)\n    z = Reshape((14,12,1024), name='reshape_vae')(z)\n    z = Conv2DTranspose(512, kernel_size=(4,4), strides = (2,2), padding='same', use_bias=False, name='conv_5_vae')(z)\n    z = BatchNormalization(name='bn_5_vae')(z)\n    z = ReLU(name='relu_5_vae')(z)\n    z = Conv2DTranspose(256, kernel_size=(4,4), strides = (2,2), padding='same', use_bias=False, name='conv_6_vae')(z)\n    z = BatchNormalization(name='bn_6_vae')(z)\n    z = ReLU(name='relu_6_vae')(z)\n    z = Conv2DTranspose(128, kernel_size=(4,4), strides = (2,2), padding='same', use_bias=False, name='conv_7_vae')(z)\n    z = BatchNormalization(name='bn_7_vae')(z)\n    z = ReLU(name='relu_7_vae')(z)\n    z = Conv2DTranspose(3, strides=(2,2), kernel_size=(1,1), name='conv_8_vae')(z)\n\n    return X, z, mu","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.682611Z","iopub.execute_input":"2022-03-27T08:38:29.682958Z","iopub.status.idle":"2022-03-27T08:38:29.701544Z","shell.execute_reply.started":"2022-03-27T08:38:29.682913Z","shell.execute_reply":"2022-03-27T08:38:29.700851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ActiveLearning(keras.Model):\n    def __init__(self, discriminator, generator, predictor, trackers, alpha):\n        super(ActiveLearning, self).__init__()\n        self.discriminator = discriminator\n        self.generator = generator\n        self.predictor = predictor\n        self.trackers = trackers\n        self.alpha=alpha\n        self.prev_loss = 10000\n\n    def compile(self, d_optimizer, g_optimizer, p_optimizer):\n        super(ActiveLearning, self).compile()\n        self.d_optimizer = d_optimizer\n        self.g_optimizer = g_optimizer\n        self.p_optimizer = p_optimizer\n\n    def train_step(self, real_images):\n\n        # get labelled_x, unlabelled_x and labelled_y\n\n        x = real_images\n        labelled_x = x[0]\n        unlabelled_x = x[1]\n        labelled_y = x[2]\n\n        ##### TRAIN THE PREDICTOR #####\n\n        # Compute output and latents\n        with tf.GradientTape() as tape:\n            labelled_prediction_y, _ = self.predictor(labelled_x, training=True)\n            predictor_loss = keras.losses.categorical_crossentropy(labelled_y, labelled_prediction_y) # ----> 1\n\n        # Compute gradients\n        trainable_vars = self.predictor.trainable_variables\n        gradients = tape.gradient(predictor_loss, trainable_vars)\n        \n        # Update weights\n        self.p_optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        # ------------------------------------------------------------------------------------------------\n\n        ##### TRAIN THE GENERATOR #####\n\n        # Create labels for VAE\n        labelled_disc_true = np.ones((hyperparameters['batch_size'],1))\n        unlabelled_disc_fake = np.ones((hyperparameters['batch_size'],1))\n\n        # Compute VAE outputs\n        with tf.GradientTape() as tape:\n            # Compute generator o/p\n            labelled_vae_y, labelled_vae_latent = self.generator(labelled_x)\n            unlabelled_vae_y, unlabelled_vae_latent = self.generator(unlabelled_x)\n\n            # Calculate loss for VAE\n            labelled_vae_loss = keras.losses.mean_squared_error(labelled_x, labelled_vae_y) # ----> 2\n            unlabelled_vae_loss = keras.losses.mean_squared_error(unlabelled_x, unlabelled_vae_y) # ----> 2\n\n            vae_loss = labelled_vae_loss + unlabelled_vae_loss #+ (self.advisory_param*disc_loss)\n\n        # Compute gradients\n        trainable_vars = self.generator.trainable_variables\n        gradients = tape.gradient(vae_loss, trainable_vars)\n\n        # Update weights\n        self.g_optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        # ------------------------------------------------------------------------------------------------\n\n        ##### TRAIN THE DISCRIMINATOR #####\n\n        # Create disc labels\n        labelled_disc_true = np.ones((hyperparameters['batch_size'],1))\n        unlabelled_disc_true = np.zeros((hyperparameters['batch_size'],1))\n\n        # Compute VAE latents\n        _, labelled_vae_latent = self.generator(labelled_x, training = False)\n        _, unlabelled_vae_latent = self.generator(unlabelled_x, training = False)\n\n        # Compute predictor latents\n        _, labelled_predictor_latent = self.predictor(labelled_x, training=False)\n        _, unlabelled_predictor_latent = self.predictor(unlabelled_x, training=False)\n\n\n        # Average out the latents for 5 tasks --- SHOULD I?\n        labelled_predictor_latent = math_ops.mean(ops.convert_to_tensor(labelled_predictor_latent), axis=0)\n        unlabelled_predictor_latent = math_ops.mean(ops.convert_to_tensor(unlabelled_predictor_latent), axis=0)\n\n        # Join vae and predictor latents\n        labelled_disc_in = tf.concat([labelled_vae_latent,labelled_predictor_latent],axis=1)\n        unlabelled_disc_in = tf.concat([unlabelled_vae_latent,unlabelled_predictor_latent],axis=1)\n\n        # Compute disc output\n        with tf.GradientTape() as tape:\n            labelled_disc_y = self.discriminator(labelled_disc_in,training=True)\n            unlabelled_disc_y = self.discriminator(unlabelled_disc_in,training=True)\n\n            labelled_disc_loss = keras.losses.binary_crossentropy(labelled_disc_true, labelled_disc_y) # ----> 3\n            unlabelled_dic_loss = keras.losses.binary_crossentropy(unlabelled_disc_true, unlabelled_disc_y) # ----> 3\n\n            disc_loss = labelled_disc_loss + unlabelled_dic_loss\n\n        # Compute gradients\n        trainable_vars = self.discriminator.trainable_variables\n        gradients = tape.gradient(disc_loss, trainable_vars)\n\n        # Update weights\n        self.d_optimizer.apply_gradients(zip(gradients, trainable_vars))\n\n        # ------------------------------------------------------------------------------------------------\n\n        # Computing Metrics\n\n        # For predictor\n\n        self.trackers['loss_tracker_predictor'].update_state(labelled_y, labelled_prediction_y)\n        self.trackers['acc_metric_predictor'].update_state(labelled_y, labelled_prediction_y)\n\n        for i in range(hyperparameters['num_tasks']):\n            self.trackers['individual_loss_tracker_predictor'][i].update_state(labelled_y[i], labelled_prediction_y[i])\n            self.trackers['individual_acc_metric_predictor'][i].update_state(labelled_y[i], labelled_prediction_y[i])\n\n        # For VAE\n        self.trackers['loss_tracker_generator'].update_state(labelled_x, labelled_vae_y)\n        self.trackers['loss_tracker_generator'].update_state(unlabelled_x, unlabelled_vae_y)\n        # For Discriminator\n        self.trackers['loss_tracker_disc'].update_state(labelled_disc_true,labelled_disc_y)\n        self.trackers['loss_tracker_disc'].update_state(unlabelled_disc_true,unlabelled_disc_y)\n        self.trackers['acc_tracker_disc'].update_state(labelled_disc_true,labelled_disc_y)\n        self.trackers['acc_tracker_disc'].update_state(unlabelled_disc_true,unlabelled_disc_y)\n\n        ret_dic = {\"loss_predictor_total\": self.trackers['loss_tracker_predictor'].result(), # loss_tracker_predictor.result(), \n                   \"acc_predictor\":self.trackers['acc_metric_predictor'].result(), # acc_metric_predictor.result(), \n                   \"loss_VAE\":  self.trackers['loss_tracker_generator'].result(), # loss_tracker_generator.result(),\n                   \"loss_disc\": self.trackers['loss_tracker_disc'].result(), # loss_tracker_disc.result(),\n                   \"acc_disc\": self.trackers['acc_tracker_disc'].result()} # acc_tracker_disc.result()}\n\n        for i in range(hyperparameters['num_tasks']):\n            ret_dic[\"loss_predictor_\"+str(i)] = self.trackers['individual_loss_tracker_predictor'][i].result() # individual_loss_tracker_predictor[i].result()\n        for i in range(hyperparameters['num_tasks']):\n            ret_dic[\"acc_predictor_\"+str(i)] = self.trackers['individual_acc_metric_predictor'][i].result() # individual_acc_metric_predictor[i].result()\n\n        return ret_dic\n\n    def call(self, x):\n        return\n\n    def test_step(self, real_images):\n\n        x = real_images\n        labelled_x = x[0]\n        labelled_y = x[1]\n\n        # Predictor step\n        labelled_prediction_y, labelled_predictor_latent = self.predictor(labelled_x, training=False)\n\n        # Generator step\n        labelled_vae_y, labelled_vae_latent = self.generator(labelled_x, training=False)\n\n        # Discriminator step\n        labelled_predictor_latent = math_ops.mean(ops.convert_to_tensor(labelled_predictor_latent), axis=0)\n        labelled_disc_in = tf.concat([labelled_vae_latent,labelled_predictor_latent],axis=1)\n\n        labelled_disc_y = self.discriminator(labelled_disc_in,training=False)\n\n        # Updating metrics\n        # For Predictor\n        self.trackers['loss_tracker_predictor'].update_state(labelled_y, labelled_prediction_y)\n        self.trackers['acc_metric_predictor'].update_state(labelled_y, labelled_prediction_y)\n\n        for i in range(hyperparameters['num_tasks']):\n            self.trackers['individual_loss_tracker_predictor'][i].update_state(labelled_y[i], labelled_prediction_y[i])\n            self.trackers['individual_acc_metric_predictor'][i].update_state(labelled_y[i], labelled_prediction_y[i])\n\n        self.trackers['loss_tracker_generator'].update_state(labelled_x, labelled_vae_y)\n\n\n        # For Discriminator\n        labelled_disc_true = np.ones((hyperparameters['batch_size'],1))\n        self.trackers['loss_tracker_disc'].update_state(labelled_disc_true,labelled_disc_y)\n        self.trackers['acc_tracker_disc'].update_state(labelled_disc_true,labelled_disc_y)\n\n        ret_dic = {\"loss_predictor_total\": self.trackers['loss_tracker_predictor'].result(), # loss_tracker_predictor.result(), \n                   \"acc_predictor\": self.trackers['acc_metric_predictor'].result(), # acc_metric_predictor.result(), \n                   \"loss_VAE\":  self.trackers['loss_tracker_generator'].result(), # loss_tracker_generator.result(),\n                   \"loss_disc\": self.trackers['loss_tracker_disc'].result(), # loss_tracker_disc.result(),\n                   \"acc_disc\": self.trackers['acc_tracker_disc'].result()} # acc_tracker_disc.result()}\n\n        for i in range(hyperparameters['num_tasks']):\n            ret_dic[\"loss_predictor_\"+str(i)] = self.trackers['individual_loss_tracker_predictor'][i].result() # individual_loss_tracker_predictor[i].result()\n        for i in range(hyperparameters['num_tasks']):\n            ret_dic[\"acc_predictor_\"+str(i)] = self.trackers['individual_acc_metric_predictor'][i].result() # individual_acc_metric_predictor[i].result()\n\n        return ret_dic\n\n    def predict_step(self, real_images):\n        unlabelled_x, unlabelled_y = real_images\n\n        # Predictor step\n        unlabelled_prediction_y, unlabelled_predictor_latent = self.predictor(unlabelled_x, training=False)\n\n        # Generator step\n        unlabelled_vae_y, unlabelled_vae_latent = self.generator(unlabelled_x, training=False)\n        \n        # Discriminator step\n        unlabelled_predictor_latent = math_ops.mean(ops.convert_to_tensor(unlabelled_predictor_latent), axis=0)\n        unlabelled_disc_in = tf.concat([unlabelled_vae_latent,unlabelled_predictor_latent],axis=1)\n\n        unlabelled_disc_y = self.discriminator(unlabelled_disc_in,training=False)\n\n        return unlabelled_prediction_y, unlabelled_disc_y, unlabelled_y, joint_weights\n\n    @property\n    def metrics(self):\n        return [self.trackers[\"loss_tracker_predictor\"], self.trackers[\"acc_metric_predictor\"], self.trackers[\"loss_tracker_generator\"], self.trackers[\"loss_tracker_disc\"], self.trackers[\"acc_tracker_disc\"]] + self.trackers[\"individual_loss_tracker_predictor\"] + self.trackers[\"individual_acc_metric_predictor\"]\n","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.70374Z","iopub.execute_input":"2022-03-27T08:38:29.704275Z","iopub.status.idle":"2022-03-27T08:38:29.748594Z","shell.execute_reply.started":"2022-03-27T08:38:29.704236Z","shell.execute_reply":"2022-03-27T08:38:29.747724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def divide_data(train, initial = False):\n    num_samples = train.values.shape[0]\n\n    if initial:\n        idx = random.sample(list(np.arange(num_samples)), ((int(hyperparameters['initial_percent_val']*num_samples)//hyperparameters['batch_size'])*hyperparameters['batch_size']))\n    else:\n        idx = random.sample(list(np.arange(num_samples)), ((int(hyperparameters['initial_percent']*num_samples)//hyperparameters['batch_size'])*hyperparameters['batch_size']))\n\n    print(len(idx))\n    return pd.DataFrame(train.values[idx,:], columns=train.columns), idx\n\ndef uncertainity(probs, weights):\n    lis = []\n    lis_output = []\n    for i in range(hyperparameters['num_tasks']):\n        attr_output = probs[i]\n        w = weights[:,i]\n        k = -1* np.sum(attr_output*np.log(attr_output),axis=1)\n        lis_output.append(k)\n        lis.append(w*k)\n\n    variance = np.var(np.array(lis),axis=0)\n    return np.array(lis).sum(axis=0), variance\n\ndef getIndices(output, hyperparameters ,pretrain=False):\n    if pretrain == True:\n        count =  hyperparameters['train_initial_batches']*hyperparameters['batch_size']\n        if ((output<=0.5).sum())>=count:\n            sort = np.argwhere(output<=0.5)[:,0]\n            return sort\n        else:\n            selection = (int((hyperparameters['train_initial_batches']*hyperparameters['batch_size'])/1000)+1)*1000\n            sort = np.argpartition((output)[:,0], selection)\n            return sort[:selection]\n    else:\n        count = hyperparameters['num_uncertain_elements']\n        if ((output<=0.5).sum())>=count:\n            sort = np.argwhere(output<=0.5)[:,0]\n            return sort\n        else:\n            selection = (int(hyperparameters['num_uncertain_elements']/1000)+1)*1000\n            sort = np.argpartition((output)[:,0], selection)\n            return sort[:selection]","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.75235Z","iopub.execute_input":"2022-03-27T08:38:29.753285Z","iopub.status.idle":"2022-03-27T08:38:29.769212Z","shell.execute_reply.started":"2022-03-27T08:38:29.753242Z","shell.execute_reply":"2022-03-27T08:38:29.768321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"break_point_ep = {'3': 5e-4,'6': 5e-4,'10': 1e-5}\nsplits = [0.1,0.15,0.2,0.25,0.3,0.35,0.40]\n\n# defining metrics\n\ntrackers = {\n    \"loss_tracker_predictor\": tf.keras.metrics.CategoricalCrossentropy(name=\"loss_predictor_total\"),\n    \"acc_metric_predictor\": tf.keras.metrics.CategoricalAccuracy(name=\"acc_predictor\"),\n    \"individual_loss_tracker_predictor\": [tf.keras.metrics.CategoricalCrossentropy(name=\"loss_predictor_\"+str(i)) for i in range(hyperparameters['num_tasks'])],\n    \"individual_acc_metric_predictor\": [tf.keras.metrics.CategoricalAccuracy(name=\"acc_predictor_\"+str(i)) for i in range(hyperparameters['num_tasks'])],\n    \"loss_tracker_generator\": tf.keras.metrics.MeanSquaredError(name='loss_VAE'),\n    \"loss_tracker_disc\":  tf.keras.metrics.BinaryCrossentropy(name='loss_disc'),\n    \"acc_tracker_disc\": tf.keras.metrics.BinaryAccuracy(\"acc_disc\")\n}","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.772632Z","iopub.execute_input":"2022-03-27T08:38:29.773281Z","iopub.status.idle":"2022-03-27T08:38:29.814107Z","shell.execute_reply.started":"2022-03-27T08:38:29.77325Z","shell.execute_reply":"2022-03-27T08:38:29.813302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CalculatingPredictions(tf.keras.callbacks.Callback):\n    def __init__(self, preds, test_gen, train_gen, lr, model_params, is_validation=False):\n        self.preds = preds\n        self.train_gen = train_gen\n        self.test_gen = test_gen\n        self.lr = lr\n        self.is_validation=is_validation\n        self.model_params = model_params\n\n    def on_epoch_end(self, epoch, logs=None):\n        \n        model_name, method_name, dataset_name, attr_grp, attempt = self.model_params\n        file_name = \"_\".join([model_name,method_name,dataset_name,attr_grp,attempt])\n\n        predict=self.model.evaluate(self.test_gen)\n        print(predict)\n        self.preds.append(predict)\n        k = np.array(self.preds)\n        if (self.is_validation==True):\n            np.save(\"./saved_history/\" + file_name + \"_validation_epoch_\"+ str(epoch)+ \".npy\", k)\n        else:\n            np.save(\"./saved_history/\" + file_name + \"_training_\" + str(epoch)+ \".npy\", k)\n        \n        if (self.is_validation==False and epoch%1==0):\n            self.model.predictor.save_weights(\"./saved_history/models/pred_model_\" + \"_\".join([method_name,dataset_name,attr_grp,attempt]) + \"_epoch_\" + str(epoch) + \".h5\")\n            self.model.discriminator.save_weights(\"./saved_history/models/disc_model_\" + \"_\".join([method_name,dataset_name,attr_grp,attempt]) + \"_epoch_\" + str(epoch) + \".h5\")\n            self.model.generator.save_weights(\"./saved_history/models/vae_model_\" + \"_\".join([method_name,dataset_name,attr_grp,attempt]) + \"_epoch_\" + str(epoch) + \".h5\")","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.815539Z","iopub.execute_input":"2022-03-27T08:38:29.815829Z","iopub.status.idle":"2022-03-27T08:38:29.828214Z","shell.execute_reply.started":"2022-03-27T08:38:29.815788Z","shell.execute_reply":"2022-03-27T08:38:29.827023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir logs\n!mkdir saved_history\n!mkdir saved_history/models","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:29.829967Z","iopub.execute_input":"2022-03-27T08:38:29.830486Z","iopub.status.idle":"2022-03-27T08:38:32.172583Z","shell.execute_reply.started":"2022-03-27T08:38:29.830437Z","shell.execute_reply":"2022-03-27T08:38:32.171753Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def startTraining(trackers, splits, break_point_ep, validation_first, load_model, further_training, model_params, last_epoch, last_iteration):\n    preds=[]\n    validation_train_history=[]\n    \n    model_name, method_name, dataset_name, attr_grp, attempt = model_params\n    file_name = \"_\".join([model_name,method_name,dataset_name,attr_grp,attempt])\n\n    logdir = \"./logs/\" + file_name\n\n    csv_logger = CSVLogger('./saved_history/training_results_' + file_name + '.csv', separator = ',', append=True)\n    logger = CSVLogger('./saved_history/pretraining_results_' + file_name + '.csv', separator = ',', append=True)\n    tensorboard_callback = TensorBoard(log_dir = logdir)\n    pre_tensorboard_callback = TensorBoard(log_dir =\"./logs/pre_\" + file_name)\n\n    # Instantiate components\n    # defining my predictor\n    pred_model = predictor()\n    pred_model.compile(optimizer = keras.optimizers.SGD(learning_rate=hyperparameters['lr'],\n                                                        clipnorm=1.0 ))\n\n    # defining my discriminator\n    disc_in, disc_out = discriminator()\n    disc = Model(inputs = disc_in, outputs = disc_out)\n    disc.compile(optimizer = keras.optimizers.SGD(learning_rate=hyperparameters['lr'],\n                                                  clipnorm=1.0 ))\n    \n    # defining my generator\n    X, z, mu = variationalAutoEncoder()\n    vae = Model(inputs = X, outputs = [z,mu])\n    vae.compile(optimizer = keras.optimizers.RMSprop(learning_rate=hyperparameters['lr'],\n                                                     clipnorm=1.0 ))\n        \n    if load_model:            \n        print(\"Discriminator weights loading ...\")\n        disc.load_weights(\"./saved_history/models/disc_model_\" + \"_\".join([method_name,dataset_name,attr_grp,attempt]) + \"_epoch_\" + last_epoch + '.h5', by_name=True)\n        \n        print(\"VAE weights loading ...\")\n        vae.load_weights(\"./saved_history/models/vae_model_\" + \"_\".join([method_name,dataset_name,attr_grp,attempt]) + \"_epoch_\" + last_epoch + '.h5' ,by_name = True)\n\n        print(\"Predictor weights loading ...\")\n        pred_model.load_weights(\"./saved_history/models/pred_model_\" + \"_\".join([method_name,dataset_name,attr_grp,attempt]) + \"_epoch_\" + last_epoch + '.h5', by_name=True)\n\n    # Instantiate AL model\n    AL_model = ActiveLearning(discriminator=disc, generator=vae, predictor=pred_model, trackers = trackers, alpha=1)\n    AL_model.compile(\n        d_optimizer=keras.optimizers.SGD(learning_rate=hyperparameters['lr'],clipnorm=1.0 ),\n        g_optimizer= keras.optimizers.RMSprop(learning_rate=hyperparameters['lr'],clipnorm=1.0),\n        p_optimizer=keras.optimizers.SGD(learning_rate=hyperparameters['lr'],clipnorm=1.0 ))\n\n    print('model loaded')\n    \n    attr = pd.read_csv('../input/celeba-dataset/list_attr_celeba.csv')\n    eval_partition = pd.read_csv('../input/celeba-dataset/list_eval_partition.csv')\n    \n    image_path = '../input/celeba-dataset/img_align_celeba/img_align_celeba'\n    train_imggen = ImageDataGenerator(rescale = 1./255)\n    train, val, test = preprocess(hyperparameters, attr, eval_partition)\n    train_gen_full = load_generator(train, False)\n    val_gen = load_generator(val)\n    test_gen = load_generator(test, False)\n\n    if validation_first==True and further_training==False:\n\n        labelled_pretrain, idx_prelabelled = divide_data(val, initial=True)\n        idx_preunlabelled = list(np.setdiff1d(list(range(val.shape[0])), idx_prelabelled))\n        unlabelled_pretrain = pd.DataFrame(val.values[idx_preunlabelled,:], columns=val.columns)\n        pretrain_gen = generate_generator_multiple(generator=train_imggen,\n                                               dir1=image_path,\n                                               dir2=image_path,\n                                               df1 = labelled_pretrain,\n                                               df2 = unlabelled_pretrain,\n                                               batch_size=hyperparameters['batch_size'],\n                                               img_height=hyperparameters['height'],\n                                               img_width=hyperparameters['width'])\n        labelled_pretrain_gen  = load_generator(labelled_pretrain, False)\n        \n        num_steps = int((val.shape[0]*hyperparameters['initial_percent_val']) / hyperparameters['batch_size'])\n        val_history = AL_model.fit(pretrain_gen, epochs = hyperparameters['pretraining_epochs'], steps_per_epoch = num_steps, callbacks = [CalculatingPredictions(preds, test_gen, labelled_pretrain_gen, 0.01, model_params, True) , pre_tensorboard_callback, logger], verbose=1)\n        validation_train_history.append(val_history.history)\n        \n        with open(\"./saved_history/pretraining_history_list_\" + file_name + \".json\", 'w') as f:\n            json.dump(validation_train_history, f, indent=2)\n\n        labelled_train, idx_labelled = divide_data(train)\n        idx_unlabelled = list(np.setdiff1d(list(range(train.shape[0])), idx_labelled))\n        unlabelled_train = pd.DataFrame(train.values[idx_unlabelled,:], columns=train.columns)\n        train_gen = generate_generator_multiple(generator=train_imggen,\n                                               dir1=image_path,\n                                               dir2=image_path,\n                                               df1 = labelled_train,\n                                               df2 = unlabelled_train,\n                                               batch_size=hyperparameters['batch_size'],\n                                               img_height=hyperparameters['height'],\n                                               img_width=hyperparameters['width'])\n        unlabelled_gen = load_generator(unlabelled_train, False)\n        labelled_train_gen = load_generator(labelled_train, False)\n        \n        ## save idx_labelled_list\n        np.save(\"./saved_history/pre_idx_labelled_\" + file_name + \".npy\",np.array(idx_labelled))\n        \n    elif further_training==True:\n        \n        ## calculate iteration and epoch\n        ite = last_iteration\n        if ite == -1:\n            ## load pretraining\n            idx_labelled = np.load(\"./saved_history/pre_idx_labelled_\" + file_name + \".npy\")\n        else:    \n            idx_labelled = np.load(\"./saved_history/idx_labelled_\" + str(ite) + '_' + file_name + \".npy\")\n\n        labelled_train = train.iloc[idx_labelled, :]\n        idx_unlabelled = list(np.setdiff1d(list(range(train.shape[0])), idx_labelled))\n        unlabelled_train = pd.DataFrame(train.values[idx_unlabelled,:], columns=train.columns)\n        train_gen = generate_generator_multiple(generator=train_imggen,\n                                               dir1=image_path,\n                                               dir2=image_path,\n                                               df1 = labelled_train,\n                                               df2 = unlabelled_train,\n                                               batch_size=hyperparameters['batch_size'],\n                                               img_height=hyperparameters['height'],\n                                               img_width=hyperparameters['width'])\n        unlabelled_gen = load_generator(unlabelled_train, False)\n        labelled_train_gen = load_generator(labelled_train, False)\n    else:\n        labelled_train, idx_labelled = divide_data(train)\n        idx_unlabelled = list(np.setdiff1d(list(range(train.shape[0])), idx_labelled))\n        unlabelled_train = pd.DataFrame(train.values[idx_unlabelled,:], columns=train.columns)\n        train_gen = generate_generator_multiple(generator=train_imggen,\n                                               dir1=image_path,\n                                               dir2=image_path,\n                                               df1 = labelled_train,\n                                               df2 = unlabelled_train,\n                                               batch_size=hyperparameters['batch_size'],\n                                               img_height=hyperparameters['height'],\n                                               img_width=hyperparameters['width'])\n        unlabelled_gen = load_generator(unlabelled_train, False)\n        labelled_train_gen = load_generator(labelled_train, False)\n\n    history_list=[]\n        \n    if further_training==True and last_epoch >= ((hyperparameters['increment_train_epoch'] * 7) - 1):\n        num_batches = idx_labelled.shape[0]//hyperparameters['batch_size']\n\n        iteration = ite + 1\n        epoch_num = last_epoch+1\n\n        history = AL_model.fit(train_gen,initial_epoch = epoch_num, epochs=epoch_num+hyperparameters['additional_epoch'], steps_per_epoch = num_batches, validation_data=val_gen,callbacks = [CalculatingPredictions(preds, test_gen,labelled_train_gen, 0.01, model_params), csv_logger, tensorboard_callback], verbose = 1)\n        history_list.append(history.history)\n\n        with open(\"./saved_history/history_list_\" + str(iteration) + '_' + file_name + \".json\", 'w') as f:\n            json.dump(history_list, f, indent=2)\n\n        with open(\"./saved_history/preds_\" + str(iteration) + '_' + file_name + \".json\", 'w') as f:\n            json.dump(preds, f, indent=2)\n        \n        np.save(\"./saved_history/idx_labelled_\" + str(iteration) + '_' + file_name + \".npy\",np.array(idx_labelled))\n    else:\n        \n        epoch_num = 0\n        ite = 0 \n        if further_training==True:\n            ite = last_iteration+1\n            epoch_num = last_epoch+1\n\n        test_predictions=[]\n        indices_list = []\n        num_batches = int((train.shape[0] * splits[ite]) / hyperparameters['batch_size'])\n\n        for iteration in range(ite, len(splits)):\n            print(iteration)\n\n            if iteration==0:\n                try:\n                    # Initial training ---- change\n                    history = AL_model.fit(train_gen, initial_epoch = epoch_num, epochs=(iteration+1)*hyperparameters['initial_train_epoch'], steps_per_epoch = num_batches, validation_data=val_gen,callbacks = [CalculatingPredictions(preds, test_gen, labelled_train_gen, 0.01, model_params), csv_logger, tensorboard_callback], verbose = 1)\n                    history_list.append(history.history)\n                    epoch_num=(iteration+1)*hyperparameters['initial_train_epoch']\n                except Exception as e:\n                    print(e)\n            else:\n                # Increment training --- change\n                history = AL_model.fit(train_gen, initial_epoch = epoch_num, epochs=(iteration+1)*hyperparameters['increment_train_epoch'], steps_per_epoch = num_batches, validation_data=val_gen,callbacks = [CalculatingPredictions(preds, test_gen, labelled_train_gen, 0.01, model_params), csv_logger, tensorboard_callback], verbose = 1)\n                history_list.append(history.history)\n                epoch_num = (iteration+1)*hyperparameters['increment_train_epoch']\n\n            # append indices\n            indices_list.append(idx_labelled)\n            inc = int(train.shape[0]*0.05 /  hyperparameters['batch_size'])\n            num_batches+= inc\n            print('Number of batches added:' , inc)\n            \n            with open(\"./saved_history/history_list_\" + str(iteration) + '_' + file_name + \".json\", 'w') as f:\n                json.dump(history_list, f, indent=2)\n\n            with open(\"./saved_history/preds_\" + str(iteration) + '_' + file_name + \".json\", 'w') as f:\n                json.dump(preds, f, indent=2)\n\n            np.save(\"./saved_history/idx_labelled_\" + str(iteration) + '_' + file_name + \".npy\",np.array(idx_labelled))\n\n            if (iteration!=(len(splits)-1)): # last iteration\n\n                k = random.sample(idx_unlabelled, inc*hyperparameters['batch_size'])\n\n                idx_labelled = list(idx_labelled)+list(k)\n                labelled_train = pd.DataFrame(train.values[idx_labelled,:], columns=train.columns)\n                idx_unlabelled = list(np.setdiff1d(idx_unlabelled, k))\n                unlabelled_train = pd.DataFrame(train.values[idx_unlabelled,:], columns=train.columns)\n                train_gen = generate_generator_multiple(generator=train_imggen,\n                                                               dir1=image_path,\n                                                               dir2=image_path,\n                                                               df1 = labelled_train,\n                                                               df2 = unlabelled_train,\n                                                               batch_size=hyperparameters['batch_size'],\n                                                               img_height=hyperparameters['height'],\n                                                               img_width=hyperparameters['width'])\n                unlabelled_gen = load_generator(unlabelled_train, False)\n                labelled_train_gen = load_generator(labelled_train, False)\n\n    return history_list, pred_model, vae, disc, AL_model, indices_list, preds","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:32.17581Z","iopub.execute_input":"2022-03-27T08:38:32.176272Z","iopub.status.idle":"2022-03-27T08:38:32.227508Z","shell.execute_reply.started":"2022-03-27T08:38:32.176232Z","shell.execute_reply":"2022-03-27T08:38:32.226769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = \"AL_model\"\nmethod_name = 'random_sampling'\ndataset_name = 'celeba'\nattr_grp = 'mouth'\nattempt = '1'\nlast_epoch = 0\nlast_iteration = 0\n\nmodel_params = [model_name, method_name, dataset_name, attr_grp, attempt]\n\nhistory_list, pred_model, vae, disc, Al_model, indices_list, preds = startTraining(trackers, splits, break_point_ep, True, False, False, model_params, last_epoch, last_iteration)","metadata":{"execution":{"iopub.status.busy":"2022-03-27T08:38:32.228534Z","iopub.execute_input":"2022-03-27T08:38:32.230842Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}