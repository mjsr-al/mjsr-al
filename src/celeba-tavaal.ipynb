{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-06-08T15:57:27.492429Z","iopub.status.busy":"2023-06-08T15:57:27.491922Z","iopub.status.idle":"2023-06-08T15:57:27.497432Z","shell.execute_reply":"2023-06-08T15:57:27.496250Z","shell.execute_reply.started":"2023-06-08T15:57:27.492384Z"},"trusted":true},"outputs":[],"source":["# !git clone https://github.com/cubeyoung/TA-VAAL.git"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-06-08T15:57:27.500547Z","iopub.status.busy":"2023-06-08T15:57:27.500037Z","iopub.status.idle":"2023-06-08T15:57:27.506637Z","shell.execute_reply":"2023-06-08T15:57:27.505626Z","shell.execute_reply.started":"2023-06-08T15:57:27.500516Z"},"trusted":true},"outputs":[],"source":["# !python3 TA-VAAL/main.py -m TA-VAAL -d cifar10 -c 3"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## TA-VAAL CelebA"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-06-08T15:57:27.513419Z","iopub.status.busy":"2023-06-08T15:57:27.511396Z","iopub.status.idle":"2023-06-08T15:57:38.729822Z","shell.execute_reply":"2023-06-08T15:57:38.728703Z","shell.execute_reply.started":"2023-06-08T15:57:27.513392Z"},"trusted":true},"outputs":[],"source":["# !pip3 install torchsummary"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-06-08T15:57:38.734338Z","iopub.status.busy":"2023-06-08T15:57:38.734020Z","iopub.status.idle":"2023-06-08T15:57:48.418095Z","shell.execute_reply":"2023-06-08T15:57:48.417193Z","shell.execute_reply.started":"2023-06-08T15:57:38.734310Z"},"trusted":true},"outputs":[],"source":["import os\n","import math\n","import random\n","import numpy as np\n","from tqdm import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.init as init\n","import torch.nn.functional as F\n","from torch.distributions import Normal\n","from torch.nn.parameter import Parameter\n","from torch.nn.modules.module import Module\n","from torch.utils.data import DataLoader, Dataset\n","import torch.optim.lr_scheduler as lr_scheduler\n","from torch.utils.data.sampler import SubsetRandomSampler\n","from torch.utils.tensorboard import SummaryWriter\n","from torchvision.datasets import CIFAR100, CIFAR10, FashionMNIST, SVHN, CelebA\n","import torchvision.transforms as T\n","import torchvision.models as models\n","from torchvision import models\n","from torchsummary import summary\n","from sklearn.metrics import confusion_matrix\n","import argparse\n","import pandas as pd\n","import cv2\n","import csv"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-06-08T15:57:48.421085Z","iopub.status.busy":"2023-06-08T15:57:48.419395Z","iopub.status.idle":"2023-06-08T15:57:48.440680Z","shell.execute_reply":"2023-06-08T15:57:48.439618Z","shell.execute_reply.started":"2023-06-08T15:57:48.421050Z"},"trusted":true},"outputs":[],"source":["class CelebADataset(Dataset):\n","\n","    def __init__(self, csv_file, root_dir, eval_file, targets, split='train', transform=None):\n","\n","        self.attr = pd.read_csv(csv_file)\n","        self.eval_partition = pd.read_csv(eval_file)\n","        self.root_dir = root_dir\n","        self.split = split\n","        self.transform = transform\n","        self.targets = targets\n","        \n","        self.attr = self.attr[self.targets]\n","        self.attr = self.attr.replace(-1, 0)\n","        self.attr = self.attr.set_index('image_id')\n","        self.eval_partition = self.eval_partition.set_index('image_id')\n","        self.attr = self.attr.join(self.eval_partition)\n","        self.attr['image_id'] = self.attr.index\n","        \n","        if self.split == 'train':\n","            self.attr = self.attr.loc[self.attr['partition']==0]\n","            self.attr = self.attr.drop('partition', axis=1)\n","        else:\n","            self.attr = self.attr.loc[self.attr['partition']==2]\n","            self.attr = self.attr.drop('partition', axis=1)\n","            \n","        self.attr = self.attr[self.targets] \n","\n","    def __len__(self):\n","        return len(self.attr)\n","\n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","    \n","        img_name = self.root_dir + self.attr.iloc[idx, 0]\n","        \n","        image = cv2.imread(img_name) / 255.0\n","        attr = self.attr.iloc[idx, 1:]\n","        attr = np.array([attr])\n","        attr = attr.astype('float')\n","        \n","        image = cv2.resize(image, dsize=(96, 96), interpolation=cv2.INTER_AREA)\n","        image = image.reshape(image.shape[2], image.shape[0], -1)\n","        sample = (torch.tensor(image).float(), attr)\n","\n","        if self.transform:\n","            sample = self.transform(sample)\n","\n","        return sample\n","    \n","class SubsetSequentialSampler(torch.utils.data.Sampler):\n","    r\"\"\"Samples elements sequentially from a given list of indices, without replacement.\n","\n","    Arguments:\n","        indices (sequence): a sequence of indices\n","    \"\"\"\n","\n","    def __init__(self, indices):\n","        self.indices = indices\n","\n","    def __iter__(self):\n","        return (self.indices[i] for i in range(len(self.indices)))\n","    \n","    def __len__(self):\n","        return len(self.indices)\n","    \n","class MyDataset(Dataset):\n","    def __init__(self, dataset_name, train_flag, targets = None, transf=None):\n","        self.dataset_name = dataset_name\n","        \n","        if self.dataset_name == \"cifar10\":\n","            self.cifar10 = CIFAR10('../input/cifar10', train=train_flag, \n","                                    download=True, transform=transf)\n","            \n","        if self.dataset_name == \"celeba\":\n","            attr_file = '../input/celeba-dataset/list_attr_celeba.csv'\n","            eval_file = '../input/celeba-dataset/list_eval_partition.csv'\n","            path = '../input/celeba-dataset/img_align_celeba/img_align_celeba/'            \n","            self.celeba = CelebADataset(attr_file, path, eval_file, targets, split='train')  \n","\n","    def __getitem__(self, index):\n","        if self.dataset_name == \"cifar10\":\n","            data, target = self.cifar10[index]\n","            \n","        if self.dataset_name == \"celeba\":\n","            data, target = self.celeba[index]\n","        \n","        return data, target, index\n","\n","    def __len__(self):\n","        if self.dataset_name == \"cifar10\":\n","            return len(self.cifar10)\n","        \n","        if self.dataset_name == \"celeba\":\n","            return len(self.celeba)"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-06-08T15:57:48.444128Z","iopub.status.busy":"2023-06-08T15:57:48.443875Z","iopub.status.idle":"2023-06-08T15:57:48.453635Z","shell.execute_reply":"2023-06-08T15:57:48.452824Z","shell.execute_reply.started":"2023-06-08T15:57:48.444106Z"},"trusted":true},"outputs":[],"source":["def load_dataset(args, targets = None):\n","    \n","    train_transform = T.Compose([\n","        T.RandomHorizontalFlip(),\n","        T.RandomCrop(size=32, padding=4),\n","        T.ToTensor(),\n","        T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]) \n","    ])\n","\n","    test_transform = T.Compose([\n","        T.ToTensor(),\n","        T.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010]) \n","    ])\n","        \n","    if args['dataset'] == 'celeba':\n","        \n","        attr_file = '../input/celeba-dataset/list_attr_celeba.csv'\n","        eval_file = '../input/celeba-dataset/list_eval_partition.csv'\n","        path = '../input/celeba-dataset/img_align_celeba/img_align_celeba/'\n","        \n","        data_train = CelebADataset(attr_file, path, eval_file, targets, split='train')    \n","        data_unlabeled = MyDataset(args['dataset'], True, targets)\n","        data_test  = CelebADataset(attr_file, path, eval_file, targets, split='test')\n","        NO_CLASSES = len(targets) - 1\n","        adden = args['ADDENDUM']\n","        NUM_TRAIN = len(data_train)        \n","        no_train = NUM_TRAIN\n","\n","    if args['dataset'] == 'cifar10':\n","        \n","        attr_file = '../input/celeba-dataset/list_attr_celeba.csv'\n","        eval_file = '../input/celeba-dataset/list_eval_partition.csv'\n","        path = '../input/celeba-dataset/img_align_celeba/img_align_celeba/'\n","        \n","        data_train = CelebADataset(attr_file, path, eval_file, targets, split='train')    \n","        data_unlabeled = MyDataset(args['dataset'], True, targets)\n","        data_test  = CelebADataset(attr_file, path, eval_file, targets, split='test')\n","        NO_CLASSES = len(targets) - 1\n","        adden = args['ADDENDUM']\n","        NUM_TRAIN = len(data_train)        \n","        no_train = NUM_TRAIN\n","\n","        \n","    return data_train, data_unlabeled, data_test, adden, NO_CLASSES, no_train"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-06-08T15:57:48.457643Z","iopub.status.busy":"2023-06-08T15:57:48.457147Z","iopub.status.idle":"2023-06-08T15:57:48.480284Z","shell.execute_reply":"2023-06-08T15:57:48.479431Z","shell.execute_reply.started":"2023-06-08T15:57:48.457606Z"},"trusted":true},"outputs":[],"source":["class BasicBlock(nn.Module):\n","    \n","    def __init__(self, in_planes, planes, expansion, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.expansion = expansion\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10, expansion=1):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","        self.num_classes = num_classes\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], 1, stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], 1, stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], 1, stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], 1, stride=2)\n","        \n","        self.linear1 = nn.Linear(512*expansion, 2)\n","        self.linear2 = nn.Linear(512*expansion, 2)\n","        self.linear3 = nn.Linear(512*expansion, 2)\n","        self.linear4 = nn.Linear(512*expansion, 2)\n","        self.linear5 = nn.Linear(512*expansion, 2)\n","        self.linear6 = nn.Linear(512*expansion, 2)\n","        self.linear7 = nn.Linear(512*expansion, 2)\n","        self.linear8 = nn.Linear(512*expansion, 2)\n","        self.linear9 = nn.Linear(512*expansion, 2)\n","        self.linear10 = nn.Linear(512*expansion, 2)\n","        self.linear11 = nn.Linear(512*expansion, 2)\n","\n","    def _make_layer(self, block, planes, num_blocks, expansion, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, expansion, stride))\n","            self.in_planes = planes * expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out1 = self.layer1(out)\n","        out2 = self.layer2(out1)\n","        out3 = self.layer3(out2)\n","        out4 = self.layer4(out3)\n","        out = F.avg_pool2d(out4, 4)\n","        outf = out.view(out.size(0), -1)\n","        outt1 = self.linear1(outf)\n","        outt2 = self.linear2(outf)\n","        outt3 = self.linear3(outf)\n","        outt4 = self.linear4(outf)\n","        outt5 = self.linear5(outf)\n","        outt6 = self.linear6(outf)\n","        outt7 = self.linear7(outf)\n","        outt8 = self.linear8(outf)\n","        outt9 = self.linear9(outf)\n","        outt10 = self.linear10(outf)\n","        outt11 = self.linear11(outf)\n","\n","        return [outt1, outt2, outt3, outt4, outt5, outt6, outt7, outt8, outt9, outt10, outt11], outf, [out1, out2, out3, out4]\n","\n","def ResNet18(num_classes = 10, expansion = 1):\n","    return ResNet(BasicBlock, [2,2,2,2], num_classes, expansion)"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-06-08T15:57:48.482188Z","iopub.status.busy":"2023-06-08T15:57:48.481797Z","iopub.status.idle":"2023-06-08T15:57:48.512957Z","shell.execute_reply":"2023-06-08T15:57:48.512126Z","shell.execute_reply.started":"2023-06-08T15:57:48.482158Z"},"trusted":true},"outputs":[],"source":["class LossNet(nn.Module):\n","    def __init__(self, feature_sizes=[32, 16, 8, 4], num_channels=[64, 128, 256, 512], interm_dim=128):\n","        super(LossNet, self).__init__()\n","        \n","        self.GAP1 = nn.AvgPool2d(feature_sizes[0])\n","        self.GAP2 = nn.AvgPool2d(feature_sizes[1])\n","        self.GAP3 = nn.AvgPool2d(feature_sizes[2])\n","        self.GAP4 = nn.AvgPool2d(feature_sizes[3])\n","\n","        self.FC1 = nn.Linear(num_channels[0]*9, interm_dim)\n","        self.FC2 = nn.Linear(num_channels[1]*9, interm_dim)\n","        self.FC3 = nn.Linear(num_channels[2]*9, interm_dim)\n","        self.FC4 = nn.Linear(num_channels[3]*9, interm_dim)\n","\n","        self.linear = nn.Linear(4 * interm_dim, 1)\n","    \n","    def forward(self, features):\n","        out1 = self.GAP1(features[0])\n","        out1 = out1.view(out1.size(0), -1)\n","        out1 = F.relu(self.FC1(out1))\n","\n","        out2 = self.GAP2(features[1])\n","        out2 = out2.view(out2.size(0), -1)\n","        out2 = F.relu(self.FC2(out2))\n","\n","        out3 = self.GAP3(features[2])\n","        out3 = out3.view(out3.size(0), -1)\n","        out3 = F.relu(self.FC3(out3))\n","\n","        out4 = self.GAP4(features[3])\n","        out4 = out4.view(out4.size(0), -1)\n","        out4 = F.relu(self.FC4(out4))\n","\n","        out = self.linear(torch.cat((out1, out2, out3, out4), 1))\n","        return out\n","\n","class View(nn.Module):\n","    def __init__(self, size):\n","        super(View, self).__init__()\n","        self.size = size\n","\n","    def forward(self, tensor):\n","        return tensor.view(self.size)\n","    \n","class VAE(nn.Module):\n","    \"\"\"Encoder-Decoder architecture for both WAE-MMD and WAE-GAN.\"\"\"\n","    def __init__(self, z_dim=32, nc=3, f_filt=4):\n","        super(VAE, self).__init__()\n","        self.z_dim = z_dim\n","        self.nc = nc\n","        self.f_filt = f_filt\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(nc, 128, 4, 2, 1, bias=False),              # B,  128, 32, 32\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(True),\n","            nn.Conv2d(128, 256, 4, 2, 1, bias=False),             # B,  256, 16, 16\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(True),\n","            nn.Conv2d(256, 512, 4, 2, 1, bias=False),             # B,  512,  8,  8\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(True),\n","            nn.Conv2d(512, 1024, self.f_filt, 2, 1, bias=False),            # B, 1024,  4,  4\n","            nn.BatchNorm2d(1024),\n","            nn.ReLU(True),\n","            View((-1, 1024*6*6)),     #1024*14*12                            # B, 1024*4*4\n","        )\n","\n","        self.fc_mu = nn.Linear(1024*6*6, z_dim)                            # B, z_dim\n","        self.fc_logvar = nn.Linear(1024*6*6, z_dim)                            # B, z_dim\n","        self.decoder = nn.Sequential(\n","            nn.Linear(z_dim + 1, 1024*6*6),                           # B, 1024*8*8\n","            View((-1, 1024, 6, 6)),                               # B, 1024,  8,  8\n","            nn.ConvTranspose2d(1024, 512, self.f_filt, 2, 1, bias=False),   # B,  512, 16, 16\n","            nn.BatchNorm2d(512),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),    # B,  256, 32, 32\n","            nn.BatchNorm2d(256),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),    # B,  128, 64, 64\n","            nn.BatchNorm2d(128),\n","            nn.ReLU(True),\n","            nn.ConvTranspose2d(128, nc, 2, 2),                       # B,   nc, 64, 64\n","        )\n","        self.weight_init()\n","\n","    def weight_init(self):\n","        for block in self._modules:\n","            try:\n","                for m in self._modules[block]:\n","                    kaiming_init(m)\n","            except:\n","                kaiming_init(block)\n","\n","    def forward(self, r, x):\n","        z = self._encode(x)\n","        mu, logvar = self.fc_mu(z), self.fc_logvar(z)\n","        z = self.reparameterize(mu, logvar)\n","        z = torch.cat([z,r],1)\n","        x_recon = self._decode(z)\n","\n","        return  x_recon, z, mu, logvar\n","\n","    def reparameterize(self, mu, logvar):\n","        stds = (0.5 * logvar).exp()\n","        epsilon = torch.randn(*mu.size())\n","        if mu.is_cuda:\n","            stds, epsilon = stds.cuda(), epsilon.cuda()\n","        latents = epsilon * stds + mu\n","        return latents\n","\n","    def _encode(self, x):\n","        return self.encoder(x)\n","\n","    def _decode(self, z):\n","        return self.decoder(z)\n","\n","class Discriminator(nn.Module):\n","    \"\"\"Adversary architecture(Discriminator) for WAE-GAN.\"\"\"\n","    def __init__(self, z_dim=10):\n","        super(Discriminator, self).__init__()\n","        self.z_dim = z_dim\n","        self.net = nn.Sequential(\n","            nn.Linear(z_dim + 1, 512),\n","            nn.ReLU(True),\n","            nn.Linear(512, 512),\n","            nn.ReLU(True),\n","            nn.Linear(512, 1),\n","            nn.Sigmoid()\n","        )\n","        self.weight_init()\n","\n","    def weight_init(self):\n","        for block in self._modules:\n","            for m in self._modules[block]:\n","                kaiming_init(m)\n","\n","    def forward(self, r,z):  \n","        z = torch.cat([z, r], 1)\n","        return self.net(z)\n","\n","def kaiming_init(m):\n","    if isinstance(m, (nn.Linear, nn.Conv2d)):\n","        init.kaiming_normal(m.weight)\n","        if m.bias is not None:\n","            m.bias.data.fill_(0)\n","    elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n","        m.weight.data.fill_(1)\n","        if m.bias is not None:\n","            m.bias.data.fill_(0)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-06-08T15:57:48.514706Z","iopub.status.busy":"2023-06-08T15:57:48.514233Z","iopub.status.idle":"2023-06-08T15:57:48.562847Z","shell.execute_reply":"2023-06-08T15:57:48.561915Z","shell.execute_reply.started":"2023-06-08T15:57:48.514675Z"},"trusted":true},"outputs":[],"source":["def LossPredLoss(input, target, margin=1.0, reduction='mean'):\n","    assert len(input) % 2 == 0, 'the batch size is not even.'\n","    assert input.shape == input.flip(0).shape\n","    criterion = nn.BCELoss()\n","    input = (input - input.flip(0))[:len(input)//2] # [l_1 - l_2B, l_2 - l_2B-1, ... , l_B - l_B+1], where batch_size = 2B\n","    target = (target - target.flip(0))[:len(target)//2]\n","    target = target.detach()\n","    diff = torch.sigmoid(input)\n","    one = torch.sign(torch.clamp(target, min=0)) # 1 operation which is defined by the authors\n","    \n","    if reduction == 'mean':\n","        loss = criterion(diff,one)\n","    elif reduction == 'none':\n","        loss = criterion(diff,one)\n","    else:\n","        NotImplementedError()\n","    \n","    return loss\n","\n","def test(models, epoch, method, criterion, dataloaders, args, mode='val'):\n","    \n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","\n","    models['backbone'].eval()\n","    if method == 'lloss':\n","        models['module'].eval()\n","    \n","    total = 0\n","    correct = [0]*args['NO_CLASSES']\n","    m = nn.LogSoftmax(dim=1)\n","    cm=[]\n","    for i in range(args['NO_CLASSES']):\n","        cm.append(np.zeros((2,2)))\n","\n","    individual_predictor_loss = [[] for i in range(args['NO_CLASSES'])]\n","    total_lloss = 0.0\n","    total_loss = 0.0\n","    total_pred_loss = 0.0\n","        \n","    with torch.no_grad():\n","        for (inputs, labels) in dataloaders[mode]:\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            scores, _, features = models['backbone'](inputs)\n","            labels = torch.reshape(labels, (labels.size(0), labels.size(2))).float()\n","            \n","            losses = []\n","            for i in range(args['NO_CLASSES']):\n","                s = m(scores[i])\n","                _,preds = torch.max(s, dim=1)\n","\n","                cm[i]+=confusion_matrix(labels[:,i].cpu().numpy(), preds.cpu().numpy())\n","                correct[i]+=(preds == labels[:,i]).sum().item()  \n","                \n","                l = criterion[i](s.float(), labels[:,i].long())\n","                losses.append(l)\n","                individual_predictor_loss[i].append(l)    \n","                \n","            total += labels.size(0)\n","            target_loss=torch.stack(losses).mean(dim=0)\n","            \n","            if method == 'lloss' or 'TA-VAAL':\n","                if epoch > args['EPOCHL']:\n","                    features[0] = features[0].detach()\n","                    features[1] = features[1].detach()\n","                    features[2] = features[2].detach()\n","                    features[3] = features[3].detach()\n","\n","                pred_loss = models['module'](features)\n","                pred_loss = pred_loss.view(pred_loss.size(0))\n","                m_module_loss   = LossPredLoss(pred_loss, target_loss, margin=args['MARGIN'])\n","\n","                total_lloss+= m_module_loss.item()\n","\n","                m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)        \n","                loss            = m_backbone_loss + args['WEIGHT'] * m_module_loss \n","                total_loss+=loss.item()\n","            else:\n","                m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)        \n","                loss            = m_backbone_loss\n","    \n","    for i in range(args['NO_CLASSES']):\n","        individual_predictor_loss[i] = torch.stack(individual_predictor_loss[i]).mean().item()            \n","    \n","    return (100 * np.array(correct)) / total, cm, individual_predictor_loss, total_lloss, total_loss\n","\n","iters = 0\n","def train_epoch(models, method, criterion, optimizers, dataloaders, epoch, args): #epoch_loss, num_tasks):\n","    \n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    models['backbone'].train()\n","    if method == 'lloss' or 'TA-VAAL':\n","        models['module'].train()\n","    global iters\n","    models['backbone'].train()\n","    models['module'].train()\n","    models['backbone'].to(device)\n","    models['module'].to(device)\n","    m = nn.LogSoftmax(dim=1)\n","    \n","    individual_predictor_loss = [[] for i in range(args['NO_CLASSES'])]\n","    total_lloss = 0.0\n","    total_loss = 0.0\n","    correct = [0 for i in range(args['NO_CLASSES'])]\n","    total=0\n","\n","    for data in tqdm(dataloaders['train'], leave=False, total=len(dataloaders['train'])):\n","        inputs = data[0].to(device)\n","        labels = data[1].to(device)\n","\n","        iters += 1\n","\n","        optimizers['backbone'].zero_grad()\n","        if method == 'lloss' or 'TA-VAAL':\n","            optimizers['module'].zero_grad()\n","        \n","        scores, _, features = models['backbone'](inputs)\n","        \n","        labels = torch.reshape(labels, (labels.size(0), labels.size(2))).float()\n","\n","        losses = []\n","        for i in range(args['NO_CLASSES']):\n","            s = m(scores[i])\n","            _,preds = torch.max(s, dim=1)\n","            correct[i]+=(preds == labels[:,i]).sum().item()  \n","            l = criterion[i](s.float(), labels[:,i].long())\n","            losses.append(l)\n","            individual_predictor_loss[i].append(l)\n","            \n","        total += labels.size(0)\n","        target_loss=torch.stack(losses).mean(dim=0)\n","\n","        if method == 'lloss' or 'TA-VAAL':\n","            if epoch > args['EPOCHL']:\n","                features[0] = features[0].detach()\n","                features[1] = features[1].detach()\n","                features[2] = features[2].detach()\n","                features[3] = features[3].detach()\n","                \n","            pred_loss = models['module'](features)\n","            pred_loss = pred_loss.view(pred_loss.size(0))\n","            m_module_loss   = LossPredLoss(pred_loss, target_loss, margin=args['MARGIN'])\n","            \n","            total_lloss+= m_module_loss.item()\n","\n","            m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)        \n","            loss            = m_backbone_loss + args['WEIGHT'] * m_module_loss \n","            total_loss+=loss.item()\n","        else:\n","            m_backbone_loss = torch.sum(target_loss) / target_loss.size(0)        \n","            loss            = m_backbone_loss\n","            \n","        loss.backward()\n","        optimizers['backbone'].step()\n","        if method == 'lloss' or 'TA-VAAL':\n","            optimizers['module'].step()\n","            \n","    for i in range(args['NO_CLASSES']):\n","        individual_predictor_loss[i] = torch.stack(individual_predictor_loss[i]).mean().item()\n","            \n","    return loss, individual_predictor_loss, total_lloss, total_loss, (100 * np.array(correct)) / total\n","\n","def train(models, method, criterion, optimizers, schedulers, dataloaders, cycle, args):\n","    \n","    print('>> Train a Model.')\n","    best_acc = 0.\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","    num_tasks=args['NO_CLASSES']\n","    \n","    rows = []\n","    \n","    print(len(dataloaders['train']))\n","    for epoch in range(args['no_of_epochs']):\n","        row = [cycle, epoch]\n","\n","        best_loss = torch.tensor([0.5]).to(device)\n","        loss, individual_predictor_loss, total_lloss, total_loss, individual_acc = train_epoch(models, method, criterion, optimizers, dataloaders, epoch, args)\n","\n","        schedulers['backbone'].step()\n","        if method == 'lloss' or 'TA-VAAL':\n","            schedulers['module'].step()\n","            \n","        # logging individual_predictor_loss\n","        for i in range(num_tasks):\n","            args['writer-train'].add_scalar(str(cycle) + ' Individual predictor loss '+ str(i),\n","                    individual_predictor_loss[i], epoch)\n","        \n","        # logging total predictor loss\n","        args['writer-train'].add_scalar(str(cycle) + ' Total predictor loss ',\n","                    sum(individual_predictor_loss)/len(individual_predictor_loss), epoch)\n","        \n","        # logging total lloss\n","        args['writer-train'].add_scalar(str(cycle) + ' Total lloss ',\n","                    total_lloss/len(dataloaders['train']), epoch)\n","        \n","        # logging total loss\n","        args['writer-train'].add_scalar(str(cycle) + ' Total loss ',\n","                    total_loss/len(dataloaders['train']), epoch)\n","        \n","        \n","        ## logging predictor acc\n","        args['writer-train'].add_scalar(str(cycle) + ' Total predictor Acc ',\n","                    sum(individual_acc) / len(individual_acc), epoch)\n","        row.append(sum(individual_acc) / len(individual_acc))\n","\n","        # logging individual_predictor_acc\n","        for i in range(num_tasks):\n","            args['writer-train'].add_scalar(str(cycle) + ' Individual predictor acc ' + str(i),\n","                        individual_acc[i], epoch)\n","            row.append(individual_acc[i])\n","\n","        # Testing\n","        acc, cm, individual_predictor_loss, total_lloss, total_loss = test(models, epoch, method, criterion, dataloaders, args, mode='test')\n","        print('Epoch ' + str(epoch)+\": \"+'Mean Accuracy: ', acc.mean())\n","        \n","        \n","        # logging predictor_acc\n","        args['writer-val'].add_scalar(str(cycle) + ' Total predictor Acc ',\n","                    acc.mean(), epoch)\n","        row.append(acc.mean())\n","\n","        # logging individual_predictor_acc\n","        for i in range(num_tasks):\n","            args['writer-val'].add_scalar(str(cycle) + ' Individual predictor acc ' + str(i),\n","                        acc[i], epoch)\n","            row.append(acc[i])\n","        \n","        # logging individual_predictor_loss\n","        for i in range(num_tasks):\n","            args['writer-val'].add_scalar(str(cycle) + ' Individual predictor loss '+ str(i),\n","                    individual_predictor_loss[i], epoch)\n","            \n","        # logging total predictor loss\n","        args['writer-val'].add_scalar(str(cycle) + ' Total predictor loss ',\n","                    sum(individual_predictor_loss)/len(individual_predictor_loss), epoch)\n","            \n","        # logging total lloss\n","        args['writer-val'].add_scalar(str(cycle) + ' Total lloss ',\n","                    total_lloss/len(dataloaders['test']), epoch)\n","        \n","        # logging total loss\n","        args['writer-val'].add_scalar(str(cycle) + ' Total loss ',\n","                    total_loss/len(dataloaders['test']), epoch)      \n","        \n","        rows.append(row)\n","        \n","    with open(\"results_\" + args['group'] + '_' + str(args['attempt']) + \".csv\", 'a') as csvfile: \n","        csvwriter = csv.writer(csvfile) \n","\n","        # writing the data rows \n","        csvwriter.writerows(rows)\n","\n","            \n","#         if True and epoch % 20  == 0:\n","#             acc, cm = test(models, epoch, method, dataloaders, args, mode='test')\n","#             print('Mean Accuracy:', acc.mean())\n","#             for i in range(args['NO_CLASSES']):\n","#                 print('val_accuracy', i, ':', acc[i])\n","    print('>> Finished.')"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-06-08T15:57:48.567103Z","iopub.status.busy":"2023-06-08T15:57:48.564004Z","iopub.status.idle":"2023-06-08T15:57:48.600594Z","shell.execute_reply":"2023-06-08T15:57:48.599762Z","shell.execute_reply.started":"2023-06-08T15:57:48.567079Z"},"trusted":true},"outputs":[],"source":["def read_data(dataloader, labels=True):\n","    if labels:\n","        while True:\n","            for img, label,_ in dataloader:\n","                yield img, label\n","    else:\n","        while True:\n","            for img, _, _ in dataloader:\n","                yield img\n","\n","def vae_loss(x, recon, mu, logvar, beta):\n","    mse_loss = nn.MSELoss()\n","    MSE = mse_loss(recon, x)\n","    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n","    KLD = KLD * beta\n","    return MSE + KLD\n","\n","def train_vaal(models, optimizers, labeled_dataloader, unlabeled_dataloader, cycle, args):\n","    \n","    vae = models['vae']\n","    discriminator = models['discriminator']\n","    task_model = models['backbone']\n","    ranker = models['module']\n","    \n","    task_model.eval()\n","    ranker.eval()\n","    vae.train()\n","    discriminator.train()\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","\n","    vae = vae.to(device)\n","    discriminator = discriminator.to(device)\n","    task_model = task_model.to(device)\n","    ranker = ranker.to(device)\n","\n","    adversary_param = 1\n","    beta          = 1\n","    num_adv_steps = 1\n","    num_vae_steps = 1\n","\n","    bce_loss = nn.BCELoss()\n","    \n","    labeled_data = read_data(labeled_dataloader)\n","    unlabeled_data = read_data(unlabeled_dataloader)\n","\n","    train_iterations = 1250 # int( (args['INCREMENTAL']*cycle+ args['SUBSET']) * args['EPOCHV'] / args['BATCH'] )\n","    print('Num of Iteration:', str(train_iterations))\n","    \n","    for iter_count in range(train_iterations):\n","        labeled_imgs, labels = next(labeled_data)\n","        unlabeled_imgs = next(unlabeled_data)[0]\n","        \n","        labeled_imgs = labeled_imgs.to(device)\n","        unlabeled_imgs = unlabeled_imgs.to(device)\n","        labels = labels.to(device)    \n","        \n","        if iter_count == 0 :\n","            r_l_0 = torch.from_numpy(np.random.uniform(0, 1, size=(labeled_imgs.shape[0],1))).type(torch.FloatTensor).to(device)\n","            r_u_0 = torch.from_numpy(np.random.uniform(0, 1, size=(unlabeled_imgs.shape[0],1))).type(torch.FloatTensor).to(device)\n","        else:\n","            with torch.no_grad():\n","                _,_,features_l = task_model(labeled_imgs)\n","                _,_,feature_u = task_model(unlabeled_imgs)\n","                r_l = ranker(features_l)\n","                r_u = ranker(feature_u)\n","        if iter_count == 0:\n","            r_l = r_l_0.detach()\n","            r_u = r_u_0.detach()\n","            r_l_s = r_l_0.detach()\n","            r_u_s = r_u_0.detach()\n","        else:\n","            r_l_s = torch.sigmoid(r_l).detach()\n","            r_u_s = torch.sigmoid(r_u).detach()   \n","            \n","        # VAE step\n","        for count in range(num_vae_steps): # num_vae_steps\n","            recon, _, mu, logvar = vae(r_l_s,labeled_imgs)\n","            unsup_loss = vae_loss(labeled_imgs, recon, mu, logvar, beta)\n","            unlab_recon, _, unlab_mu, unlab_logvar = vae(r_u_s,unlabeled_imgs)\n","            transductive_loss = vae_loss(unlabeled_imgs, \n","                    unlab_recon, unlab_mu, unlab_logvar, beta)\n","        \n","            labeled_preds = discriminator(r_l,mu)\n","            unlabeled_preds = discriminator(r_u,unlab_mu)\n","            \n","            lab_real_preds = torch.ones(labeled_imgs.size(0))\n","            unlab_real_preds = torch.ones(unlabeled_imgs.size(0))\n","                \n","            lab_real_preds = lab_real_preds.to(device)\n","            unlab_real_preds = unlab_real_preds.to(device)            \n","\n","            dsc_loss = bce_loss(labeled_preds[:,0], lab_real_preds) + \\\n","                       bce_loss(unlabeled_preds[:,0], unlab_real_preds)\n","            total_vae_loss = unsup_loss + transductive_loss + adversary_param * dsc_loss\n","            \n","            optimizers['vae'].zero_grad()\n","            total_vae_loss.backward()\n","            optimizers['vae'].step()\n","\n","            # sample new batch if needed to train the adversarial network\n","            if count < (num_vae_steps - 1):\n","                labeled_imgs, _ = next(labeled_data)\n","                unlabeled_imgs = next(unlabeled_data)[0]\n","                \n","                labeled_imgs = labeled_imgs.to(device)\n","                unlabeled_imgs = unlabeled_imgs.to(device)\n","                labels = labels.to(device)                \n","\n","        # Discriminator step\n","        for count in range(num_adv_steps):\n","            with torch.no_grad():\n","                _, _, mu, _ = vae(r_l_s,labeled_imgs)\n","                _, _, unlab_mu, _ = vae(r_u_s,unlabeled_imgs)\n","            \n","            labeled_preds = discriminator(r_l,mu)\n","            unlabeled_preds = discriminator(r_u,unlab_mu)\n","            \n","            lab_real_preds = torch.ones(labeled_imgs.size(0))\n","            unlab_fake_preds = torch.zeros(unlabeled_imgs.size(0))\n","\n","            lab_real_preds = lab_real_preds.to(device)\n","            unlab_fake_preds = unlab_fake_preds.to(device)            \n","            \n","            dsc_loss = bce_loss(labeled_preds[:,0], lab_real_preds) + \\\n","                       bce_loss(unlabeled_preds[:,0], unlab_fake_preds)\n","\n","            optimizers['discriminator'].zero_grad()\n","            dsc_loss.backward()\n","            optimizers['discriminator'].step()\n","\n","            # sample new batch if needed to train the adversarial network\n","            if count < (num_adv_steps-1):\n","                labeled_imgs, _ = next(labeled_data)\n","                unlabeled_imgs = next(unlabeled_data)[0]\n","\n","                labeled_imgs = labeled_imgs.to(device)\n","                unlabeled_imgs = unlabeled_imgs.to(device)\n","                labels = labels.to(device)                \n","                \n","            if iter_count % 50 == 0:\n","                # print(\"Iteration: \" + str(iter_count) + \"  vae_loss: \" + str(total_vae_loss.item()) + \" dsc_loss: \" +str(dsc_loss.item()))\n","                args['writer-train'].add_scalar(str(cycle) + ' Total VAE Loss ',\n","                        total_vae_loss.item(), iter_count)\n","                args['writer-train'].add_scalar(str(cycle) + ' Total DSC Loss ',\n","                        dsc_loss.item(), iter_count)\n","                \n","# Select the indices of the unlablled data according to the methods\n","def query_samples(model, method, data_unlabeled, subset, labeled_set, cycle, args):\n","    \n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","    if method == 'TA-VAAL':\n","        # Create unlabeled dataloader for the unlabeled subset\n","        unlabeled_loader = DataLoader(data_unlabeled, batch_size=args['BATCH'], \n","                                    sampler=SubsetSequentialSampler(subset), \n","                                    pin_memory=True)\n","        labeled_loader = DataLoader(data_unlabeled, batch_size=args['BATCH'], \n","                                    sampler=SubsetSequentialSampler(labeled_set), \n","                                    pin_memory=True)\n","        \n","        vae = VAE()\n","        discriminator = Discriminator(32)\n","     \n","        models      = {'backbone': model['backbone'], 'module': model['module'], 'vae': vae, 'discriminator': discriminator}\n","        \n","        optim_vae = optim.Adam(vae.parameters(), lr=5e-4)\n","        optim_discriminator = optim.Adam(discriminator.parameters(), lr=5e-4)\n","        optimizers = {'vae': optim_vae, 'discriminator':optim_discriminator}\n","\n","        train_vaal(models,optimizers, labeled_loader, unlabeled_loader, cycle+1, args)\n","        task_model = models['backbone']\n","        ranker = models['module']        \n","        all_preds, all_indices = [], []\n","\n","        for images, _, indices in unlabeled_loader:                       \n","            images = images.to(device)\n","            with torch.no_grad():\n","                _,_,features = task_model(images)\n","                r = ranker(features)\n","                _, _, mu, _ = vae(torch.sigmoid(r),images)\n","                preds = discriminator(r,mu)\n","\n","            preds = preds.cpu().data\n","            all_preds.extend(preds)\n","            all_indices.extend(indices)\n","\n","        all_preds = torch.stack(all_preds)\n","        all_preds = all_preds.view(-1)\n","        # need to multiply by -1 to be able to use torch.topk \n","        all_preds *= -1\n","        # select the points which the discriminator things are the most likely to be unlabeled\n","        _, arg = torch.sort(all_preds) \n","        \n","        torch.save(vae, 'saved_history/models/vae-' + args['group'] +'cycle-'+str(cycle)+'.pth')\n","        torch.save(discriminator, 'saved_history/models/discriminator-' + args['group'] +'cycle-'+str(cycle)+'.pth')\n","        \n","    return arg"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2023-06-08T15:57:48.603398Z","iopub.status.busy":"2023-06-08T15:57:48.602733Z","iopub.status.idle":"2023-06-08T15:57:48.633418Z","shell.execute_reply":"2023-06-08T15:57:48.632548Z","shell.execute_reply.started":"2023-06-08T15:57:48.603367Z"},"trusted":true},"outputs":[],"source":["# Main\n","def main(args):\n","    \n","    targets = ['image_id', 'Bald', 'Bangs', 'Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair', 'Receding_Hairline', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat' ]\n","\n","    if args['dataset'] == 'celeba':\n","        args['NO_CLASSES'] = len(targets) - 1\n","    \n","    method = args['method_type']\n","    results = open('results_'+str(args['method_type'])+\"_\"+args['dataset'] +'_main'+str(args['CYCLES'])+str(args['total'])+'.txt','w')\n","    print(\"Dataset: %s\"%args['dataset'])\n","    print(\"Method type:%s\"%method)\n","    \n","    if args['total']:\n","        args['TRIALS'] = 1\n","        args['CYCLES'] = 1\n","    else:\n","        args['CYCLES'] = args['CYCLES']\n","        \n","    # fields\n","    fields = ['cycle', 'epoch', 'total_train_pred_acc']\n","    for i in range(len(targets)-1):\n","        fields.append('train_pred_'+ str(i+1) + '_acc')\n","    fields.append('total_val_pred_acc')\n","    for i in range(len(targets)-1):\n","        fields.append('val_pred_'+ str(i+1) + '_acc')\n","    \n","    # name of csv file \n","    filename = \"results_\" + args['group'] + '_' + str(args['attempt']) + \".csv\"\n","\n","    # writing to csv file \n","    with open(filename, 'a') as csvfile: \n","        # creating a csv writer object \n","        csvwriter = csv.writer(csvfile) \n","        # writing the fields \n","        csvwriter.writerow(fields)     \n","        \n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n","    for trial in range(args['trials']):\n","        \n","        # Load training and testing dataset\n","        data_train, data_unlabeled, data_test, adden, NO_CLASSES, no_train = load_dataset(args, targets)\n","        print('The entire datasize is {}'.format(len(data_train)))  \n","        \n","        ADDENDUM = adden\n","        NUM_TRAIN = no_train\n","        indices = list(range(NUM_TRAIN))\n","        random.shuffle(indices)\n","\n","        if args['total']:\n","            labeled_set= indices\n","        else:\n","            labeled_set = indices[:ADDENDUM]\n","            unlabeled_set = [x for x in indices if x not in labeled_set]\n","        \n","        train_loader = DataLoader(data_train, batch_size=args['BATCH'], \n","                                     sampler=SubsetRandomSampler(labeled_set), \n","                                     pin_memory=True, drop_last=True)\n","        test_loader  = DataLoader(data_test, batch_size=args['BATCH'],  drop_last=True)\n","        dataloaders  = {'train': train_loader, 'test': test_loader}\n","        \n","        np.save(\"saved_history/initial-labelled-\" + args['group'] + \".npy\", np.array(labeled_set))\n","        print('Len: ', len(train_loader), 'Len:', len(test_loader))\n","        \n","        for cycle in range(args['CYCLES']):\n","            print(cycle)\n","            \n","            if not args['total']:\n","                random.shuffle(unlabeled_set)\n","                subset = unlabeled_set[:args['SUBSET']]\n","                            \n","            resnet18    = ResNet18(num_classes=args['NO_CLASSES'], expansion=9).to(device)\n","            if method == 'lloss' or 'TA-VAAL':\n","                loss_module = LossNet().to(device)\n","\n","            models      = {'backbone': resnet18}\n","            if method =='lloss' or 'TA-VAAL':\n","                models = {'backbone': resnet18, 'module': loss_module}\n","            torch.backends.cudnn.benchmark = True\n","                        \n","            labels_l = []\n","            for i in labeled_set:\n","                labels_l.append(data_train[i][1])\n","\n","            counts = np.array(labels_l).reshape((-1, args['NO_CLASSES'])).sum(axis=0)\n","\n","            wts_0 = (counts / len(labels_l))\n","            wts_1 = 1 - (counts / len(labels_l))\n","            print(wts_0)\n","            \n","            # Loss, criterion and scheduler (re)initialization\n","            criterion      = [nn.NLLLoss(reduction='none', weight = torch.tensor([wts_0[i], wts_1[i]]).to(device)).float() for i in range(args['NO_CLASSES'])]\n","            optim_backbone = optim.SGD(models['backbone'].parameters(), lr=args['LR'], \n","                momentum=args['MOMENTUM'], weight_decay=args['WDECAY'])\n"," \n","            sched_backbone = lr_scheduler.MultiStepLR(optim_backbone, milestones=args['MILESTONES'])\n","            optimizers = {'backbone': optim_backbone}\n","            schedulers = {'backbone': sched_backbone}\n","            if method == 'lloss' or 'TA-VAAL':\n","                optim_module   = optim.SGD(models['module'].parameters(), lr=args['LR'], \n","                    momentum=args['MOMENTUM'], weight_decay=args['WDECAY'])\n","                sched_module   = lr_scheduler.MultiStepLR(optim_module, milestones=args['MILESTONES'])\n","                optimizers = {'backbone': optim_backbone, 'module': optim_module}\n","                schedulers = {'backbone': sched_backbone, 'module': sched_module} \n","\n","            # Training and testing\n","            train(models, method, criterion, optimizers, schedulers, dataloaders, cycle+1, args)\n","            acc, cm, individual_predictor_loss, total_lloss, total_loss = test(models, args['no_of_epochs'], method, criterion, dataloaders, args, mode='test')\n","            torch.save(models['backbone'], 'saved_history/models/predictor-backbone-' + args['group'] + 'cycle-'+str(cycle+1)+'.pth')\n","            torch.save(models['module'], 'saved_history/models/predictor-module-'+args['group']+'cycle-'+str(cycle+1)+'.pth')\n","            \n","            print('Trial {}/{} || Cycle {}/{} || Label set size {}'.format(trial+1, args['trials'], cycle+1, args['CYCLES'], len(labeled_set)))\n","\n","            for i in range(args['NO_CLASSES']):\n","                print('val_accuracy', i, ':', acc[i])\n","                print(cm[i])\n","                print(\"\")\n","            \n","            if cycle == (args['CYCLES']-1):\n","                # Reached final training cycle\n","                print(\"Finished.\")\n","                break\n","                \n","            # Get the indices of the unlabeled samples to train on next cycle\n","            arg = query_samples(models, method, data_unlabeled, subset, labeled_set, cycle, args)\n","            \n","            # Update the labeled dataset and the unlabeled dataset, respectively\n","            new_list = list(torch.tensor(subset)[arg][:args['INCREMENTAL']].numpy())\n","            labeled_set += list(torch.tensor(subset)[arg][-args['INCREMENTAL']:].numpy())\n","            listd = list(torch.tensor(subset)[arg][:-args['INCREMENTAL']].numpy()) \n","            unlabeled_set = listd + unlabeled_set[args['SUBSET']:]\n","            print(len(labeled_set), min(labeled_set), max(labeled_set))\n","            \n","            np.save(\"saved_history/labelled-\" + args['group'] + str(cycle) + \".npy\", np.array(labeled_set))\n","            \n","            # Create a new dataloader for the updated labeled dataset\n","            dataloaders['train'] = DataLoader(data_train, batch_size=args['BATCH'], \n","                                            sampler=SubsetRandomSampler(labeled_set), \n","                                            pin_memory=True, drop_last=True)\n","\n","    results.close()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2023-06-08T15:57:48.636902Z","iopub.status.busy":"2023-06-08T15:57:48.636573Z","iopub.status.idle":"2023-06-08T15:57:48.645059Z","shell.execute_reply":"2023-06-08T15:57:48.644232Z","shell.execute_reply.started":"2023-06-08T15:57:48.636856Z"},"trusted":true},"outputs":[],"source":["splits = [0.1,0.15,0.2,0.25,0.3,0.35,0.4]"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2023-06-08T15:57:48.648371Z","iopub.status.busy":"2023-06-08T15:57:48.648105Z","iopub.status.idle":"2023-06-08T15:57:48.654295Z","shell.execute_reply":"2023-06-08T15:57:48.653441Z","shell.execute_reply.started":"2023-06-08T15:57:48.648349Z"},"trusted":true},"outputs":[],"source":["# change targets in main\n","# change attr. group name and in writer-train/val\n","# change len(targets) in args NO_CLASSES\n","# change outputs in Predictor \n","# Change lr (if)\n","# Change epochs (if)\n","# Change attempt number (if) and change attempt in writer-train/val"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2023-06-08T15:57:48.655728Z","iopub.status.busy":"2023-06-08T15:57:48.655429Z","iopub.status.idle":"2023-06-08T15:57:48.665805Z","shell.execute_reply":"2023-06-08T15:57:48.664781Z","shell.execute_reply.started":"2023-06-08T15:57:48.655682Z"},"trusted":true},"outputs":[],"source":["args = {\n","    'method_type': 'TA-VAAL', \n","    'dataset': 'celeba',\n","    'group': 'head',\n","    'total': False,\n","    'trials': 1,\n","    'CYCLES': 7,\n","    'ADDENDUM': 16277, ## num of images increment\n","    'INCREMENTAL': 8138, ## \n","    'BATCH': 32, ## \n","    'SUBSET': 20000, ##\n","    'NO_CLASSES': 11, ##\n","    'LR': 1e-4,\n","    'WDECAY': 5e-4,\n","    'MOMENTUM': 0.9,\n","    'MILESTONES': [160, 240],\n","    'MARGIN': 1.0,\n","    'WEIGHT': 1.0,\n","    'no_of_epochs': 20,\n","    'lambda_loss': 1.2,\n","    's_margin': 0.1,\n","    'hidden_units': 128,\n","    'dropout_rate': 0.3,\n","    'EPOCHV':10, \n","    'EPOCHL': 12, \n","    'attempt': 1,\n","    'writer-train': SummaryWriter('logs/TAVAAL-CELEBA-head-1-Train'),\n","    'writer-val': SummaryWriter('logs/TAVAAL-CELEBA-head-1-Val')\n","}"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2023-06-08T15:57:48.667522Z","iopub.status.busy":"2023-06-08T15:57:48.667193Z","iopub.status.idle":"2023-06-08T15:57:48.672344Z","shell.execute_reply":"2023-06-08T15:57:48.671385Z","shell.execute_reply.started":"2023-06-08T15:57:48.667492Z"},"trusted":true},"outputs":[],"source":["#Mouth       4 'Big_Lips', 'Mouth_Slightly_Open', 'Smiling', 'Wearing_Lipstick' 19 3 5e-4 1e-4\n","#Eyes        5 'Arched_Eyebrows', 'Bag_Under_Eyes', 'Bushy_Eyebrows', 'Eyeglasses', 'Narrow_Eyes' 19 1 5e-4 1e-4\n","#Face        6 'Attractive', 'Blurry', 'Heavy_Makeup', 'Oval_Face', 'Pale_Skin', 'Young'\n","#Facial Hair 5 '5_o_clock shadow', 'Goatee', 'Moustache', 'No_Beard', 'Sideburns'\n","#Head       11 'Bald', 'Bangs', 'Black_Hair', 'Blond_Hair', 'Brown_Hair', 'Gray_Hair', 'Receding_Hairline', 'Straight_Hair', 'Wavy_Hair', 'Wearing_Earrings', 'Wearing_Hat'"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2023-06-08T15:57:48.677800Z","iopub.status.busy":"2023-06-08T15:57:48.677028Z","iopub.status.idle":"2023-06-08T15:57:49.637889Z","shell.execute_reply":"2023-06-08T15:57:49.636610Z","shell.execute_reply.started":"2023-06-08T15:57:48.677770Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory ‘logs’: File exists\n","mkdir: cannot create directory ‘saved_history’: File exists\n","mkdir: cannot create directory ‘saved_history/models’: File exists\n"]}],"source":["!mkdir logs saved_history saved_history/models"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2023-06-08T15:57:49.641474Z","iopub.status.busy":"2023-06-08T15:57:49.640810Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Dataset: cifar10\n","Method type:TA-VAAL\n"]},{"ename":"UnboundLocalError","evalue":"local variable 'data_train' referenced before assignment","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main(args)\n","Cell \u001b[0;32mIn[11], line 42\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     38\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m) \n\u001b[1;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m trial \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(args[\u001b[39m'\u001b[39m\u001b[39mtrials\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[1;32m     40\u001b[0m     \n\u001b[1;32m     41\u001b[0m     \u001b[39m# Load training and testing dataset\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     data_train, data_unlabeled, data_test, adden, NO_CLASSES, no_train \u001b[39m=\u001b[39m load_dataset(args, targets)\n\u001b[1;32m     43\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mThe entire datasize is \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(data_train)))  \n\u001b[1;32m     45\u001b[0m     ADDENDUM \u001b[39m=\u001b[39m adden\n","Cell \u001b[0;32mIn[6], line 29\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(args, targets)\u001b[0m\n\u001b[1;32m     26\u001b[0m     NUM_TRAIN \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data_train)        \n\u001b[1;32m     27\u001b[0m     no_train \u001b[39m=\u001b[39m NUM_TRAIN\n\u001b[0;32m---> 29\u001b[0m \u001b[39mreturn\u001b[39;00m data_train, data_unlabeled, data_test, adden, NO_CLASSES, no_train\n","\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'data_train' referenced before assignment"]}],"source":["main(args)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# dataloader -----------> Done\n","# model_architecture \n","# model ----------------> Done\n","# query_samples \n","# hyperparameters\n","\n","# import torch, gc\n","# gc.collect()\n","# torch.cuda.empty_cache()\n","\n","# !nvidia-smi"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# summary(a, [(3,96,96)])\n","# train_iterations = int( (args['INCREMENTAL']*cycle+ args['SUBSET']) * args['EPOCHV'] / args['BATCH'] )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# cycle, epoch, total_train_pred_acc, train_pred_1_acc, train_pred_2_acc, train_pred_3_acc, train_pred_4_acc, \n","# total_val_pred_acc, val_pred_1_acc, val_pred_2_acc, val_pred_3_acc, val_pred_4_acc"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
