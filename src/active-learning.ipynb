{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm \u001b[39mas\u001b[39;00m progressbar\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm as progressbar\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "from torchvision.datasets import CIFAR100, CIFAR10\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebA(Dataset):\n",
    "    ...\n",
    "\n",
    "class CelebAMask(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, eval_file, targets, split='train', transform=None):\n",
    "\n",
    "        self.attr = pd.read_csv(csv_file)\n",
    "        self.eval_partition = pd.read_csv(eval_file)\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.targets = targets\n",
    "        \n",
    "        self.attr = self.attr[self.targets]\n",
    "        self.attr = self.attr.replace(-1, 0)\n",
    "        self.attr = self.attr.set_index('image_id')\n",
    "        self.eval_partition = self.eval_partition.set_index('image_id')\n",
    "        self.attr = self.attr.join(self.eval_partition)\n",
    "        self.attr['image_id'] = self.attr.index\n",
    "        \n",
    "        if self.split == 'train':\n",
    "            self.attr = self.attr.loc[self.attr['partition']==0]\n",
    "            self.attr = self.attr.drop('partition', axis=1)\n",
    "        else:\n",
    "            self.attr = self.attr.loc[self.attr['partition']==2]\n",
    "            self.attr = self.attr.drop('partition', axis=1)\n",
    "            \n",
    "        self.attr = self.attr[self.targets] \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.attr)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "    \n",
    "        img_name = self.root_dir + self.attr.iloc[idx, 0]\n",
    "        \n",
    "        image = cv2.imread(img_name) / 255.0\n",
    "        attr = self.attr.iloc[idx, 1:]\n",
    "        attr = np.array([attr])\n",
    "        attr = attr.astype('float')\n",
    "        \n",
    "        image = cv2.resize(image, dsize=(96, 96), interpolation=cv2.INTER_AREA)\n",
    "        image = image.reshape(image.shape[2], image.shape[0], -1)\n",
    "        sample = (torch.tensor(image).float(), attr)\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "    \n",
    "class SubsetSequentialSampler(torch.utils.data.Sampler):\n",
    "    r\"\"\"Samples elements sequentially from a given list of indices, without replacement.\n",
    "\n",
    "    Arguments:\n",
    "        indices (sequence): a sequence of indices\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, indices):\n",
    "        self.indices = indices\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (self.indices[i] for i in range(len(self.indices)))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, dataset_name, train_flag, targets = None, transf=None):\n",
    "        self.dataset_name = dataset_name\n",
    "        \n",
    "        if self.dataset_name == \"cifar10\":\n",
    "            self.cifar10 = CIFAR10('../input/cifar10', train=train_flag, \n",
    "                                    download=True, transform=transf)\n",
    "            \n",
    "        if self.dataset_name == \"celeba\":\n",
    "            attr_file = '../input/celeba-dataset/list_attr_celeba.csv'\n",
    "            eval_file = '../input/celeba-dataset/list_eval_partition.csv'\n",
    "            path = '../input/celeba-dataset/img_align_celeba/img_align_celeba/'            \n",
    "            self.celeba = CelebA(attr_file, path, eval_file, targets, split='train')  \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.dataset_name == \"cifar10\":\n",
    "            data, target = self.cifar10[index]\n",
    "            \n",
    "        if self.dataset_name == \"celeba\":\n",
    "            data, target = self.celeba[index]\n",
    "        \n",
    "        return data, target, index\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.dataset_name == \"cifar10\":\n",
    "            return len(self.cifar10)\n",
    "        \n",
    "        if self.dataset_name == \"celeba\":\n",
    "            return len(self.celeba)\n",
    "\n",
    "class CityScapes(Dataset):\n",
    "    ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PT4AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_planes, planes, expansion, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.expansion = expansion\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10, expansion=1):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], 1, stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], 1, stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], 1, stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], 1, stride=2)\n",
    "        \n",
    "        self.linear1 = nn.Linear(512*expansion, 2)\n",
    "        self.linear2 = nn.Linear(512*expansion, 2)\n",
    "        self.linear3 = nn.Linear(512*expansion, 2)\n",
    "        self.linear4 = nn.Linear(512*expansion, 2)\n",
    "        self.linear5 = nn.Linear(512*expansion, 2)\n",
    "        self.linear6 = nn.Linear(512*expansion, 2)\n",
    "        self.linear7 = nn.Linear(512*expansion, 2)\n",
    "        self.linear8 = nn.Linear(512*expansion, 2)\n",
    "        self.linear9 = nn.Linear(512*expansion, 2)\n",
    "        self.linear10 = nn.Linear(512*expansion, 2)\n",
    "        self.linear11 = nn.Linear(512*expansion, 2)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, expansion, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, expansion, stride))\n",
    "            self.in_planes = planes * expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out1 = self.layer1(out)\n",
    "        out2 = self.layer2(out1)\n",
    "        out3 = self.layer3(out2)\n",
    "        out4 = self.layer4(out3)\n",
    "        out = F.avg_pool2d(out4, 4)\n",
    "        outf = out.view(out.size(0), -1)\n",
    "        outt1 = self.linear1(outf)\n",
    "        outt2 = self.linear2(outf)\n",
    "        outt3 = self.linear3(outf)\n",
    "        outt4 = self.linear4(outf)\n",
    "        outt5 = self.linear5(outf)\n",
    "        outt6 = self.linear6(outf)\n",
    "        outt7 = self.linear7(outf)\n",
    "        outt8 = self.linear8(outf)\n",
    "        outt9 = self.linear9(outf)\n",
    "        outt10 = self.linear10(outf)\n",
    "        outt11 = self.linear11(outf)\n",
    "\n",
    "        return [outt1, outt2, outt3, outt4, outt5, outt6, outt7, outt8, outt9, outt10, outt11], outf, [out1, out2, out3, out4]\n",
    "\n",
    "def ResNet18(num_classes = 10, expansion = 1):\n",
    "    return ResNet(BasicBlock, [2,2,2,2], num_classes, expansion)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alexp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
