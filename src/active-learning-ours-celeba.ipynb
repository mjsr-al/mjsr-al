{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2022-03-02T17:25:58.775347Z","iopub.status.busy":"2022-03-02T17:25:58.775019Z","iopub.status.idle":"2022-03-02T17:25:58.783345Z","shell.execute_reply":"2022-03-02T17:25:58.78217Z","shell.execute_reply.started":"2022-03-02T17:25:58.775318Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2023-06-23 09:14:42.782453: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n","2023-06-23 09:14:42.784299: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-06-23 09:14:42.831006: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n","2023-06-23 09:14:42.832000: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2023-06-23 09:14:43.506536: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"]}],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import random as random\n","\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.layers import Dense, Dot, Multiply, Softmax, Reshape, Input, Flatten, Conv2DTranspose, Conv2D, GlobalAveragePooling2D,BatchNormalization, Lambda, MaxPooling2D, ReLU, Dropout, Concatenate\n","from tensorflow.keras.optimizers import Adam, SGD\n","from tensorflow.keras.models import Model, model_from_json\n","from tensorflow.keras import regularizers\n","from tensorflow.keras.utils import plot_model, to_categorical\n","\n","from tensorflow.keras.callbacks import ModelCheckpoint, LambdaCallback, ReduceLROnPlateau, CSVLogger, TensorBoard\n","from tensorflow.keras import backend as K\n","\n","import datetime\n","from tensorflow.python.framework import ops\n","from tensorflow.python.ops import math_ops\n","from tensorflow.python.keras import backend as K\n","import tensorflow.keras as keras\n","import json"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T17:26:00.581123Z","iopub.status.busy":"2022-03-02T17:26:00.580746Z","iopub.status.idle":"2022-03-02T17:26:00.585187Z","shell.execute_reply":"2022-03-02T17:26:00.584084Z","shell.execute_reply.started":"2022-03-02T17:26:00.581092Z"},"trusted":true},"outputs":[],"source":["# Attractive, Blurry, Heavy_Makeup, Oval_Face, Pale_Skin, Young\n","# Big_Lips, Mouth_Slightly_Open, Smiling, Wearing_Lipstick\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T17:26:03.385321Z","iopub.status.busy":"2022-03-02T17:26:03.384949Z","iopub.status.idle":"2022-03-02T17:26:03.39159Z","shell.execute_reply":"2022-03-02T17:26:03.390717Z","shell.execute_reply.started":"2022-03-02T17:26:03.385285Z"},"trusted":true},"outputs":[],"source":["hyperparameters={\n","    'targets': ['image_id', 'Big_Lips', 'Mouth_Slightly_Open', 'Smiling', 'Wearing_Lipstick'],  ### MODIFY \n","    'height': 224, \n","    'width': 192 ,\n","    'channels': 3, \n","    'batch_size': 16, \n","    'epochs': 1, \n","    'num_tasks': 4, ### MODIFY\n","    'initializer': 'he_uniform', \n","    'reg_lambda': 1e-3, \n","    'output': [2]*4, ### MODIFY === [2]*number of tasks\n","    'lr': 1e-5, ### MODIFY\n","    'is_trained': False, \n","    'dropout_prob': 0.3,\n","    'enable_cs': False, \n","    'enable_sluice': False,\n","    'initial_percent':0.1,\n","    'initial_train_epoch': 5, ### NEED TO SEE----- Done\n","    'increment_train_epoch':2, ### NEED TO SEE --- Done\n","    'uncertainity_repeat': 5, \n","    'num_uncertain_elements': 5008, \n","    'additional_epoch': 1, ### NEED TO SEE\n","    'pretraining_epochs': 2, ### -- make it 2 or originally 1 - Done\n","    'train_initial_batches': 8138, #6103, ### NEED TO SEE ------------- Done\n","    'enable_additional': False,\n","    'additional_attr_count':2,\n","    'all_updates':False,\n","    'initial_percent_val': 0.6 ### Added Now\n","    }"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T17:26:09.808869Z","iopub.status.busy":"2022-03-02T17:26:09.808484Z","iopub.status.idle":"2022-03-02T17:26:09.817437Z","shell.execute_reply":"2022-03-02T17:26:09.816608Z","shell.execute_reply.started":"2022-03-02T17:26:09.808833Z"},"trusted":true},"outputs":[],"source":["def preprocess(hyperparameters, attr, eval_partition):\n","    attr = attr[hyperparameters['targets']]\n","    attr = attr.replace(-1, 0)\n","    attr = attr.set_index('image_id')\n","    eval_partition = eval_partition.set_index('image_id')\n","    attr = attr.join(eval_partition)\n","    attr['image_id'] = attr.index\n","    \n","    for column in attr.columns[:-2]:\n","        k = to_categorical(attr[column])\n","        attr = attr.drop(column, axis=1)\n","        attr[column] = k.tolist()\n","\n","    train = attr.loc[attr['partition']==0]\n","    val = attr.loc[attr['partition']==1]\n","    test = attr.loc[attr['partition']==2]\n","    \n","    train = train.drop('partition', axis=1)\n","    val = val.drop('partition', axis=1)\n","    test = test.drop('partition', axis=1)\n","    \n","    train = train[:(len(train)//hyperparameters['batch_size'])*hyperparameters['batch_size']]\n","    val = val[:(len(val)//hyperparameters['batch_size'])*hyperparameters['batch_size']]\n","    test = test[:(len(test)//hyperparameters['batch_size'])*hyperparameters['batch_size']]\n","    \n","    return (train, val, test)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T17:26:10.121018Z","iopub.status.busy":"2022-03-02T17:26:10.120675Z","iopub.status.idle":"2022-03-02T17:26:10.131707Z","shell.execute_reply":"2022-03-02T17:26:10.130721Z","shell.execute_reply.started":"2022-03-02T17:26:10.120988Z"},"trusted":true},"outputs":[],"source":["def load_generator(df, shuffle=True):\n","    \n","    # image_id = '000014.jpg'\n","    image_path = '../input/celeba-dataset/img_align_celeba/img_align_celeba/'\n","    data_gen = ImageDataGenerator(rescale=1/255.0)\n","    \n","    generator = data_gen.flow_from_dataframe(dataframe = df, \n","                                     directory=image_path, \n","                                     x_col = 'image_id', \n","                                     y_col=hyperparameters['targets'][1:], \n","                                     class_mode = 'multi_output',\n","                                     target_size=(hyperparameters['height'], hyperparameters['width']), \n","                                     batch_size = hyperparameters['batch_size'],shuffle=shuffle)\n","    \n","    return generator\n","\n","def generate_generator_multiple(generator,dir1, dir2, df1,df2,batch_size, img_height,img_width,shuffle=True):\n","    # labelled \n","    genX1 = generator.flow_from_dataframe(df1, dir1,\n","                                          x_col = 'image_id', \n","                                          y_col=hyperparameters['targets'][1:], \n","                                          class_mode = 'multi_output',\n","                                          target_size=(img_height,img_width), \n","                                          batch_size = hyperparameters['batch_size'],\n","                                          shuffle=shuffle)\n","    # train --- make it train generator\n","    genX2 = generator.flow_from_dataframe(df2, dir2,\n","                                          x_col = 'image_id', \n","                                          y_col=hyperparameters['targets'][1:], \n","                                          class_mode = 'multi_output',\n","                                          target_size=(img_height, img_width), \n","                                          batch_size = batch_size,\n","                                          shuffle=shuffle)\n","    while True:\n","            X1,y1 = genX1.next()\n","            X2,_ = genX2.next()\n","            yield X1, X2, y1  #Yield both images and their mutual label     "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T17:26:10.52471Z","iopub.status.busy":"2022-03-02T17:26:10.524357Z","iopub.status.idle":"2022-03-02T17:26:10.531558Z","shell.execute_reply":"2022-03-02T17:26:10.530579Z","shell.execute_reply.started":"2022-03-02T17:26:10.524678Z"},"trusted":true},"outputs":[],"source":["# changed input to 360+360\n","def discriminator():\n","    \n","    X = Input((720,), name='input_disc')\n","    \n","    x = Dense(units = 512, activation = None, name='dense_1_disc')(X)\n","    x = BatchNormalization(name = 'bn_1_disc')(x)\n","    x = ReLU(name='relu_1_disc')(x)\n","    x = Dense(units = 512, activation = None, name='dense_2_disc')(x)\n","    x = BatchNormalization(name = 'bn_2_disc')(x)\n","    x = ReLU(name='relu_2_disc')(x)\n","    x = Dense(units = 1, activation = 'sigmoid', name='output_disc')(x)\n","    \n","    return X,x"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T17:26:10.724237Z","iopub.status.busy":"2022-03-02T17:26:10.723918Z","iopub.status.idle":"2022-03-02T17:26:10.732338Z","shell.execute_reply":"2022-03-02T17:26:10.73143Z","shell.execute_reply.started":"2022-03-02T17:26:10.724208Z"},"trusted":true},"outputs":[],"source":["def addConvBlock(num_filters, kernel_size, hyperparameters, pool_size, tops, stride, pad, pool_stride, isPool , i):\n","    \n","    for task_id in range(hyperparameters['num_tasks']+hyperparameters['enable_additional']*hyperparameters['additional_attr_count']):\n","        \n","        tops[task_id] = Conv2D(num_filters, kernel_size=kernel_size, name = 'conv'+str(i)+'_'+str(task_id),  strides=(stride, stride), padding = pad, kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l1(hyperparameters['reg_lambda']))(tops[task_id])\n","        tops[task_id] = ReLU(name='relu'+str(i)+'_'+str(task_id))(tops[task_id])\n","\n","        if (isPool==True):\n","            name = 'pool'+str(i)+'_'+str(task_id)\n","            tops[task_id] = MaxPooling2D(pool_size=(pool_size, pool_size), name = name, strides = (pool_stride, pool_stride), padding='valid')(tops[task_id])\n","            \n","    return tops"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T17:26:11.241223Z","iopub.status.busy":"2022-03-02T17:26:11.240929Z","iopub.status.idle":"2022-03-02T17:26:11.266276Z","shell.execute_reply":"2022-03-02T17:26:11.265236Z","shell.execute_reply.started":"2022-03-02T17:26:11.241198Z"},"trusted":true},"outputs":[],"source":["def combined_model():\n","    \n","    num_tasks = hyperparameters['num_tasks']+hyperparameters['enable_additional']*hyperparameters['additional_attr_count']\n","    \n","    x,y,z = hyperparameters['height'], hyperparameters['width'], hyperparameters['channels']\n","    X = Input((x,y,z), name = 'input_predictor_'+\"_\".join(str(num) for num in range(num_tasks)))\n","    tops = [X]*num_tasks\n","\n","# ------------------------------------------- Block 1 BEGINS ------------------------------------\n","\n","    tops = addConvBlock(40, 5, hyperparameters, 3, tops, 1, 'same', 2, True, 1)\n","    if hyperparameters[\"enable_cs\"]:\n","        cs1 = CrossStitch(num_tasks,1, hyperparameters, True)(tops) \n","        tops = tf.unstack(cs1, axis=0)\n","    \n","    tops = addConvBlock(60, 5, hyperparameters, 3, tops, 1, 'same', 2, True, 2)\n","    if hyperparameters[\"enable_cs\"]:\n","        cs2 = CrossStitch(num_tasks, 2, hyperparameters, True)(tops)\n","        tops = tf.unstack(cs2, axis=0)\n","    \n","    tops = addConvBlock(80, 3, hyperparameters, 3, tops, 1, 'same', 2, True, 3)\n","    if hyperparameters[\"enable_cs\"]:\n","        cs3 = CrossStitch(num_tasks, 3, hyperparameters, True)(tops)\n","        tops = tf.unstack(cs3, axis=0)\n","        \n","    \n","    tops = addConvBlock(100, 3, hyperparameters, 3, tops, 1, 'same', 2, True,  4)  \n","    if hyperparameters[\"enable_cs\"]:\n","        cs4 = CrossStitch(num_tasks, 4, hyperparameters, True)(tops)\n","        tops = tf.unstack(cs4, axis=0)\n","\n","# ------------------------------------------- Block 4 ENDS ------------------------------------\n","    \n","# ------------------------------------------- Block 5 BEGINS ------------------------------------\n","    tops = addConvBlock(140, 2, hyperparameters, 3, tops, 1, 'same', 2, True, 5)\n","    if hyperparameters[\"enable_cs\"]:\n","        cs5 = CrossStitch(num_tasks, 5, hyperparameters, True)(tops)\n","        tops = tf.unstack(cs5, axis=0)\n","\n","# ------------------------------------------- Block 5 ENDS ------------------------------------\n","    \n","    latents=[]\n","    weights=[]\n","    for task_id in range(num_tasks): \n","\n","        tops[task_id] = Flatten(name = 'flat'+'_'+str(task_id))(tops[task_id])\n","\n","        tops[task_id] = Dense(units = 720, name = 'dense0'+'_'+str(task_id), kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(tops[task_id])\n","        if (task_id==0):\n","            tops[task_id] = ReLU(name='re_lu0'+'_'+str(0))(tops[task_id])\n","        else:\n","            tops[task_id] = ReLU(name='re_lu0'+'_'+str(task_id)+'_'+str(task_id))(tops[task_id])\n","            \n","            \n","        tops[task_id] = Dense(units = 360, name = 'dense1'+'_'+str(task_id), kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(tops[task_id])\n","        if (task_id==0):\n","            tops[task_id] = ReLU(name='re_lu'+'_'+str(0))(tops[task_id])\n","        else:\n","            tops[task_id] = ReLU(name='re_lu'+'_'+str(task_id)+'_'+str(task_id))(tops[task_id])\n","                                                        \n","        latents.append(tops[task_id])\n","        weights.append(tops[task_id])\n","        \n","        tops[task_id] = Dense(units = 180, name = 'dense2'+'_'+str(task_id), kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(tops[task_id])\n","        if (task_id==0):\n","            tops[task_id] = ReLU(name='re_lu2'+'_'+str(0))(tops[task_id])\n","        else:\n","            tops[task_id] = ReLU(name='re_lu2'+'_'+str(task_id)+'_'+str(task_id))(tops[task_id])\n","        \n","\n","    # added joined weights\n","    joint = Concatenate(name='joined_1')(weights)\n","    joint = Dense(units = 360, name=\"joined_2\",kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(joint)\n","    joint = Dense(units = num_tasks, name=\"joined_3\",activation = 'softmax', kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(joint)\n","    tops = taskEmbeddings(5)([tops, joint])\n","       \n","    for task_id in range(num_tasks):\n","        tops[task_id] = Dense(units = 90, name = 'dense3'+'_'+str(task_id), kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(tops[task_id])\n","        if (task_id==0):\n","            tops[task_id] = ReLU(name='re_lu3'+'_'+str(0))(tops[task_id])\n","        else:\n","            tops[task_id] = ReLU(name='re_lu3'+'_'+str(task_id)+'_'+str(task_id))(tops[task_id])\n","    \n","\n","    for task_id in range(num_tasks): \n","        tops[task_id] = Dense(units = 2, name='output'+'_'+str(task_id),activation = 'softmax', kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(tops[task_id])    \n","        \n","    return X, tops, latents, joint"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T17:26:11.312536Z","iopub.status.busy":"2022-03-02T17:26:11.312277Z","iopub.status.idle":"2022-03-02T17:26:11.328546Z","shell.execute_reply":"2022-03-02T17:26:11.327621Z","shell.execute_reply.started":"2022-03-02T17:26:11.312493Z"},"trusted":true},"outputs":[],"source":["class CrossStitch(tf.keras.layers.Layer):\n","\n","    def __init__(self, num_tasks,layer_num, hyperparameters, trainable, *args, **kwargs):\n","        self.num_tasks = num_tasks\n","        self.layer_num = layer_num\n","        self.hyperparameters = hyperparameters\n","        self.trainable = trainable\n","        super(CrossStitch, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        self.kernel = []\n","        for task_num in range(self.hyperparameters['num_tasks']):\n","            self.kernel.append(self.add_weight(name='cs_kernel_'+str(task_num)+'_'+str(self.layer_num), shape=(1, self.hyperparameters['num_tasks']),initializer=CrossStitchWeightInitializer(task_num, self.hyperparameters['num_tasks']),trainable=self.trainable))\n","\n","        if self.hyperparameters['enable_additional'] == True:\n","            for task_num in range(self.hyperparameters['num_tasks'], self.num_tasks):\n","                self.kernel.append(self.add_weight(name='cs_kernel_'+str(task_num)+'_'+str(self.layer_num), shape=(1, self.num_tasks),initializer=CrossStitchWeightInitializer(task_num, self.num_tasks),trainable=self.trainable))\n","        super(CrossStitch, self).build(input_shape)  \n","\n","    def call(self, input_feature_maps):        \n","        if (len(input_feature_maps)!=self.num_tasks):\n","            print(\"ERROR IN CROSS-STITCH\")\n","      \n","        output_feature_maps = []\n","        for current_task in range(self.hyperparameters['num_tasks']):\n","            output = tf.math.scalar_mul(self.kernel[current_task][0,current_task], input_feature_maps[current_task])\n","            for other_task in range(self.hyperparameters['num_tasks']):\n","                if (current_task==other_task):\n","                    continue    \n","                output+= tf.math.scalar_mul(self.kernel[current_task][0,other_task], input_feature_maps[other_task])\n","            output_feature_maps.append(output)\n","        \n","        if self.hyperparameters['enable_additional'] == True:\n","            for current_task in range(self.hyperparameters['num_tasks'], self.num_tasks):\n","                output = tf.math.scalar_mul(self.kernel[current_task][0,current_task], input_feature_maps[current_task])\n","                for other_task in range(self.num_tasks):\n","                    if (current_task==other_task):\n","                        continue\n","                    output+= tf.math.scalar_mul(self.kernel[current_task][0,other_task], input_feature_maps[other_task])\n","                output_feature_maps.append(output)\n","        return tf.stack(output_feature_maps, axis=0)\n","  \n","    def compute_output_shape(self, input_shape):\n","        return [self.num_tasks] + input_shape\n","\n","    def get_config(self):\n","        base_config = super(CrossStitch, self).get_config()\n","        base_config['num_tasks'] = self.num_tasks\n","        base_config['layer_num'] = self.task_num\n","        base_config['trainable'] = self.trainable\n","        base_config['hyperparameters'] = self.hyperparameters\n","\n","        return base_config"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T17:26:11.389793Z","iopub.status.busy":"2022-03-02T17:26:11.389543Z","iopub.status.idle":"2022-03-02T17:26:11.395368Z","shell.execute_reply":"2022-03-02T17:26:11.394282Z","shell.execute_reply.started":"2022-03-02T17:26:11.389767Z"},"trusted":true},"outputs":[],"source":["class CrossStitchWeightInitializer(tf.keras.initializers.Initializer):\n","\n","    def __init__(self, index, num_tasks):\n","        self.index=index\n","        self.num_tasks = num_tasks\n","\n","    def __call__(self, shape, dtype=None):\n","        lis = [[0]*self.num_tasks]\n","        lis[0][self.index]=1\n","        return tf.Variable(lis, dtype=tf.float32)\n","\n","    def get_config(self):  # To support serialization\n","        return {'index':self.index, 'num_tasks':self.num_tasks}"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T17:26:11.492157Z","iopub.status.busy":"2022-03-02T17:26:11.491919Z","iopub.status.idle":"2022-03-02T17:26:11.505681Z","shell.execute_reply":"2022-03-02T17:26:11.504557Z","shell.execute_reply.started":"2022-03-02T17:26:11.492133Z"},"trusted":true},"outputs":[],"source":["def variationalAutoEncoder(embed_size=360):\n","    # input\n","    X = Input((hyperparameters['height'],hyperparameters['width'],hyperparameters['channels']), name=\"input_vae\")\n","    \n","    # encoder\n","    x = Conv2D(128, kernel_size=(4,4), strides = (2,2), padding='same', use_bias=False, name='conv_1_vae')(X)\n","    x = BatchNormalization(name='bn_1_vae')(x)\n","    x = ReLU(name='relu_1_vae')(x)\n","    x = Conv2D(256, kernel_size=(4,4), strides = (2,2), padding='same', use_bias=False, name='conv_2_vae')(x)\n","    x = BatchNormalization(name='bn_2_vae')(x)\n","    x = ReLU(name='relu_2_vae')(x)\n","    x = Conv2D(512, kernel_size=(4,4), strides = (2,2), padding='same', use_bias=False, name='conv_3_vae')(x)\n","    x = BatchNormalization(name='bn_3_vae')(x)\n","    x = ReLU(name='relu_3_vae')(x)\n","    x = Conv2D(1024, kernel_size=(4,4), strides = (2,2), padding='same', use_bias=False, name='conv_4_vae')(x)\n","    x = BatchNormalization(name='bn_4_vae')(x)\n","    x = ReLU(name='relu_4_vae')(x)\n","    \n","    x = Flatten(name='flatten_1_vae')(x)\n","    \n","    mu = Dense(units = embed_size, activation = None, name='dense_1_vae')(x)\n","    \n","    #logvar = Dense(units = 32, activation = None)(x)\n","    # check what to be done here --->  ??\n","    #stds = Lambda(lambda x: x * 0.5)(logvar)\n","    #stds = tf.keras.backend.exp(stds)\n","    #epsilon = tf.keras.backend.random_normal((32,))\n","    #m = tf.keras.layers.Multiply()([stds,epsilon])\n","    #latents = tf.keras.layers.Add()([m,mu])\n","        \n","    # decoder\n","    z = Dense(units = 14*12*1024, activation = None, name='dense_2_vae')(mu)\n","    z = Reshape((14,12,1024), name='reshape_vae')(z)\n","    z = Conv2DTranspose(512, kernel_size=(4,4), strides = (2,2), padding='same', use_bias=False, name='conv_5_vae')(z)\n","    z = BatchNormalization(name='bn_5_vae')(z)\n","    z = ReLU(name='relu_5_vae')(z)\n","    z = Conv2DTranspose(256, kernel_size=(4,4), strides = (2,2), padding='same', use_bias=False, name='conv_6_vae')(z)\n","    z = BatchNormalization(name='bn_6_vae')(z)\n","    z = ReLU(name='relu_6_vae')(z)\n","    z = Conv2DTranspose(128, kernel_size=(4,4), strides = (2,2), padding='same', use_bias=False, name='conv_7_vae')(z)\n","    z = BatchNormalization(name='bn_7_vae')(z)\n","    z = ReLU(name='relu_7_vae')(z)\n","    z = Conv2DTranspose(3, strides=(2,2), kernel_size=(1,1), name='conv_8_vae')(z)\n","    \n","    return X, z, mu"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T17:26:11.56342Z","iopub.status.busy":"2022-03-02T17:26:11.563182Z","iopub.status.idle":"2022-03-02T17:26:11.568012Z","shell.execute_reply":"2022-03-02T17:26:11.567133Z","shell.execute_reply.started":"2022-03-02T17:26:11.563397Z"},"trusted":true},"outputs":[],"source":["# def single_model(X):\n","\n","# # ------------------------------------------- Block 1 BEGINS ------------------------------------\n","\n","#     tops = addConvBlock(40, 5, hyperparameters, 3, X, 1, 'same', 2, True, 1)\n","\n","\n","# # ------------------------------------------- Block 1 ENDS ------------------------------------\n","\n","# # ------------------------------------------- Block 2 BEGINS ------------------------------------\n","#     tops = addConvBlock(60, 5, hyperparameters, 3, tops, 1, 'same', 2, True, 2)\n","\n","# # ------------------------------------------- Block 2 ENDS ------------------------------------\n","# # ------------------------------------------- Block 3 BEGINS ------------------------------------\n","\n","#     tops = addConvBlock(80, 3, hyperparameters, 3, tops, 1, 'same', 2, True, 3)\n","\n","# # ------------------------------------------- Block 3 ENDS ------------------------------------\n","\n","# # ------------------------------------------- Block 4 BEGINS ------------------------------------\n","#     tops = addConvBlock(100, 3, hyperparameters, 3, tops, 1, 'same', 2, True,  4)\n","\n","# # ------------------------------------------- Block 4 ENDS ------------------------------------\n","\n","# # ------------------------------------------- Block 5 BEGINS ------------------------------------\n","#     tops = addConvBlock(140, 2, hyperparameters, 3, tops, 1, 'same', 2, True, 5)\n","\n","# # ------------------------------------------- Block 5 ENDS ------------------------------------\n","\n","#     tops = Flatten(name = 'flat')(tops)\n","#     latent = Dense(units = 360, name = 'dense1', kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(tops)\n","#     latent = ReLU()(latent)\n","\n","#     output = Dense(units = 2, name='output',activation = 'softmax', kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l2(hyperparameters['reg_lambda']))(latent)\n","\n","#     # add 360 wala unit to o/p\n","#     return output, latent\n","\n","\n","# def addConvBlock_v1(num_filters, kernel_size, hyperparameters, pool_size, tops, stride, pad, pool_stride, isPool , i):\n","    \n","#     input_tensor = tops\n","#     conv = Conv2D(num_filters, kernel_size=kernel_size, name = 'conv'+str(i),  strides=(stride, stride), padding = pad, kernel_initializer=hyperparameters['initializer'], kernel_regularizer=regularizers.l1(hyperparameters['reg_lambda']))(input_tensor)\n","#     conv = ReLU(name='relu'+str(i))(conv)\n","\n","#     if (isPool==True):\n","#         name = 'pool'+str(i)\n","#         conv = MaxPooling2D(pool_size=(pool_size, pool_size), name = name, strides = (pool_stride, pool_stride), padding='valid')(conv)\n","#     tops= conv\n","  \n","#     return tops\n","\n","\n","# def predictor_v1():\n","#     x,y,z = hyperparameters['height'], hyperparameters['width'], hyperparameters['channels']\n","#     X = Input((x,y,z), name='input_predictor')\n","#     model_list=[]\n","    \n","#     output_list=[]\n","#     latent_list=[]\n","#     for task in range(hyperparameters['num_tasks']+hyperparameters['enable_additional']*hyperparameters['additional_attr_count']):\n","#         output, latent = single_model(X) \n","#         output_list.append(output)\n","#         latent_list.append(latent)\n","#         model = Model(inputs=X, outputs=[output, latent])\n","#         for i in model.layers:\n","#             if (i._name[0:5] == 're_lu'):\n","#                 if (task==0):\n","#                     i._name = 're_lu'+'_'+str(0)\n","#                 else:\n","#                     i._name='re_lu'+'_'+str(task)+'_'+str(task)\n","#             else:   \n","#                 i._name+='_'+str(task)\n","\n","#         model_list.append(model)\n","#     model = Model(inputs = X, outputs = [output_list, latent_list])\n","#     # model = Model(inputs = X, outputs = [model.output for model in model_list])\n","    \n","#     return model"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T17:26:11.639662Z","iopub.status.busy":"2022-03-02T17:26:11.63937Z","iopub.status.idle":"2022-03-02T17:26:11.643784Z","shell.execute_reply":"2022-03-02T17:26:11.642919Z","shell.execute_reply.started":"2022-03-02T17:26:11.639636Z"},"trusted":true},"outputs":[],"source":["def predictor():\n","    X, tops, latents,joint = combined_model()\n","    model = Model(inputs=X, outputs=[tops,latents,joint])\n","    return model"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T17:26:11.74829Z","iopub.status.busy":"2022-03-02T17:26:11.748054Z","iopub.status.idle":"2022-03-02T17:26:11.756795Z","shell.execute_reply":"2022-03-02T17:26:11.755669Z","shell.execute_reply.started":"2022-03-02T17:26:11.748267Z"},"trusted":true},"outputs":[],"source":["class taskEmbeddings(tf.keras.layers.Layer):\n","    def __init__(self, num_layers, **kwargs):\n","        self.num_layers = num_layers\n","        super(taskEmbeddings, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        super(taskEmbeddings, self).build(input_shape)\n","\n","    def call(self, x):\n","\n","        dynamic_weights = x[1] # 16x5\n","        output_feature_maps = x[0] # 16x5x90\n","\n","        data = np.identity(len(output_feature_maps), dtype = np.float32)\n","        #onehotencoder = OneHotEncoder() # categorical_features = [0]\n","        #data = onehotencoder.fit_transform(np.arange(len(output_feature_maps)).reshape((-1,1))).toarray()\n","        weighted_output=[]\n","        for output in range(len(output_feature_maps)):\n","            one_hot = tf.constant(data[output])\n","            one_hot = tf.keras.backend.reshape(one_hot, shape=(-1, len(output_feature_maps)))\n","            #one_hot = tf.repeat(one_hot, repeats=[hyperparameters['batch_size']], axis = 0)\n","            product1 = Dot(axes=1)([dynamic_weights, one_hot]) # 16x1\n","            product1 = tf.repeat(product1, repeats=180, axis = 1)\n","            product2 = Multiply()([product1, output_feature_maps[output]])\n","            weighted_output.append(product2)\n","\n","        return weighted_output\n","\n","    #def compute_output_shape(self, input_shape):\n","    #    return input_shape\n","\n","    def get_config(self):\n","        base_config = super(taskEmbeddings, self).get_config()\n","        base_config['num_layers'] = self.num_layers\n","        return base_config"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T17:26:11.872653Z","iopub.status.busy":"2022-03-02T17:26:11.872282Z","iopub.status.idle":"2022-03-02T17:26:11.910334Z","shell.execute_reply":"2022-03-02T17:26:11.909541Z","shell.execute_reply.started":"2022-03-02T17:26:11.872622Z"},"trusted":true},"outputs":[],"source":["class ActiveLearning(keras.Model):\n","    def __init__(self, discriminator, generator, predictor, trackers, alpha):\n","        super(ActiveLearning, self).__init__()\n","        self.discriminator = discriminator\n","        self.generator = generator\n","        self.predictor = predictor\n","        self.trackers = trackers\n","        self.alpha=alpha\n","        self.prev_loss = 10000\n","\n","    def compile(self, d_optimizer, g_optimizer, p_optimizer):\n","        super(ActiveLearning, self).compile()\n","        self.d_optimizer = d_optimizer\n","        self.g_optimizer = g_optimizer\n","        self.p_optimizer = p_optimizer\n","                \n","    def train_step(self, real_images):\n","        \n","        # get labelled_x, unlabelled_x and labelled_y\n","        \n","        x = real_images\n","        labelled_x = x[0]\n","        unlabelled_x = x[1]\n","        labelled_y = x[2]\n","        \n","        ##### TRAIN THE PREDICTOR #####\n","        \n","        \n","        # Compute output and latents\n","        with tf.GradientTape() as tape:\n","            labelled_prediction_y, _, _ = self.predictor(labelled_x, training=True)\n","            predictor_loss = keras.losses.categorical_crossentropy(labelled_y, labelled_prediction_y) # ----> 1\n","        \n","        # Compute gradients\n","        trainable_vars = self.predictor.trainable_variables\n","        gradients = tape.gradient(predictor_loss, trainable_vars)\n","\n","        # Update weights\n","        self.p_optimizer.apply_gradients(zip(gradients, trainable_vars)) \n","        \n","        # ------------------------------------------------------------------------------------------------\n","        \n","        ##### TRAIN THE GENERATOR #####\n","        \n","        # Create labels for VAE\n","        labelled_disc_true = np.ones((hyperparameters['batch_size'],1))\n","        unlabelled_disc_fake = np.ones((hyperparameters['batch_size'],1))\n","        \n","        # Compute VAE outputs\n","        with tf.GradientTape() as tape:\n","            # Compute generator o/p\n","            labelled_vae_y, labelled_vae_latent = self.generator(labelled_x)\n","            unlabelled_vae_y, unlabelled_vae_latent = self.generator(unlabelled_x)\n","            \n","            # Calculate loss for VAE\n","            labelled_vae_loss = keras.losses.mean_squared_error(labelled_x, labelled_vae_y) # ----> 2\n","            unlabelled_vae_loss = keras.losses.mean_squared_error(unlabelled_x, unlabelled_vae_y) # ----> 2\n","            \n","            # Compute predictor o/p\n","           # _, labelled_predictor_latent = self.predictor(labelled_x)\n","            #_, unlabelled_predictor_latent = self.predictor(unlabelled_x)\n","            \n","            # Average out the latents for 5 tasks --- SHOULD I?\n","            # labelled_vae_latent = math_ops.mean(ops.convert_to_tensor(labelled_vae_latent), axis=0)\n","            # unlabelled_vae_latent = math_ops.mean(ops.convert_to_tensor(unlabelled_vae_latent), axis=0)\n","            #labelled_predictor_latent = math_ops.mean(ops.convert_to_tensor(labelled_predictor_latent), axis=0)\n","            #unlabelled_predictor_latent = math_ops.mean(ops.convert_to_tensor(unlabelled_predictor_latent), axis=0)\n","            \n","            # Join vae and predictor latents\n","            #labelled_disc_in = tf.concat([labelled_vae_latent,labelled_predictor_latent],axis=1)\n","            #unlabelled_disc_in = tf.concat([unlabelled_vae_latent,unlabelled_predictor_latent],axis=1)            \n","            \n","            # Compute disc o/p\n","            #labelled_disc_y = self.discriminator(labelled_disc_in)\n","            #unlabelled_disc_y = self.discriminator(unlabelled_disc_in)\n","            \n","            # Calculate loss for disc\n","            #labelled_disc_loss = keras.losses.binary_crossentropy(labelled_disc_true, labelled_disc_y) # ----> 3\n","            #unlabelled_dic_loss = keras.losses.binary_crossentropy(unlabelled_disc_fake, unlabelled_disc_y) # ----> 3\n","            \n","            # Compute total VAE loss\n","            #disc_loss = labelled_disc_loss + unlabelled_dic_loss\n","            vae_loss = labelled_vae_loss + unlabelled_vae_loss #+ (self.advisory_param*disc_loss)\n","        \n","        # Compute gradients\n","        trainable_vars = self.generator.trainable_variables\n","        gradients = tape.gradient(vae_loss, trainable_vars)\n","        \n","        # Update weights\n","        self.g_optimizer.apply_gradients(zip(gradients, trainable_vars))         \n","        \n","        # ------------------------------------------------------------------------------------------------\n","        \n","        ##### TRAIN THE DISCRIMINATOR #####\n","        \n","        # Create disc labels\n","        labelled_disc_true = np.ones((hyperparameters['batch_size'],1))\n","        unlabelled_disc_true = np.zeros((hyperparameters['batch_size'],1))\n","        \n","        # Compute VAE latents\n","        _, labelled_vae_latent = self.generator(labelled_x, training = False)\n","        _, unlabelled_vae_latent = self.generator(unlabelled_x, training = False)\n","        \n","        # Compute predictor latents\n","        _, labelled_predictor_latent, _ = self.predictor(labelled_x, training=False)\n","        _, unlabelled_predictor_latent, _ = self.predictor(unlabelled_x, training=False)\n","        \n","        # Average out the latents for 5 tasks --- SHOULD I?\n","        # labelled_vae_latent = math_ops.mean(ops.convert_to_tensor(labelled_vae_latent), axis=0)\n","        # unlabelled_vae_latent = math_ops.mean(ops.convert_to_tensor(unlabelled_vae_latent), axis=0)\n","        labelled_predictor_latent = math_ops.mean(ops.convert_to_tensor(labelled_predictor_latent), axis=0)\n","        unlabelled_predictor_latent = math_ops.mean(ops.convert_to_tensor(unlabelled_predictor_latent), axis=0)\n","        \n","        # Join vae and predictor latents\n","        labelled_disc_in = tf.concat([labelled_vae_latent,labelled_predictor_latent],axis=1)\n","        unlabelled_disc_in = tf.concat([unlabelled_vae_latent,unlabelled_predictor_latent],axis=1)\n","        \n","        # Compute disc output\n","        with tf.GradientTape() as tape:\n","            labelled_disc_y = self.discriminator(labelled_disc_in,training=True)\n","            unlabelled_disc_y = self.discriminator(unlabelled_disc_in,training=True)\n","            \n","            labelled_disc_loss = keras.losses.binary_crossentropy(labelled_disc_true, labelled_disc_y) # ----> 3\n","            unlabelled_dic_loss = keras.losses.binary_crossentropy(unlabelled_disc_true, unlabelled_disc_y) # ----> 3\n","            \n","            disc_loss = labelled_disc_loss + unlabelled_dic_loss\n","        \n","        # Compute gradients\n","        trainable_vars = self.discriminator.trainable_variables\n","        gradients = tape.gradient(disc_loss, trainable_vars)\n","        \n","        # Update weights\n","        self.d_optimizer.apply_gradients(zip(gradients, trainable_vars)) \n","    \n","        # ------------------------------------------------------------------------------------------------\n","        \n","        # Computing Metrics\n","        \n","        # For predictor\n","        \n","        self.trackers['loss_tracker_predictor'].update_state(labelled_y, labelled_prediction_y)\n","        self.trackers['acc_metric_predictor'].update_state(labelled_y, labelled_prediction_y)\n","        \n","        #loss_tracker_predictor.update_state(labelled_y, labelled_prediction_y)\n","        #acc_metric_predictor.update_state(labelled_y, labelled_prediction_y)\n","        \n","        for i in range(hyperparameters['num_tasks']):\n","            self.trackers['individual_loss_tracker_predictor'][i].update_state(labelled_y[i], labelled_prediction_y[i])\n","            self.trackers['individual_acc_metric_predictor'][i].update_state(labelled_y[i], labelled_prediction_y[i])\n","                \n","        # For VAE\n","        self.trackers['loss_tracker_generator'].update_state(labelled_x, labelled_vae_y)\n","        self.trackers['loss_tracker_generator'].update_state(unlabelled_x, unlabelled_vae_y)\n","        # For Discriminator\n","        self.trackers['loss_tracker_disc'].update_state(labelled_disc_true,labelled_disc_y)\n","        self.trackers['loss_tracker_disc'].update_state(unlabelled_disc_true,unlabelled_disc_y)\n","        self.trackers['acc_tracker_disc'].update_state(labelled_disc_true,labelled_disc_y)\n","        self.trackers['acc_tracker_disc'].update_state(unlabelled_disc_true,unlabelled_disc_y)\n","\n","\n","            \n","        ret_dic = {\"loss_predictor_total\": self.trackers['loss_tracker_predictor'].result(), # loss_tracker_predictor.result(), \n","                   \"acc_predictor\":self.trackers['acc_metric_predictor'].result(), # acc_metric_predictor.result(), \n","                   \"loss_VAE\":  self.trackers['loss_tracker_generator'].result(), # loss_tracker_generator.result(),\n","                   \"loss_disc\": self.trackers['loss_tracker_disc'].result(), # loss_tracker_disc.result(),\n","                   \"acc_disc\": self.trackers['acc_tracker_disc'].result()} # acc_tracker_disc.result()}\n","        \n","        for i in range(hyperparameters['num_tasks']):\n","            ret_dic[\"loss_predictor_\"+str(i)] = self.trackers['individual_loss_tracker_predictor'][i].result() # individual_loss_tracker_predictor[i].result()\n","        for i in range(hyperparameters['num_tasks']):\n","            ret_dic[\"acc_predictor_\"+str(i)] = self.trackers['individual_acc_metric_predictor'][i].result() # individual_acc_metric_predictor[i].result()\n","            \n","        return ret_dic\n","    \n","    def call(self, x):\n","        return\n","    \n","    def test_step(self, real_images):\n","        \n","        x = real_images\n","        labelled_x = x[0]\n","        labelled_y = x[1]\n","        \n","        # Predictor step\n","        labelled_prediction_y, labelled_predictor_latent, _ = self.predictor(labelled_x, training=False)\n","        \n","        # Generator step\n","        labelled_vae_y, labelled_vae_latent = self.generator(labelled_x, training=False)\n","        \n","        # Discriminator step\n","        labelled_predictor_latent = math_ops.mean(ops.convert_to_tensor(labelled_predictor_latent), axis=0)\n","        labelled_disc_in = tf.concat([labelled_vae_latent,labelled_predictor_latent],axis=1)\n","        \n","        labelled_disc_y = self.discriminator(labelled_disc_in,training=False)\n","        \n","        # Updating metrics\n","        # For Predictor\n","        self.trackers['loss_tracker_predictor'].update_state(labelled_y, labelled_prediction_y)\n","        self.trackers['acc_metric_predictor'].update_state(labelled_y, labelled_prediction_y)\n","        \n","        for i in range(hyperparameters['num_tasks']):\n","            self.trackers['individual_loss_tracker_predictor'][i].update_state(labelled_y[i], labelled_prediction_y[i])\n","            self.trackers['individual_acc_metric_predictor'][i].update_state(labelled_y[i], labelled_prediction_y[i])\n","            \n","        self.trackers['loss_tracker_generator'].update_state(labelled_x, labelled_vae_y)\n","        \n","        \n","        # For Discriminator\n","        labelled_disc_true = np.ones((hyperparameters['batch_size'],1))\n","        self.trackers['loss_tracker_disc'].update_state(labelled_disc_true,labelled_disc_y)\n","        self.trackers['acc_tracker_disc'].update_state(labelled_disc_true,labelled_disc_y)\n","        \n","        \n","        ret_dic = {\"loss_predictor_total\": self.trackers['loss_tracker_predictor'].result(), # loss_tracker_predictor.result(), \n","                   \"acc_predictor\": self.trackers['acc_metric_predictor'].result(), # acc_metric_predictor.result(), \n","                   \"loss_VAE\":  self.trackers['loss_tracker_generator'].result(), # loss_tracker_generator.result(),\n","                   \"loss_disc\": self.trackers['loss_tracker_disc'].result(), # loss_tracker_disc.result(),\n","                   \"acc_disc\": self.trackers['acc_tracker_disc'].result()} # acc_tracker_disc.result()}\n","        \n","        for i in range(hyperparameters['num_tasks']):\n","            ret_dic[\"loss_predictor_\"+str(i)] = self.trackers['individual_loss_tracker_predictor'][i].result() # individual_loss_tracker_predictor[i].result()\n","        for i in range(hyperparameters['num_tasks']):\n","            ret_dic[\"acc_predictor_\"+str(i)] = self.trackers['individual_acc_metric_predictor'][i].result() # individual_acc_metric_predictor[i].result()\n","                    \n","#         ret_dic = {\"loss_predictor_total\": loss_tracker_predictor.result(), \n","#                    \"acc_predictor\": acc_metric_predictor.result(), \n","#                    \"loss_VAE\": loss_tracker_generator.result(),\n","#                    \"loss_disc\": loss_tracker_disc.result(),\n","#                    \"acc_disc\": acc_tracker_disc.result()}\n","        \n","#         for i in range(hyperparameters['num_tasks']):\n","#             ret_dic[\"loss_predictor_\"+str(i)] = individual_loss_tracker_predictor[i].result()\n","#         for i in range(hyperparameters['num_tasks']):\n","#             ret_dic[\"acc_predictor_\"+str(i)] = individual_acc_metric_predictor[i].result()\n","            \n","        return ret_dic        \n","    \n","    def predict_step(self, real_images):\n","        unlabelled_x, unlabelled_y = real_images\n","        \n","        # Predictor step\n","        unlabelled_prediction_y, unlabelled_predictor_latent, joint_weights = self.predictor(unlabelled_x, training=False)\n","        \n","        # Generator step\n","        unlabelled_vae_y, unlabelled_vae_latent = self.generator(unlabelled_x, training=False)\n","        \n","        # Discriminator step\n","        unlabelled_predictor_latent = math_ops.mean(ops.convert_to_tensor(unlabelled_predictor_latent), axis=0)\n","        unlabelled_disc_in = tf.concat([unlabelled_vae_latent,unlabelled_predictor_latent],axis=1)\n","        \n","        unlabelled_disc_y = self.discriminator(unlabelled_disc_in,training=False)\n","        \n","        return unlabelled_prediction_y, unlabelled_disc_y, unlabelled_y, joint_weights\n","\n","    @property\n","    def metrics(self):\n","        # We list our `Metric` objects here so that `reset_states()` can be\n","        # called automatically at the start of each epoch\n","        # or at the start of `evaluate()`.\n","        # If you don't implement this property, you have to call\n","        # `reset_states()` yourself at the time of your choosing.\n","        return [self.trackers[\"loss_tracker_predictor\"], self.trackers[\"acc_metric_predictor\"], self.trackers[\"loss_tracker_generator\"], self.trackers[\"loss_tracker_disc\"], self.trackers[\"acc_tracker_disc\"]] + self.trackers[\"individual_loss_tracker_predictor\"] + self.trackers[\"individual_acc_metric_predictor\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T17:26:11.981509Z","iopub.status.busy":"2022-03-02T17:26:11.981258Z","iopub.status.idle":"2022-03-02T17:26:11.993534Z","shell.execute_reply":"2022-03-02T17:26:11.992427Z","shell.execute_reply.started":"2022-03-02T17:26:11.981486Z"},"trusted":true},"outputs":[],"source":["def uncertainity(probs, weights):\n","    lis = []\n","    lis_output = []\n","    for i in range(hyperparameters['num_tasks']):\n","        attr_output = probs[i]\n","        w = weights[:,i]\n","        k = -1* np.sum(attr_output*np.log(attr_output),axis=1)\n","        lis_output.append(k)\n","        lis.append(w*k)\n","    \n","    variance = np.var(np.array(lis),axis=0)\n","    return np.array(lis).sum(axis=0), variance\n","\n","def getIndices(output, hyperparameters ,pretrain=False):\n","    if pretrain == True:\n","        count =  hyperparameters['train_initial_batches']*hyperparameters['batch_size']\n","        if ((output<=0.5).sum())>=count:\n","            sort = np.argwhere(output<=0.5)[:,0]\n","            return sort\n","        else:\n","            selection = (int((hyperparameters['train_initial_batches']*hyperparameters['batch_size'])/1000)+1)*1000\n","            sort = np.argpartition((output)[:,0], selection)\n","            return sort[:selection]\n","    else:\n","        count = hyperparameters['num_uncertain_elements']\n","        if ((output<=0.5).sum())>=count:\n","            sort = np.argwhere(output<=0.5)[:,0]\n","            return sort\n","        else:\n","            selection = (int(hyperparameters['num_uncertain_elements']/1000)+1)*1000\n","            sort = np.argpartition((output)[:,0], selection)\n","            return sort[:selection]\n","        \n","def divide_data(train, initial = False):\n","    num_samples = train.values.shape[0]\n","    \n","    if initial:\n","        idx = random.sample(list(np.arange(num_samples)), ((int(hyperparameters['initial_percent_val']*num_samples)//hyperparameters['batch_size'])*hyperparameters['batch_size']))\n","    else:\n","        idx = random.sample(list(np.arange(num_samples)), ((int(hyperparameters['initial_percent']*num_samples)//hyperparameters['batch_size'])*hyperparameters['batch_size']))\n","\n","    return pd.DataFrame(train.values[idx,:], columns=train.columns), idx"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T17:26:12.070507Z","iopub.status.busy":"2022-03-02T17:26:12.070267Z","iopub.status.idle":"2022-03-02T17:26:13.026542Z","shell.execute_reply":"2022-03-02T17:26:13.025608Z","shell.execute_reply.started":"2022-03-02T17:26:12.070484Z"},"trusted":true},"outputs":[],"source":["attr = pd.read_csv('../input/celeba-dataset/list_attr_celeba.csv')\n","eval_partition = pd.read_csv('../input/celeba-dataset/list_eval_partition.csv')"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T17:26:14.967073Z","iopub.status.busy":"2022-03-02T17:26:14.966725Z","iopub.status.idle":"2022-03-02T17:26:16.895821Z","shell.execute_reply":"2022-03-02T17:26:16.894887Z","shell.execute_reply.started":"2022-03-02T17:26:14.967041Z"},"trusted":true},"outputs":[],"source":["break_point_ep = {'3': 5e-4,'6': 5e-4,'10': 1e-5}\n","splits = [0.1,0.15,0.2,0.25,0.3,0.35,0.40]\n","\n","# defining metrics\n","\n","trackers = {\n","    \"loss_tracker_predictor\": tf.keras.metrics.CategoricalCrossentropy(name=\"loss_predictor_total\"),\n","    \"acc_metric_predictor\": tf.keras.metrics.CategoricalAccuracy(name=\"acc_predictor\"),\n","    \"individual_loss_tracker_predictor\": [tf.keras.metrics.CategoricalCrossentropy(name=\"loss_predictor_\"+str(i)) for i in range(hyperparameters['num_tasks'])],\n","    \"individual_acc_metric_predictor\": [tf.keras.metrics.CategoricalAccuracy(name=\"acc_predictor_\"+str(i)) for i in range(hyperparameters['num_tasks'])],\n","    \"loss_tracker_generator\": tf.keras.metrics.MeanSquaredError(name='loss_VAE'),\n","    \"loss_tracker_disc\":  tf.keras.metrics.BinaryCrossentropy(name='loss_disc'),\n","    \"acc_tracker_disc\": tf.keras.metrics.BinaryAccuracy(\"acc_disc\")\n","}"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T17:26:16.897603Z","iopub.status.busy":"2022-03-02T17:26:16.897274Z","iopub.status.idle":"2022-03-02T17:26:16.910586Z","shell.execute_reply":"2022-03-02T17:26:16.909794Z","shell.execute_reply.started":"2022-03-02T17:26:16.897568Z"},"trusted":true},"outputs":[],"source":["class CalculatingPredictions(tf.keras.callbacks.Callback):\n","    def __init__(self, preds, test_gen, train_gen, lr, is_validation=False):\n","        self.preds = preds\n","        self.train_gen = train_gen\n","        self.test_gen = test_gen\n","        self.lr = lr\n","        self.is_validation=is_validation\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        p = self.model.predict(self.train_gen, verbose=1)  \n","        pre = p[0]\n","        out = p[2]\n","        h = keras.losses.mae(out, pre).numpy().mean()\n","        print(h)\n","\n","        if (h>=self.model.prev_loss):\n","            self.model.alpha = self.model.alpha + (self.lr*h)\n","        else:\n","            self.model.alpha = self.model.alpha - (self.lr*h)\n","        self.model.prev_loss = h\n","        variance_weight=np.array([self.model.prev_loss,self.model.alpha])\n","        if (self.is_validation == True):\n","            np.save(\"./saved_history/val_params\" +str(epoch)+ '.npy', variance_weight)\n","        else:\n","            np.save(\"./saved_history/params\" + str(epoch) +'.npy', variance_weight)\n","\n","        print(self.model.alpha)\n","        predict=self.model.evaluate(self.test_gen)\n","        print(predict)\n","        self.preds.append(predict)\n","        k = np.array(self.preds)\n","        if (self.is_validation==True):\n","            np.save(\"./saved_history/AL_model_from_scratch_non_cs_validation\"+ str(epoch)+ \".npy\", k)\n","        else:\n","            np.save(\"./saved_history/AL_model_from_scratch_non_cs_\"+ str(epoch)+ \".npy\", k)\n","        date = datetime.datetime.now().strftime(\"%d - %b - %y - %H:%M:%S\")\n","        \n","        if (self.is_validation==False and epoch%2==0):\n","            self.model.predictor.save_weights(\"./saved_history/models/pred_model_from_scratch_vanilla_epoch\"+str(epoch)+'.h5')\n","            self.model.discriminator.save_weights(\"./saved_history/models/disc_model_from_scratch_vanilla_epoch\"+str(epoch)+'.h5')\n","            self.model.generator.save_weights(\"./saved_history/models/vae_model_from_scratch_vanilla_epoch\"+str(epoch)+'.h5')"]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T17:26:16.912726Z","iopub.status.busy":"2022-03-02T17:26:16.912225Z","iopub.status.idle":"2022-03-02T17:26:18.958906Z","shell.execute_reply":"2022-03-02T17:26:18.957373Z","shell.execute_reply.started":"2022-03-02T17:26:16.912688Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: cannot create directory ‘logs’: File exists\n","mkdir: cannot create directory ‘saved_history’: File exists\n","mkdir: cannot create directory ‘saved_history/models’: File exists\n"]}],"source":["!mkdir logs\n","!mkdir saved_history\n","!mkdir saved_history/models"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T17:26:18.961991Z","iopub.status.busy":"2022-03-02T17:26:18.961591Z","iopub.status.idle":"2022-03-02T17:26:19.038341Z","shell.execute_reply":"2022-03-02T17:26:19.036971Z","shell.execute_reply.started":"2022-03-02T17:26:18.961948Z"},"trusted":true},"outputs":[],"source":["def startTraining(trackers, splits, break_point_ep,  validation_first = False, load_pred_model = False, load_vae = False, load_disc = False, further_training=False):\n","    preds=[] ####\n","    validation_train_history=[] # new\n","    date = datetime.datetime.now().strftime(\"%d - %b - %y - %H:%M:%S\")\n","\n","    filename = \"AL_model_from_scratch_non_cs_\"+date ###\n","    logdir = \"./logs/\" + filename\n","\n","    filepath = \"./saved_history/model/AL_model_from_scratch_non_cs\"\n","    checkpoint = ModelCheckpoint(filepath, verbose=1, save_best_only=False, mode='max', period = 1)\n","    csv_logger = CSVLogger('./saved_history/training_results_AL_model_from_scratch_non_cs.csv', separator = ',', append=True)\n","    pre_train_logger = CSVLogger('./saved_history/pre_training_results_AL_model_from_scratch_non_cs.csv', separator = ',', append=True)\n","    tensorboard_callback = TensorBoard(log_dir = logdir)\n","\n","    pre_tensorboard_callback = TensorBoard(log_dir =\" ./logs/pre_\"+filename)\n","\n","    # Instantiate components\n","    # defining my predictor\n","    pred_model = predictor()\n","    pred_model.compile(optimizer = keras.optimizers.SGD(learning_rate=hyperparameters['lr'],\n","                                                        clipnorm=1.0 ))\n","    if load_pred_model: #####\n","        print(\"Predictor weights loading ...\")\n","        pred_model.load_weights(\"./saved_history/models/pred_model_from_scratch_vanilla_4.h5\", by_name=True)\n","    \n","    # defining my discriminator\n","    disc_in, disc_out = discriminator()\n","    disc = Model(inputs = disc_in, outputs = disc_out)\n","    disc.compile(optimizer = keras.optimizers.SGD(learning_rate=hyperparameters['lr'],\n","                                                  clipnorm=1.0 ))\n","    if load_disc: ###\n","        print(\"Discriminator weights loading ...\")\n","        disc.load_weights(\"./saved_history/models/disc_model_from_scratch_vanilla_4.h5\", by_name=True)\n","        \n","    # defining my generator\n","    X, z, mu = variationalAutoEncoder()\n","    vae = Model(inputs = X, outputs = [z,mu])\n","    vae.compile(optimizer = keras.optimizers.RMSprop(learning_rate=hyperparameters['lr'],\n","                                                     clipnorm=1.0 ))\n","    if load_vae: ####\n","        print(\"VAE weights loading ...\")\n","        vae.load_weights(\"./saved_history/models/vae_model_from_scratch_vanilla_4.h5\" ,by_name = True)\n","    \n","    # Instantiate AL model\n","    AL_model = ActiveLearning(discriminator=disc, generator=vae, predictor=pred_model, trackers = trackers, alpha=1)\n","    AL_model.compile(\n","        d_optimizer=keras.optimizers.SGD(learning_rate=hyperparameters['lr'],clipnorm=1.0 ),\n","        g_optimizer= keras.optimizers.RMSprop(learning_rate=hyperparameters['lr'],clipnorm=1.0),\n","        p_optimizer=keras.optimizers.SGD(learning_rate=hyperparameters['lr'],clipnorm=1.0 ))\n","\n","    print('model loaded')\n","\n","    train_imggen = ImageDataGenerator(rescale = 1./255)\n","    image_path = '../input/celeba-dataset/img_align_celeba/img_align_celeba/'\n","    train, val, test = preprocess(hyperparameters, attr, eval_partition)\n","    train_gen_full = load_generator(train, False)\n","    val_gen = load_generator(val)\n","    test_gen = load_generator(test, False)\n","\n","    if validation_first == True:\n","        logger = CSVLogger('./saved_history/pretraining_results_AL_model_from_scratch_non_cs.csv', separator = ',', append=True)\n","        pre_tensorboard_callback = TensorBoard(log_dir =\" ./logs/pre_\"+filename)\n","        \n","        labelled_pretrain, idx_prelabelled = divide_data(val, initial=True)\n","        idx_preunlabelled = list(np.setdiff1d(list(range(val.shape[0])), idx_prelabelled))\n","        unlabelled_pretrain = pd.DataFrame(val.values[idx_preunlabelled,:], columns=val.columns)\n","       \n","        pretrain_gen = generate_generator_multiple(generator=train_imggen,\n","                                               dir1=image_path,\n","                                               dir2=image_path,\n","                                               df1 = labelled_pretrain,\n","                                               df2 = unlabelled_pretrain,\n","                                               batch_size=hyperparameters['batch_size'],\n","                                               img_height=hyperparameters['height'],\n","                                               img_width=hyperparameters['width'])\n","        \n","        labelled_pretrain_gen  = load_generator(labelled_pretrain, False)\n","        \n","        val_history = AL_model.fit(pretrain_gen, epochs = hyperparameters['pretraining_epochs'], steps_per_epoch = 744, callbacks = [CalculatingPredictions(preds, test_gen, labelled_pretrain_gen, 0.01, True) , pre_tensorboard_callback, logger], verbose=1)\n","        validation_train_history.append(val_history.history)\n","        with open(\"./saved_history/pretraining_history_list.json\", 'w') as f:\n","            json.dump(validation_train_history, f, indent=2)\n","            \n","        labelled_train, idx_labelled = divide_data(train)\n","        idx_unlabelled = list(np.setdiff1d(list(range(train.shape[0])), idx_labelled))\n","        unlabelled_train = pd.DataFrame(train.values[idx_unlabelled,:], columns=train.columns)        \n","        train_gen = generate_generator_multiple(generator=train_imggen,\n","                                               dir1=image_path,\n","                                               dir2=image_path,\n","                                               df1 = labelled_train,\n","                                               df2 = unlabelled_train, \n","                                               batch_size=hyperparameters['batch_size'],\n","                                               img_height=hyperparameters['height'],\n","                                               img_width=hyperparameters['width'])\n","        unlabelled_gen = load_generator(unlabelled_train, False)    \n","        labelled_train_gen = load_generator(labelled_train, False)\n","        \n","    elif further_training==True:\n","\n","        idx_labelled = np.load(\"./saved_history/idx_labelled_4.npy\")\n","        labelled_train = train.iloc[idx_labelled, :]\n","        idx_unlabelled = list(np.setdiff1d(list(range(train.shape[0])), idx_labelled))\n","        unlabelled_train = pd.DataFrame(train.values[idx_unlabelled,:], columns=train.columns)\n","        train_gen = generate_generator_multiple(generator=train_imggen,\n","                                               dir1=image_path,\n","                                               dir2=image_path,\n","                                               df1 = labelled_train,\n","                                               df2 = unlabelled_train,\n","                                               batch_size=hyperparameters['batch_size'],\n","                                               img_height=hyperparameters['height'],\n","                                               img_width=hyperparameters['width'])\n","        unlabelled_gen = load_generator(unlabelled_train, False)\n","        labelled_train_gen = load_generator(labelled_train, False)\n","    else:\n","        labelled_train, idx_labelled = divide_data(train)\n","        idx_unlabelled = list(np.setdiff1d(list(range(train.shape[0])), idx_labelled))\n","        unlabelled_train = pd.DataFrame(train.values[idx_unlabelled,:], columns=train.columns)        \n","        train_gen = generate_generator_multiple(generator=train_imggen,\n","                                               dir1=image_path,\n","                                               dir2=image_path,\n","                                               df1 = labelled_train,\n","                                               df2 = unlabelled_train, \n","                                               batch_size=hyperparameters['batch_size'],\n","                                               img_height=hyperparameters['height'],\n","                                               img_width=hyperparameters['width'])\n","        unlabelled_gen = load_generator(unlabelled_train, False)    \n","        labelled_train_gen = load_generator(labelled_train, False)\n","\n","    history_list=[]\n","\n","    if further_training==True:\n","        num_batches = 10 #idx_labelled.shape[0]//hyperparameters['batch_size']\n","        tensorboard_callback = TensorBoard(log_dir = 'AL_model_from_scratch_non_cs_09 - Jan - 21 - 12:50:36')\n","\n","        iteration =5\n","        epoch_num = 13\n","\n","        history = AL_model.fit(train_gen,initial_epoch = epoch_num, epochs=epoch_num+7, steps_per_epoch = num_batches, validation_data=val_gen,callbacks = [CalculatingPredictions(preds, test_gen,labelled_train_gen, 0.01), csv_logger], verbose = 1)\n","        history_list.append(history.history)\n","        pred_model.save_weights(\"./saved_history/models/pred_model_from_scratch_vanilla_\"+str(iteration)+'.h5')\n","        disc.save_weights(\"./saved_history/models/disc_model_from_scratch_vanilla_\"+str(iteration)+'.h5')\n","        vae.save_weights(\"./saved_history/models/vae_model_from_scratch_vanilla_\"+str(iteration)+'.h5')\n","        with open(\"./saved_history/history_list_only_\"+str(iteration)+\".json\", 'w') as f:\n","            json.dump(history_list, f, indent=2)\n","\n","        with open(\"./saved_history/preds_only_\"+str(iteration)+\".json\", 'w') as f:\n","            json.dump(preds, f, indent=2)\n","    else:\n","        test_predictions=[]\n","        indices_list = []\n","        epoch_num = 0\n","        num_batches= int(train.shape[0] * (hyperparameters['initial_percent']) / hyperparameters['batch_size'])\n","\n","        for iteration in range(len(splits)):\n","            print(iteration)\n","\n","            if iteration==0:\n","                # Initial training ---- change\n","                history = AL_model.fit(train_gen, epochs=hyperparameters['initial_train_epoch'], steps_per_epoch = num_batches, validation_data=val_gen,callbacks = [CalculatingPredictions(preds, test_gen, labelled_train_gen, 0.01), csv_logger, tensorboard_callback], verbose = 1)\n","                history_list.append(history.history)\n","                epoch_num+=hyperparameters['initial_train_epoch']\n","            else:\n","                # Increment training --- change\n","                history = AL_model.fit(train_gen, initial_epoch = epoch_num, epochs=epoch_num+hyperparameters['increment_train_epoch'], steps_per_epoch = num_batches, validation_data=val_gen,callbacks = [CalculatingPredictions(preds, test_gen, labelled_train_gen, 0.01), csv_logger, tensorboard_callback], verbose = 1)\n","                history_list.append(history.history)\n","                epoch_num+=hyperparameters['increment_train_epoch']\n","\n","            pred_model.save_weights(\"./saved_history/models/pred_model_from_scratch_vanilla_\"+str(iteration)+'.h5')\n","            disc.save_weights(\"./saved_history/models/disc_model_from_scratch_vanilla_\"+str(iteration)+'.h5')\n","            vae.save_weights(\"./saved_history/models/vae_model_from_scratch_vanilla_\"+str(iteration)+'.h5')\n","\n","            # append indices\n","            indices_list.append(idx_labelled)\n","            inc = int(train.shape[0]*0.05 /  hyperparameters['batch_size'])\n","            num_batches+= inc\n","            print('Number of batches added:' , inc)\n","\n","            with open(\"./saved_history/history_list_\"+str(iteration)+\".json\", 'w') as f:\n","                json.dump(history_list, f, indent=2)\n","\n","            with open(\"./saved_history/preds_\"+str(iteration)+\".json\", 'w') as f:\n","                json.dump(preds, f, indent=2)\n","\n","            np.save(\"./saved_history/idx_labelled_\"+str(iteration)+\".npy\",np.array(idx_labelled))\n","\n","            if (iteration!=(len(splits)-1)): # last iteration\n","                # print(\"checking for uncertainities\")\n","                # Get uncertainities\n","\n","                # _, disc_output,_ ,_= AL_model.predict(unlabelled_gen, verbose=1)\n","\n","                # unlabelled_indices = getIndices(disc_output, hyperparameters) # 0-49999\n","                # outputs = []\n","                # weights = []\n","                # for i in range(hyperparameters['uncertainity_repeat']):\n","                #    print(\"Done \"+str(i))\n","                #    p = AL_model.predict(unlabelled_gen, verbose=1)\n","                #    predictor_output= p[0]\n","                #    joint_weights = p[3]\n","                #    weights.append(np.array(joint_weights)[unlabelled_indices])\n","                #    unlabelled_predictions = np.array(predictor_output)[:,unlabelled_indices,:]\n","                #    outputs.append(unlabelled_predictions)\n","\n","                # avg_output = np.mean(np.array(outputs), axis=0)\n","                # avg_weights = np.mean(np.array(weights), axis=0)    \n","\n","                # t = uncertainity(avg_output, avg_weights)\n","                # t=t[0] + (t[1] * AL_model.alpha)\n","                # uncertain_indices = np.argpartition(t, -1*hyperparameters['num_uncertain_elements'])[-1*hyperparameters['num_uncertain_elements']:]\n","                # img_indice = np.array(unlabelled_indices)[uncertain_indices]\n","\n","                # k = np.array(idx_unlabelled)[img_indice]\n","                \n","                k = random.sample(idx_unlabelled, hyperparameters['num_uncertain_elements'])\n","                \n","                \n","                idx_labelled = idx_labelled+list(k)\n","                labelled_train = pd.DataFrame(train.values[idx_labelled,:], columns=train.columns)\n","                idx_unlabelled = list(np.setdiff1d(idx_unlabelled, k))\n","                unlabelled_train = pd.DataFrame(train.values[idx_unlabelled,:], columns=train.columns)\n","                train_gen = generate_generator_multiple(generator=train_imggen,\n","                                                               dir1=image_path,\n","                                                               dir2=image_path,\n","                                                               df1 = labelled_train,\n","                                                               df2 = unlabelled_train,\n","                                                               batch_size=hyperparameters['batch_size'],\n","                                                               img_height=hyperparameters['height'],\n","                                                               img_width=hyperparameters['width'])\n","                unlabelled_gen = load_generator(unlabelled_train, False)\n","                labelled_train_gen = load_generator(labelled_train, False)\n","    \n","    return history_list, pred_model, vae, disc, AL_model, indices_list, preds"]},{"cell_type":"code","execution_count":22,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T17:26:19.040027Z","iopub.status.busy":"2022-03-02T17:26:19.039481Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","model loaded\n","Found 162768 validated image filenames.\n","Found 19856 validated image filenames.\n","Found 19952 validated image filenames.\n","Found 11904 validated image filenames.\n","Found 11904 validated image filenames.\n","Found 7952 validated image filenames.\n","Epoch 1/2\n"]},{"name":"stderr","output_type":"stream","text":["2023-06-23 09:14:50.436958: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n","\t [[{{node Placeholder/_0}}]]\n"]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."]},{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["history_list, pred_model, vae, disc, Al_model, indices_list, preds = startTraining(trackers, splits, break_point_ep, True, False, False, False, False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2022-03-02T13:21:16.040288Z","iopub.status.idle":"2022-03-02T13:21:16.042719Z"},"trusted":true},"outputs":[],"source":["# k = pd.read_csv('./saved_history/training_results_AL_model_from_scratch_non_cs.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T07:20:35.834659Z","iopub.status.busy":"2022-03-02T07:20:35.834178Z","iopub.status.idle":"2022-03-02T07:20:37.412609Z","shell.execute_reply":"2022-03-02T07:20:37.411179Z","shell.execute_reply.started":"2022-03-02T07:20:35.83462Z"},"trusted":true},"outputs":[],"source":["# preds=[] ####\n","# validation_train_history=[] # new\n","# date = datetime.datetime.now().strftime(\"%d - %b - %y a- %H:%M:%S\")\n","\n","# filename = \"AL_model_from_scratch_non_cs_\"+date ###\n","# logdir = \"./logs/\" + filename\n","\n","# filepath = \"./saved_history/model/AL_model_from_scratch_non_cs\"\n","# checkpoint = ModelCheckpoint(filepath, verbose=1, save_best_only=False, mode='max', period = 1)\n","# csv_logger = CSVLogger('./saved_history/training_results_AL_model_from_scratch_non_cs.csv', separator = ',', append=True)\n","# pre_train_logger = CSVLogger('./saved_history/pre_training_results_AL_model_from_scratch_non_cs.csv', separator = ',', append=True)\n","# tensorboard_callback = TensorBoard(log_dir = logdir)\n","\n","# pre_tensorboard_callback = TensorBoard(log_dir =\" ./logs/pre_\"+filename)\n","\n","# # Instantiate components\n","# # defining my predictor\n","# pred_model = predictor()\n","# pred_model.compile(optimizer = keras.optimizers.SGD(learning_rate=hyperparameters['lr'],\n","#                                                     clipnorm=1.0 ))\n","# # if load_pred_model: #####\n","# #     print(\"Predictor weights loading ...\")\n","# #     pred_model.load_weights(\"./saved_history/models/pred_model_from_scratch_vanilla_4.h5\", by_name=True)\n","\n","# # defining my discriminator\n","# disc_in, disc_out = discriminator()\n","# disc = Model(inputs = disc_in, outputs = disc_out)\n","# disc.compile(optimizer = keras.optimizers.SGD(learning_rate=hyperparameters['lr'],\n","#                                               clipnorm=1.0 ))\n","# # if load_disc: ###\n","# #     print(\"Discriminator weights loading ...\")\n","# #     disc.load_weights(\"./saved_history/models/disc_model_from_scratch_vanilla_4.h5\", by_name=True)\n","\n","# # defining my generator\n","# X, z, mu = variationalAutoEncoder()\n","# vae = Model(inputs = X, outputs = [z,mu])\n","# vae.compile(optimizer = keras.optimizers.RMSprop(learning_rate=hyperparameters['lr'],\n","#                                                  clipnorm=1.0 ))\n","# # if load_vae: ####\n","# #     print(\"VAE weights loading ...\")\n","# #     vae.load_weights(\"./saved_history/models/vae_model_from_scratch_vanilla_4.h5\" ,by_name = True)\n","\n","# # Instantiate AL model\n","# AL_model = ActiveLearning(discriminator=disc, generator=vae, predictor=pred_model, trackers = trackers, alpha=1)\n","# AL_model.compile(\n","#     d_optimizer=keras.optimizers.SGD(learning_rate=hyperparameters['lr'],clipnorm=1.0 ),\n","#     g_optimizer= keras.optimizers.RMSprop(learning_rate=hyperparameters['lr'],clipnorm=1.0),\n","#     p_optimizer=keras.optimizers.SGD(learning_rate=hyperparameters['lr'],clipnorm=1.0 ))\n","\n","# print('model loaded')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T07:20:37.414489Z","iopub.status.busy":"2022-03-02T07:20:37.414137Z","iopub.status.idle":"2022-03-02T07:21:54.832582Z","shell.execute_reply":"2022-03-02T07:21:54.831632Z","shell.execute_reply.started":"2022-03-02T07:20:37.414453Z"},"trusted":true},"outputs":[],"source":["# train_imggen = ImageDataGenerator(rescale = 1./255)\n","# image_path = '../input/celeba-dataset/img_align_celeba/img_align_celeba/'\n","# train, val, test = preprocess(hyperparameters, attr, eval_partition)\n","# train_gen_full = load_generator(train, False)\n","# val_gen = load_generator(val)\n","# test_gen = load_generator(test, False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T07:21:54.834598Z","iopub.status.busy":"2022-03-02T07:21:54.834259Z","iopub.status.idle":"2022-03-02T07:21:54.8391Z","shell.execute_reply":"2022-03-02T07:21:54.8382Z","shell.execute_reply.started":"2022-03-02T07:21:54.834563Z"},"trusted":true},"outputs":[],"source":["# validation_first=True\n","# further_training = False"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T07:22:33.869716Z","iopub.status.busy":"2022-03-02T07:22:33.869328Z","iopub.status.idle":"2022-03-02T07:29:12.877843Z","shell.execute_reply":"2022-03-02T07:29:12.876894Z","shell.execute_reply.started":"2022-03-02T07:22:33.86968Z"},"trusted":true},"outputs":[],"source":["# if validation_first == True:\n","#     print(1)\n","#     logger = CSVLogger('./saved_history/pretraining_results_AL_model_from_scratch_non_cs.csv', separator = ',', append=True)\n","#     pre_tensorboard_callback = TensorBoard(log_dir =\" ./logs/pre_\"+filename)\n","\n","#     labelled_pretrain, idx_prelabelled = divide_data(val, initial=True)\n","#     print(\"Size labelled val \" , labelled_pretrain.shape)\n","#     idx_preunlabelled = list(np.setdiff1d(list(range(val.shape[0])), idx_prelabelled))\n","#     unlabelled_pretrain = pd.DataFrame(val.values[idx_preunlabelled,:], columns=val.columns)\n","#     print(\"Size unlabelled val: \" , unlabelled_pretrain.shape)\n","#     pretrain_gen = generate_generator_multiple(generator=train_imggen,\n","#                                            dir1=image_path,\n","#                                            dir2=image_path,\n","#                                            df1 = labelled_pretrain,\n","#                                            df2 = unlabelled_pretrain,\n","#                                            batch_size=hyperparameters['batch_size'],\n","#                                            img_height=hyperparameters['height'],\n","#                                            img_width=hyperparameters['width'])\n","    \n","#     labelled_pretrain_gen  = load_generator(labelled_pretrain, False)# 744\n","#     ## change \n","#     val_history = AL_model.fit(pretrain_gen, epochs = hyperparameters['pretraining_epochs'], steps_per_epoch = 10, callbacks = [CalculatingPredictions(preds, test_gen, labelled_pretrain_gen, 0.01, True) , pre_tensorboard_callback, logger], verbose=1)\n","#     validation_train_history.append(val_history.history)\n","#     with open(\"./saved_history/pretraining_history_list.json\", 'w') as f:\n","#         json.dump(validation_train_history, f, indent=2)\n","\n","#     labelled_train, idx_labelled = divide_data(train)\n","#     idx_unlabelled = list(np.setdiff1d(list(range(train.shape[0])), idx_labelled))\n","#     unlabelled_train = pd.DataFrame(train.values[idx_unlabelled,:], columns=train.columns)        \n","#     train_gen = generate_generator_multiple(generator=train_imggen,\n","#                                            dir1=image_path,\n","#                                            dir2=image_path,\n","#                                            df1 = labelled_train,\n","#                                            df2 = unlabelled_train, \n","#                                            batch_size=hyperparameters['batch_size'],\n","#                                            img_height=hyperparameters['height'],\n","#                                            img_width=hyperparameters['width'])\n","#     unlabelled_gen = load_generator(unlabelled_train, False)    \n","#     labelled_train_gen = load_generator(labelled_train, False)\n","# elif further_training==True:\n","\n","#     idx_labelled = np.load(\"./saved_history/idx_labelled_4.npy\")\n","#     labelled_train = train.iloc[idx_labelled, :]\n","#     idx_unlabelled = list(np.setdiff1d(list(range(train.shape[0])), idx_labelled))\n","#     unlabelled_train = pd.DataFrame(train.values[idx_unlabelled,:], columns=train.columns)\n","#     train_gen = generate_generator_multiple(generator=train_imggen,\n","#                                            dir1=image_path,\n","#                                            dir2=image_path,\n","#                                            df1 = labelled_train,\n","#                                            df2 = unlabelled_train,\n","#                                            batch_size=hyperparameters['batch_size'],\n","#                                            img_height=hyperparameters['height'],\n","#                                            img_width=hyperparameters['width'])\n","#     unlabelled_gen = load_generator(unlabelled_train, False)\n","#     labelled_train_gen = load_generator(labelled_train, False)\n","# else:\n","#     labelled_train, idx_labelled = divide_data(train)\n","#     idx_unlabelled = list(np.setdiff1d(list(range(train.shape[0])), idx_labelled))\n","#     unlabelled_train = pd.DataFrame(train.values[idx_unlabelled,:], columns=train.columns)        \n","#     train_gen = generate_generator_multiple(generator=train_imggen,\n","#                                            dir1=image_path,\n","#                                            dir2=image_path,\n","#                                            df1 = labelled_train,\n","#                                            df2 = unlabelled_train, \n","#                                            batch_size=hyperparameters['batch_size'],\n","#                                            img_height=hyperparameters['height'],\n","#                                            img_width=hyperparameters['width'])\n","#     unlabelled_gen = load_generator(unlabelled_train, False)    \n","#     labelled_train_gen = load_generator(labelled_train, False)\n","\n","# history_list=[]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T07:30:24.972697Z","iopub.status.busy":"2022-03-02T07:30:24.972372Z","iopub.status.idle":"2022-03-02T07:30:24.977081Z","shell.execute_reply":"2022-03-02T07:30:24.976241Z","shell.execute_reply.started":"2022-03-02T07:30:24.97267Z"},"trusted":true},"outputs":[],"source":["# further_training=False"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T07:30:31.599545Z","iopub.status.busy":"2022-03-02T07:30:31.5992Z","iopub.status.idle":"2022-03-02T07:52:29.94012Z","shell.execute_reply":"2022-03-02T07:52:29.939181Z","shell.execute_reply.started":"2022-03-02T07:30:31.599515Z"},"trusted":true},"outputs":[],"source":["# if further_training==True:\n","#     num_batches = idx_labelled.shape[0]//hyperparameters['batch_size']\n","#     tensorboard_callback = TensorBoard(log_dir = 'AL_model_from_scratch_non_cs_09 - Jan - 21 - 12:50:36')\n","\n","#     iteration =5\n","#     epoch_num = 13\n","\n","#     history = AL_model.fit(train_gen,initial_epoch = epoch_num, epochs=epoch_num+7, steps_per_epoch = num_batches, validation_data=val_gen,callbacks = [CalculatingPredictions(preds, test_gen,labelled_train_gen, 0.01), csv_logger], verbose = 1)\n","#     history_list.append(history.history)\n","#     pred_model.save_weights(\"./saved_history/models/pred_model_from_scratch_vanilla_\"+str(iteration)+'.h5')\n","#     disc.save_weights(\"./saved_history/models/disc_model_from_scratch_vanilla_\"+str(iteration)+'.h5')\n","#     vae.save_weights(\"./saved_history/models/vae_model_from_scratch_vanilla_\"+str(iteration)+'.h5')\n","#     with open(\"./saved_history/history_list_only_\"+str(iteration)+\".json\", 'w') as f:\n","#         json.dump(history_list, f, indent=2)\n","\n","#     with open(\"./saved_history/preds_only_\"+str(iteration)+\".json\", 'w') as f:\n","#         json.dump(preds, f, indent=2)\n","# else:\n","#     test_predictions=[]\n","#     indices_list = []\n","#     epoch_num = 0\n","#     num_batches= 10# int(train.shape[0] * (hyperparameters['initial_percent']) / hyperparameters['batch_size'])\n","\n","#     for iteration in range(len(splits)):\n","#         print(\"Split: \", splits[iteration])\n","\n","#         if iteration==0:\n","#             # Initial training ---- change hyperparameters['initial_train_epoch']\n","#             history = AL_model.fit(train_gen, epochs=1, steps_per_epoch = num_batches, validation_data=val_gen,callbacks = [csv_logger, tensorboard_callback], verbose = 1)\n","#             history_list.append(history.history)\n","#             epoch_num+=hyperparameters['initial_train_epoch']\n","#         else:\n","#             # Increment training --- change\n","#             history = AL_model.fit(train_gen, initial_epoch = epoch_num, epochs=epoch_num+1, steps_per_epoch = 10, validation_data=val_gen,callbacks = [ csv_logger, tensorboard_callback], verbose = 1)\n","#             history_list.append(history.history)\n","#             epoch_num+=hyperparameters['increment_train_epoch']\n","\n","#         pred_model.save_weights(\"./saved_history/models/pred_model_from_scratch_vanilla_\"+str(iteration)+'.h5')\n","#         disc.save_weights(\"./saved_history/models/disc_model_from_scratch_vanilla_\"+str(iteration)+'.h5')\n","#         vae.save_weights(\"./saved_history/models/vae_model_from_scratch_vanilla_\"+str(iteration)+'.h5')\n","\n","#         # append indices\n","#         indices_list.append(idx_labelled)\n","#         inc = 10 # int(train.shape[0]*0.05 /  hyperparameters['batch_size'])\n","#         num_batches+= inc \n","#         print('Number of batches added:' , inc)\n","\n","#         with open(\"./saved_history/history_list_\"+str(iteration)+\".json\", 'w') as f:\n","#             json.dump(history_list, f, indent=2)\n","\n","#         with open(\"./saved_history/preds_\"+str(iteration)+\".json\", 'w') as f:\n","#             json.dump(preds, f, indent=2)\n","\n","#         np.save(\"./saved_history/idx_labelled_\"+str(iteration)+\".npy\",np.array(idx_labelled))\n","\n","#         if (iteration!=(len(splits)-1)): # last iteration\n","#             print(\"checking for uncertainities\")\n","#             k = random.sample(idx_unlabelled, 10*hyperparameters['batch_size'] )#hyperparameters['num_uncertain_elements'])\n","\n","#             idx_labelled = idx_labelled+list(k)\n","#             labelled_train = pd.DataFrame(train.values[idx_labelled,:], columns=train.columns)\n","#             idx_unlabelled = list(np.setdiff1d(idx_unlabelled, k))\n","#             unlabelled_train = pd.DataFrame(train.values[idx_unlabelled,:], columns=train.columns)\n","#             train_gen = generate_generator_multiple(generator=train_imggen,\n","#                                                            dir1=image_path,\n","#                                                            dir2=image_path,\n","#                                                            df1 = labelled_train,\n","#                                                            df2 = unlabelled_train,\n","#                                                            batch_size=hyperparameters['batch_size'],\n","#                                                            img_height=hyperparameters['height'],\n","#                                                            img_width=hyperparameters['width'])\n","#             unlabelled_gen = load_generator(unlabelled_train, False)\n","#             labelled_train_gen = load_generator(labelled_train, False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T06:54:13.824864Z","iopub.status.busy":"2022-03-02T06:54:13.824541Z","iopub.status.idle":"2022-03-02T06:54:13.830839Z","shell.execute_reply":"2022-03-02T06:54:13.829795Z","shell.execute_reply.started":"2022-03-02T06:54:13.824835Z"},"trusted":true},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-02T06:54:31.4804Z","iopub.status.busy":"2022-03-02T06:54:31.480013Z","iopub.status.idle":"2022-03-02T06:54:31.487259Z","shell.execute_reply":"2022-03-02T06:54:31.486347Z","shell.execute_reply.started":"2022-03-02T06:54:31.480365Z"},"trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# check hyperparameters\n","# check splits and their accuracy"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":4}
